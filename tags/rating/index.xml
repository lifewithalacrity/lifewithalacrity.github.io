<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rating on Life With Alacrity</title>
    <link>https://lifewithalacrity.github.io/tags/rating/</link>
    <description>Recent content in Rating on Life With Alacrity</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Sep 2006 16:28:58 -0700</lastBuildDate>
    <atom:link href="https://lifewithalacrity.github.io/tags/rating/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Ratings: Who Do You Trust?</title>
      <link>https://lifewithalacrity.github.io/2006/09/ratings_who_do_.html</link>
      <pubDate>Thu, 14 Sep 2006 16:28:58 -0700</pubDate>
      
      <guid>https://lifewithalacrity.github.io/2006/09/ratings_who_do_.html</guid>
      <description>&lt;p&gt;My colleague, &lt;a href=&#34;http://www.skotos.net/about/staff/shannon_appelcline.php&#34;&gt;Shannon Appelcline&lt;/a&gt;, has been working on a game rating system for &lt;a href=&#34;http://www.rpg.net&#34;&gt;RPGnet&lt;/a&gt;. This has resulted in real-world application of the principles for designing rating systems which we&#39;ve previously discussed in our &lt;a href=&#34;https://lifewithalacrity.github.io/2005/12/collective_choi.html&#34;&gt;Collective Choice&lt;/a&gt; articles. Shannon&#39;s newest article, &lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_196.phtml&#34;&gt;Ratings,
Who Do You Trust?&lt;/a&gt; offers a look at weighting ratings based on reliability.&lt;/p&gt;
&lt;p&gt;&lt;img border=&#34;0&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/shannon_appelcline.jpg&#34; title=&#34;Shannon_appelcline&#34; alt=&#34;Shannon_appelcline&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;On the &lt;a href=&#34;http://index.rpg.net/&#34;&gt;RPGnet Gaming Index&lt;/a&gt; we&#39;ve put this all together to form a tree of weighted ratings that answer the question, &lt;em&gt;who do you trust&lt;/em&gt;?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Here&#39;s how we measured each type of trust, and what we did about it:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Volume of Ratings for an Item.&lt;/strong&gt; Introduce a bayesian weight to offset the variability of items with low-volume ratings.&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Volume of Ratings by a User.&lt;/strong&gt; Give each user a weight based on his volume of contribution which is applied to his ratings.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Depth of Content by a User.&lt;/strong&gt; Give each rating a weight based on the depth of thought implicit in the rating which is applied to that rating.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;These all get put together to create our final ratings for
the Gaming Index, with each user&#39;s individual rating for an item
getting multiplied by its user weight and its content weight, and then
all of that averaged with the other user ratings and the bayesian
weight too. The result is in no way intuitive, but users don&#39;t really
need to understand the back end of a rating system. Conversely we hope
it&#39;s accurate, or at least more accurate than would otherwise be true
given the relatively low volume of ratings we&#39;ve collected thus far.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here are some of Shannon&#39;s earlier discussions about the design behind the new &amp;quot;user content&amp;quot; based &lt;a href=&#34;http://index.rpg.net&#34;&gt;RPGnet Gaming Index&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_191.phtml&#34;&gt;Encouraging
User Creativity&lt;/a&gt; - A look at the &amp;quot;XP&amp;quot; system which has helped to incentivize the creation of the database at the heart of the ratings.
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_193.phtml&#34;&gt;Managing User Creativity, Part Two&lt;/a&gt; - An examination of the nuts and bolts of RPGnet&#39;s Gaming Index database.
&lt;/li&gt;&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Related articles from this blog:&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://lifewithalacrity.github.io/2005/12/systems_for_col.html&#34;&gt;2005-12: Systems for Collective Choice&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://lifewithalacrity.github.io/2005/12/collective_choi.html&#34;&gt;2005-12: Collective Choice: Rating Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://lifewithalacrity.github.io/2006/01/ranking_systems.html&#34;&gt;2006-01: Collective Choice: Competitive Ranking Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://lifewithalacrity.github.io/2006/08/using_5star_rat.html&#34;&gt;2006-08: Using 5-Star Rating Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://lifewithalacrity.github.io/2007/01/collective_choi.html&#34;&gt;2007-01: Experimenting with Ratings&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;p&gt;Related articles from Shannon Appelcline&#39;s &lt;a href=&#34;http://www.skotos.net/articles/show-column.phtml?colname=TTnT_&#34;&gt;Trials, Triumphs &amp;amp; Trivialities:&lt;/a&gt;
&lt;/p&gt;&lt;blockquote&gt;&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_196.phtml&#34;&gt;#196: Collective Choice: Ratings, Who Do You Trust?&lt;/a&gt;&lt;/em&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_198.phtml&#34;&gt;#198: Collective Choice: More Thoughts About Ratings&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;footer&gt;&lt;h3&gt;Comments&lt;/h3&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;URL:
Hmm, interesting on the weighting system.
Can you explain with more detail how the rating by depth of content is generated?
I&#39;m looking through the other articles, so hopefully I find the info there.
Frank
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;#&#34;&gt;magicback (Frank)&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-10-12T01:30:32-07:00&#34;&gt;2006-10-12T01:30:32-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;URL:
My first pass system just used a different multiplier for weight based on the type of content:
1x for just a raw rating
2x for a rating with a non-blank comment
5x for a review (which goes through a different, approved content system)
My second pass system also included &#34;volume of ratings by user&#34; and thus applied a variable multiplier depending on how many ratings the user has made:
(0-2x) for a raw rating
2x(0-2x) for a rating with comment
5x for a review
The 0-2x is calculated as (# of ratings by user)/50, to 2 max.
I&#39;m pretty sure that the core weighting system by depth is producing better results, though I haven&#39;t done any studies of that yet. The additional weighting for volume by users has definitely prevented bias by hit-and-run raters who just stop by to rate one game that they&#39;ve been asked to.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;#&#34;&gt;Shannon Appelcline&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-10-16T15:39:53-07:00&#34;&gt;2006-10-16T15:39:53-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;/footer&gt;
&lt;p class=&#34;previous&#34;&gt;&lt;a href=&#34;https://lifewithalacrity.github.io/previous/2006/09/ratings_who_do_.html&#34; rel=&#34;syndication&#34;&gt;orginal layout&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using 5-Star Rating Systems</title>
      <link>https://lifewithalacrity.github.io/2006/08/using_5star_rat.html</link>
      <pubDate>Fri, 11 Aug 2006 08:49:14 -0700</pubDate>
      
      <guid>https://lifewithalacrity.github.io/2006/08/using_5star_rat.html</guid>
      <description>&lt;p&gt;In &lt;a href=&#34;https://lifewithalacrity.github.io/2005/12/collective_choi.html&#34;&gt;Collective Choice: Rating Systems&lt;/a&gt; I discuss ratings scales of various sorts, from eBay&#39;s 3-point scale to RPGnet&#39;s double 5-point scale, and BoardGame Geek&#39;s 10-point scale.&lt;/p&gt;
&lt;p&gt;&lt;a onclick=&#34;window.open(this.href, &#39;_blank&#39;, &#39;width=373,height=239,scrollbars=no,resizable=no,toolbar=no,directories=no,location=no,menubar=no,status=no,left=0,top=0&#39;); return false&#34; href=&#34;http://lifewithalacrity.blogs.com/.shared/image.html?/photos/uncategorized/stars_1.gif&#34;&gt;&lt;img width=&#34;200&#34; height=&#34;128&#34; border=&#34;0&#34; src=&#34;https://lifewithalacrity.github.io/images/stars_1.gif&#34; title=&#34;Stars_1&#34; alt=&#34;Stars_1&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Of the various ratings scales, 5-point scales are probably the most common on the Internet. You can find them not just in my own RPGnet, but also on Amazon, Netflix, and iTunes, as well as many other sites and services. Unfortunately 5-point rating scales also face many challenges in their use, and different studies suggest different flaws with this particular methodology.&lt;/p&gt;
&lt;p&gt;First, &lt;a href=&#34;http://sloan.ucr.edu/category/working-papers/product-reviews/&#34;&gt;one study&lt;/a&gt; using Amazon data has shown that many &lt;em&gt;undetailed&lt;/em&gt; ratings (where the rater isn&#39;t required to add any additional information other than the rating they select) show a &lt;a href=&#34;http://en.wikipedia.org/wiki/Bimodal_distribution&#34;&gt;bimodal distribution&lt;/a&gt;.&amp;nbsp; In other words the distribution of ratings tends to cluster around two different numbers (e.g., 1 and 5) rather than offering a normal distribution where the ratings cluster around a single height (e.g., 3). Thus the median of these ratings is not an accurate reflection of product quality, but instead is a statement of conflicting opinions.&lt;/p&gt;
&lt;p&gt;Second, &lt;a href=&#34;https://lifewithalacrity.github.io/2005/12/collective_choi.html&#34;&gt;our own study&lt;/a&gt; using RPGnet data has shown that many &lt;em&gt;detailed&lt;/em&gt; ratings (where the rater does add additional information, in this case a full review) offer normal distributions, however it is biased toward the high end of the scale. On RPGnet, for example, we discovered that 90% of this 5-point rating system was 3 or higher with an average around 4.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://360.yahoo.com/profile-9lciejI3aafX1stHPoIRNmkmv4EowQ--&#34;&gt;Randy Farmer&lt;/a&gt; of Yahoo suggests that this scale limitation is particularly troublesome for fan-based ratings, such as those found on episodic TV sites:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;Only the fans of a show evaluate the episodes, and being fans, will never rate an episode one or two stars, ever. I&#39;ve seen this attempted over and over on the net with the same results every time: Each episode of a show is 4-stars +/- .5 stars. This goes all the way back to the Babylon-5 website, probably the first source for this kind of data.&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;(And indeed, the TV episode &lt;a href=&#34;http://www.tv.com/babylon-5/tko/episode/25148/summary.html?tag=ep_list;title;14&#34;&gt;TKO&lt;/a&gt;, from &lt;em&gt;Babylon 5&lt;/em&gt;&#39;s first season, is considered an entirely atrocious episode by even the fans. Yet it has a 6.1 of 10 &amp;quot;Fair&amp;quot; rating on &lt;a href=&#34;http://www.tv.com&#34;&gt;tv.com&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Thus even when a bimodal distribution is not a problem, on a 5-point scale the upward bias often results in only 2 or 3 meaningful data points. This is problematic because it minimizes differentiation. In many cases, a 5-star rating system where most of the ratings are either 3 or 4 is actually no better then just a thumbs-up/thumbs-down rating system. &lt;/p&gt;
&lt;p&gt;However, given that 5-point scales are probably here to stay, we are forced to make the best use of them we can.&lt;/p&gt;
&lt;p&gt;First, we need to provide raters with &lt;em&gt;incentives&lt;/em&gt;, so that they provide meaningful ratings. We&#39;ve already seen that this can be done by requesting detailed ratings: when a person takes the time to write text, and knows that his name will be attached to it, he generally does a better job in his rating. There are other possible incentives techniques as well, such as RPGnet&#39;s new &lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_191.phtml&#34;&gt;XP System&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Second, we need to provide means for a 5-point scale to become more meaningful by encouraging raters to use not just the top half of the scale, but the bottom half as well. One method to accomplish this is to make ratings &lt;em&gt;distinct&lt;/em&gt; -- as I briefly mentioned in my previous article on this topic -- and encourage standards so that an &amp;quot;average&amp;quot; rating is 2 or 3, not 4.&lt;/p&gt;
&lt;p&gt;As an example of how to accomplish both of these goals with already existing 5-point rating scales, I&#39;ve detailed my own experiences with using ratings on two popular services -- iTunes and Amazon. By providing myself with incentives and making my use of ratings very distinctive, I have created more meaningful and useful output for myself.&lt;/p&gt;
&lt;h3&gt;Music Ratings - iTunes&lt;/h3&gt;
&lt;p&gt;Apple&#39;s iTunes software offers you the ability to rate individual songs with a 0-5 Star rating. If you use iTunes with an iPod, you can change the rating of a song on your iPod and the change will be reflected in your iTunes database the next time you sync your iPod. The &amp;quot;Shuffle Songs&amp;quot; feature available on more modern iPods has an option to have songs with higher ratings be played more often. A very powerful feature, Smart Playlists, can dynamically create sophisticated playlists based on ratings. All of this makes rating music on iTunes very useful.&lt;/p&gt;
&lt;p&gt;After Shannon and I wrote our Rating Systems article, I examined the ratings in my iTunes catalog. Using the &lt;a href=&#34;http://girtby.net/archives/2005/11/03/itunes-library-preening/&#34;&gt;Alastair&#39;s&lt;/a&gt; fabulous XLST &lt;a href=&#34;http://girtby.net/offerings/itunes-ratings-stats/&#34;&gt;iTunes rating statistics tool&lt;/a&gt;, I discovered that the ratings I created in iTunes clearly were biased overly high, matching the pattern we&#39;d described. I had far too many songs rated with 4 Stars, and almost nothing rated 1 or 2. This made my ratings less useful.&lt;/p&gt;
&lt;table align=&#34;center&#34;&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;td colspan=&#34;6&#34;&gt;Here are some statistics from your iTunes Library: 4172 tracks, 412 (10%) rated&lt;/td&gt;&lt;/tr&gt;
&lt;br /&gt; &lt;tr&gt;&lt;th colspan=&#34;3&#34;&gt;&amp;nbsp;&lt;/th&gt;
&lt;th colspan=&#34;3&#34;&gt;Cumulative % of Rated&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;th&gt;Number&lt;/th&gt;
&lt;th&gt;% of rated&lt;/th&gt;
&lt;th&gt;Actual&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Shortfall&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 5 stars:&lt;/td&gt;
&lt;td&gt;112&lt;/td&gt;
&lt;td&gt;27&lt;/td&gt;
&lt;td&gt;27&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;-22&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 4 stars:&lt;/td&gt;
&lt;td&gt;183&lt;/td&gt;
&lt;td&gt;44&lt;/td&gt;
&lt;td&gt;72&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;-57&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 3 stars:&lt;/td&gt;
&lt;td&gt;92&lt;/td&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;94&lt;/td&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;-44&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 2 stars:&lt;/td&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;99&lt;/td&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;td&gt;-9&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 1 stars:&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;So over the last few months I&#39;ve completely revamped my iTunes ratings. Since I can&#39;t change the user interface, I&#39;ve changed my behavior. I&#39;m also taking advantage of two other fields: &amp;quot;checked&amp;quot; which I use to give more distinctiveness to my ratings, and &amp;quot;play count&amp;quot; which shows whether or not I&#39;ve listened to something through to the end.&lt;/p&gt;
&lt;p&gt;Here are the criteria I used:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 5 - Exemplars &lt;img border=&#34;0&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/myrating_5_checked.png&#34; title=&#34;Myrating_5_checked&#34; alt=&#34;Myrating_5_checked&#34; /&gt;:&lt;/strong&gt; Only my most favorite songs are rated 5. They have to meet the following criteria: they make me feel good or excite me no matter how often I listen to them, I can typically listen to them often without getting tired of them, and they are the best of their particular genre.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 4 - Great &lt;img border=&#34;0&#34; alt=&#34;Myrating_4_checked&#34; title=&#34;Myrating_4_checked&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/myrating_4_checked.png&#34; /&gt;:&lt;/strong&gt; There is only a small difference between a song that is rated 4 and 5 in my ratings -- typically it doesn&#39;t excite me or make me smile quite as much, or it isn&#39;t necessarily an exemplar of its genre. However, I still can typically listen to them often without getting tired of them. Items that are rated 4 and 5 are ones that I carry on my iPod Shuffle.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 4 - Great (Unchecked) &lt;img border=&#34;0&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/myrating_4_unchecked.png&#34; title=&#34;Myrating_4_unchecked&#34; alt=&#34;Myrating_4_unchecked&#34; /&gt;:&lt;/strong&gt; There are a few songs that I do consider to be great, but that I only want to play when I&#39;m in the mood for them, or I want to only play in a specific order, or they &amp;quot;don&#39;t play well&amp;quot; with other music. For instance I love the song &amp;quot;The Highwayman&amp;quot; by Loreena McKennitt, however, it is over 10 minutes long and I just don&#39;t want to hear that type of song unless I&#39;m in the mood for it. Other examples are the 12 songs that make up Mussorgsky&#39;s &amp;quot;Pictures at an Exhibition&amp;quot;&amp;nbsp; -- I want them played in order when I do play them, and I really don&#39;t want them played in the middle of my other songs. Unfortunately, iTunes does not let you select only unchecked items, so I don&#39;t have a Smart Playlist for these; instead I keep them in a regular playlist.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 3 - Good &lt;img border=&#34;0&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/myrating_3_checked.png&#34; title=&#34;Myrating_3_checked&#34; alt=&#34;Myrating_3_checked&#34; /&gt;:&lt;/strong&gt; These are songs I like. Typically I can play them regularly but not too often. Songs rated 3-5 go on my iPod Nano.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 3 - Good (Unchecked) &lt;img border=&#34;0&#34; alt=&#34;Myrating_3_unchecked&#34; title=&#34;Myrating_3_unchecked&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/myrating_3_unchecked.png&#34; /&gt;:&lt;/strong&gt; There is a lot of music that I think is Good, but I don&#39;t want to play all the time. I have a large catalog of sound tracks from movies. All but a few of those tracks are in this category. Again, iTunes does not let you select only unchecked items in a Smart Playlist, so I have several regular playlists for these items.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 2 - Ok &lt;img border=&#34;0&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/myrating_2_checked.png&#34; title=&#34;Myrating_2_checked&#34; alt=&#34;Myrating_2_checked&#34; /&gt;:&lt;/strong&gt; I have very diverse musical tastes, starting with jazz, various ethnic and world music, and also including quite a bit of pop, rap, R&amp;amp;B, punk, and metal that I enjoy. I don&#39;t enjoy them all the time -- but I do like them to pop up every once in a while for variety. So I rate these 2 and leave them checked. I have an old 40GB iPod that I take on long trips, and it stores everything I have that is checked and rated 2-5.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 2 - Ok (Unchecked) &lt;img border=&#34;0&#34; alt=&#34;Myrating_2_unchecked&#34; title=&#34;Myrating_2_unchecked&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/myrating_2_unchecked.png&#34; /&gt;:&lt;/strong&gt; Some songs are OK, but I really have to be in the mood specifically for that song. Listening to Jimmy Buffet&#39;s &amp;quot;Margaritaville&amp;quot; can be a guilty pleasure on a lazy summer day at the beach, but it isn&#39;t something I want to regularly listen to. I have a number of special playlists for songs rated like this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 1 - Don&#39;t Like &lt;img border=&#34;0&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/myrating_1_checked.png&#34; title=&#34;Myrating_1_checked&#34; alt=&#34;Myrating_1_checked&#34; /&gt;:&lt;/strong&gt; These are the songs that I don&#39;t like. They&#39;re just not my style. Many are still quality music, they just doesn&#39;t work for me. I do keep most of these for completeness -- it might just be one or two songs on the album, and I want to keep the album complete. Or I keep it in case my tastes change. But in general, once something is rate 1 Star, I&#39;ll probably never listen to it again.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 1 - Trash (Unchecked) &lt;img border=&#34;0&#34; alt=&#34;Myrating_1_unchecked&#34; title=&#34;Myrating_1_unchecked&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/myrating_1_unchecked.png&#34; /&gt;:&lt;/strong&gt; These are songs that not only do I not like, they just are not good music. I don&#39;t like most rap music, but I can tell that most are still quality. Some are junk -- these I rate 1 and uncheck, and are candidates for deletion the next time I purge my collection.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unrated &amp;amp; Listened &lt;img border=&#34;0&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/myrating_0_checked.png&#34; title=&#34;Myrating_0_checked&#34; alt=&#34;Myrating_0_checked&#34; /&gt;, playcount &amp;gt; 0:&lt;/strong&gt; If I&#39;ve listened to something through to the end, but haven&#39;t rated it yet, it shows up in this Smart Playlist. Periodically I check this Smart Playlist, sort by playcount, and try to rate everything that I&#39;ve listened to more then once.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unrated &amp;amp; Unlistened &lt;img border=&#34;0&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/myrating_0_checked.png&#34; title=&#34;Myrating_0_checked&#34; alt=&#34;Myrating_0_checked&#34; /&gt;, play count=0:&lt;/strong&gt; This is the default when a new song is added to my library. So any song that is unrated, checked, and has a play count of 0 shows up in my &amp;quot;Unrated &amp;amp; Unlistened&amp;quot; Smart Playlist. When I&#39;m in the mood for variety, I go through this playlist and rate songs.&lt;/p&gt;
&lt;p&gt;Modifying my rating system in this way has caused my average rating for music to change from around 4 to somewhere between 2 and 3. It will probably, over time, become closer to 2 as I rate more of my collection. This gives me a lot of distinctiveness so that I can create Smart Playlists that work well for me.&lt;/p&gt;
&lt;table align=&#34;center&#34;&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;td colspan=&#34;6&#34;&gt;Here are some statistics from your iTunes Library: 6519 tracks, 726 (11%) rated&lt;/td&gt;&lt;/tr&gt;
&lt;br /&gt; &lt;tr&gt;&lt;th colspan=&#34;3&#34;&gt;&amp;nbsp;&lt;/th&gt;
&lt;th colspan=&#34;3&#34;&gt;Cumulative % of Rated&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;th&gt;Number&lt;/th&gt;
&lt;th&gt;% of rated&lt;/th&gt;
&lt;th&gt;Actual&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Shortfall&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 5 stars:&lt;/td&gt;
&lt;td&gt;74&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;-5&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 4 stars:&lt;/td&gt;
&lt;td&gt;144&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;-15&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 3 stars:&lt;/td&gt;
&lt;td&gt;211&lt;/td&gt;
&lt;td&gt;29&lt;/td&gt;
&lt;td&gt;59&lt;/td&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;-9&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 2 stars:&lt;/td&gt;
&lt;td&gt;270&lt;/td&gt;
&lt;td&gt;37&lt;/td&gt;
&lt;td&gt;96&lt;/td&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;td&gt;-6&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 1 stars:&lt;/td&gt;
&lt;td&gt;27&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Obviously rating a large music collection can become a chore -- you don&#39;t want to spend your limited music listening time always fine tuning your ratings. So I have some approaches that make it easier for me to rate my music with less effort:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;p&gt;First, I sorted my catalog by my old ratings, and modified everything down by 1, Starting with everything rated 2 becoming 1, 3 becoming 2, etc. This gave me a good base to start with&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://lifewithalacrity.blogs.com/.shared/image.html?/photos/uncategorized/exemplar_smart_playlist.png&#34; onclick=&#34;window.open(this.href, &#39;_blank&#39;, &#39;width=512,height=216,scrollbars=no,resizable=no,toolbar=no,directories=no,location=no,menubar=no,status=no,left=0,top=0&#39;); return false&#34;&gt;&lt;img width=&#34;200&#34; height=&#34;84&#34; border=&#34;0&#34; alt=&#34;Exemplar_smart_playlist&#34; title=&#34;Exemplar_smart_playlist&#34; src=&#34;https://lifewithalacrity.github.io/images/exemplar_smart_playlist.png&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
Next I created Smart Playlists for each rating, i.e. &amp;quot;Rating 5 - Exemplar&amp;quot; with &amp;quot;Match only checked songs&amp;quot; and &amp;quot;Live updating&amp;quot; checked. I then added &amp;quot;Play Count&amp;quot; as a column to my view, and sorted by it. This gave me the songs that I played the most and least, and I adjusted some songs up and down accordingly.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://lifewithalacrity.blogs.com/.shared/image.html?/photos/uncategorized/plays_well_with_others_smart_playlist.png&#34; onclick=&#34;window.open(this.href, &#39;_blank&#39;, &#39;width=512,height=241,scrollbars=no,resizable=no,toolbar=no,directories=no,location=no,menubar=no,status=no,left=0,top=0&#39;); return false&#34;&gt;&lt;img width=&#34;200&#34; height=&#34;94&#34; border=&#34;0&#34; alt=&#34;Plays_well_with_others_smart_playlist&#34; title=&#34;Plays_well_with_others_smart_playlist&#34; src=&#34;https://lifewithalacrity.github.io/images/plays_well_with_others_smart_playlist.png&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
Then I created a new Smart Playlist that simply plays songs rated 3 to 5, limiting the list to the first 100 GB selected by random (i.e. everything random), and saved this Smart Playlist as &amp;quot;Plays Well With Others&amp;quot;. I play this on occasion in the background, and when I hear something that jars me I know something isn&#39;t rated right. Thus without a lot of effort I can change ratings for songs that no longer fit their rating, or uncheck items where the rating was appropriate but it &amp;quot;didn&#39;t play well with others&amp;quot;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I try to be aware when I&#39;m using my iPod of what a songs rating is, and change it if it seems wrong. The next time I sync the iPod my ratings will be adjusted in my iTunes catalog.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;img border=&#34;0&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/play_count.png&#34; title=&#34;Play_count&#34; alt=&#34;Play_count&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;I also try to be aware of Play Count -- this number only goes up if you play a song to the end. So even if I&#39;m not able to take a look at the rating (for instance when I&#39;m in a car), I can at least forward to the next song. Periodically I review the play counts for songs that I&#39;ve rated and consider moving them up and down accordingly. Of course, this means that I have to be careful and not let the iPod keep running when I&#39;m not listening.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;A tip for those of you that do put a lot of effort into your iTunes ratings: I&#39;ve learned the hard way that unlike most song information, the rating is &lt;strong&gt;NOT&lt;/strong&gt; stored in the song itself, so if your iTunes database gets corrupted, or you move your music to another server, you&#39;ll lose all your ratings. One way to avoid this is to periodically backup your ratings into a field that is stored in the song itself. I personally use the &amp;quot;Grouping&amp;quot; field as it is rarely used, select all songs with the same rating and click on &amp;quot;Get Info&amp;quot;, and change the Grouping field to &amp;quot;My Rating: 5 Stars&amp;quot;.&lt;/p&gt;
&lt;p&gt;I only have 11% of my collection rated so far, but using this system I&#39;m finding it a lot easier to manage my ratings. I&#39;m already getting many benefits from it -- I&#39;m playing my music more often, my iPods typically have the music I want on them, and various music discovery services can use my ratings to help me identify new music I might enjoy. This provides the &lt;em&gt;incentive&lt;/em&gt; to keep me entering meaningful ratings.&lt;/p&gt;
&lt;h3&gt;Book Ratings - Amazon&lt;/h3&gt;
&lt;p&gt;Amazon also uses a 5-Star rating system, and your ratings can be used by Amazon to help you find books that you might like. Though I like to support my local bookstores, it is this feature that brings me back to Amazon time and again. Whenever I browse through Amazon and see a book I&#39;ve already read I try to take the time to update my rating.&lt;/p&gt;
&lt;p&gt;Amazon has a number of different tools to assist you in your ratings. If you are an Amazon customer, you can go to &lt;a href=&#34;http://www.amazon.com/gp/yourstore/iyr/?ie=UTF8&amp;amp;collection=owned&#34;&gt;Improve Your Recommendations: Edit Items You Own&lt;/a&gt; and see all the books that you&#39;ve purchased and quickly rate them with a nice AJAX interface. You can also review items that you&#39;ve already rated, whether or not you own them, at &lt;a href=&#34;http://www.amazon.com/gp/yourstore/iyr/?ie=UTF8&amp;amp;collection=rated&#34;&gt;Improve Your Recommendations: Edit Items You&#39;ve Rated&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a onclick=&#34;window.open(this.href, &#39;_blank&#39;, &#39;width=640,height=509,scrollbars=no,resizable=no,toolbar=no,directories=no,location=no,menubar=no,status=no,left=0,top=0&#39;); return false&#34; href=&#34;http://lifewithalacrity.blogs.com/.shared/image.html?/photos/uncategorized/amazon_your_media_library.png&#34;&gt;&lt;img width=&#34;200&#34; height=&#34;159&#34; border=&#34;0&#34; src=&#34;https://lifewithalacrity.github.io/images/amazon_your_media_library.png&#34; title=&#34;Amazon_your_media_library&#34; alt=&#34;Amazon_your_media_library&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
Amazon has also recently added a very nice web service called &lt;a href=&#34;http://www.amazon.com/gp/library/&#34;&gt;Your Media Library&lt;/a&gt; that can be used to help manage your media library of books, music, and dvds. I personally only have used it to manage my books and dvds, as I find rating albums useless -- it is songs that I prefer to rate.&lt;/p&gt;
&lt;p&gt;After browsing through my ratings to date, I discovered the same flaws I found iTunes -- my ratings typically were too high; most were a 4. This is particularly encouraged by the popup when your cursor is over the Stars &amp;quot;1 - I hate it, 2 - I don&#39;t like it, 3 - It&#39;s Ok, 4 - I like it, and 5 - I love it&amp;quot;. I suspect if I use the same trick that I use for iTunes of making a rating of 2 Stars mean &amp;quot;Ok&amp;quot; I could potentially cause the recommendation engine to be less effective (though it could possibly make it better, I don&#39;t know). So I am being much more brutal with my ratings and pushing many more down to 3, so that my ratings of 4 and 5 have more meaning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5 Stars &lt;img border=&#34;0&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/amazon_5_star.png&#34; title=&#34;Amazon_5_star&#34; alt=&#34;Amazon_5_star&#34; /&gt;:&lt;/strong&gt; These have to be the exemplars -- the best books I&#39;ve ever read, would be glad to read again, would be proud to show off on my best bookshelf, and will buy extra copies to give to friends.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4 Stars &lt;img border=&#34;0&#34; alt=&#34;Amazon_4_star&#34; title=&#34;Amazon_4_star&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/amazon_4_star.png&#34; /&gt;:&lt;/strong&gt; These have to be really good books -- most of them I&#39;m willing to read again and I promote them by offering to loan them to my more discriminating friends. Although I may keep them on my bookshelf I&#39;d rather give them to a friend then sell them at a used book store.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3 Stars &lt;img border=&#34;0&#34; alt=&#34;Amazon_3_star&#34; title=&#34;Amazon_3_star&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/amazon_3_star.png&#34; /&gt;:&lt;/strong&gt; These are books are decent books, and I do share them with my voracious reader friends. But I don&#39;t push them and I&#39;m much more likely to sell them at a used bookstore then keep them on my shelf. This is the rating that I significantly underused previously, and I&#39;m finding that the key discriminator for me so far is how much I feel like recommending this to friends who are more discriminating readers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2 Stars &lt;img border=&#34;0&#34; alt=&#34;Amazon_2_star&#34; title=&#34;Amazon_2_star&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/amazon_2_star.png&#34; /&gt;:&lt;/strong&gt; This rating is where the Amazon rating system fails the most -- these are suppost to be books that &amp;quot;I don&#39;t like&amp;quot;, however, most of the time I don&#39;t buy books that I probably wouldn&#39;t like, much less read them, so I have very few in this category. However, I&#39;ve decided this category is for books that are just not quite good enough, or are slightly disappointing. Not bad, or disliked, but just somewhat disappointing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1 Stars &lt;img border=&#34;0&#34; alt=&#34;Amazon_1_star&#34; title=&#34;Amazon_1_star&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/amazon_1_star.png&#34; /&gt;:&lt;/strong&gt; This is where I put the books that I don&#39;t like, or worse, I hate. Not many here, but I&#39;m willing to risk more then many people are so I have some. Also books go here that just don&#39;t fit my interest, like romance novels that get recommended to me because I like some crossover fantasy-romance authors.&lt;/p&gt;
&lt;p&gt;Since I started more accurately rating my books at Amazon, I&#39;ve found that their suggestions for other books to read to be more accurate. Thus I am getting value from rating these books, and I have &lt;em&gt;incentive&lt;/em&gt; to continue to make the effort.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Offering an incentive for people to rate is important for ratings of all sorts, with both individual gain and status recognition being powerful motivators.&lt;/p&gt;
&lt;p&gt;However the easiest technique for making a 5-point rating scale more useful is to make it &amp;quot;distinct&amp;quot;. If a user has a more specific meaning for each rating, ratings will slowly settle toward a truer average, and thus more of each rating scale will be used. We&#39;ve also tried this technique recently on RPGnet, with our new &lt;a href=&#34;http://index.rpg.net/&#34;&gt;Gaming Index&lt;/a&gt;; and thus far our new 10-point scale -- which has distinct meanings for each number -- is averaging &lt;a href=&#34;http://index.rpg.net/browse.phtml?count=50&amp;amp;type=Best&amp;amp;productCategory=Entries&#34;&gt;7.27&lt;/a&gt;. That&#39;s still a fair amount above the real average of 5.5, but at least it&#39;s below the 8+ rating that our old double 5-point scale resulted in.&lt;/p&gt;
&lt;p&gt;Often you, as a consumer of rating systems, will be making use of rating scales designed by others, rather than those you&#39;re designing yourself. For those cases it often makes sense to design your own rules for what each number means, and to do so in such a way that your median is the average of the scale, rather than toward one of the extremes. When you do, even if you&#39;re using a tight 5-point scale you&#39;ll end up with enough differentiation for it to actually be more meaningful than a thumbs up or a thumbs down.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Related articles from this blog:&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://lifewithalacrity.github.io/2005/12/systems_for_col.html&#34;&gt;2005-12: Systems for Collective Choice&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://lifewithalacrity.github.io/2005/12/collective_choi.html&#34;&gt;2005-12: Collective Choice: Rating Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://lifewithalacrity.github.io/2006/01/ranking_systems.html&#34;&gt;2006-01: Collective Choice: Competitive Ranking Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://lifewithalacrity.github.io/2007/01/collective_choi.html&#34;&gt;2007-01: Experimenting with Ratings&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;p&gt;Related articles from Shannon Appelcline&#39;s &lt;a href=&#34;http://www.skotos.net/articles/show-column.phtml?colname=TTnT_&#34;&gt;Trials, Triumphs &amp;amp; Trivialities:&lt;/a&gt;
&lt;/p&gt;&lt;blockquote&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_192.phtml&#34;&gt;#192: Managing User Creativity, Part One&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_193.phtml&#34;&gt;#193: Managing User Creativity, Part Two&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_196.phtml&#34;&gt;#196: Collective Choice: Ratings, Who Do You Trust?&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_198.phtml&#34;&gt;#198: Collective Choice: More Thoughts About Ratings&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;footer&gt;&lt;h3&gt;Comments&lt;/h3&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;See
http://web.archive.org/web/20010207224515/http://people.delphi.com/mike100000/p5summary.html#summary (or click my name below) for a summary of all the votes for the B5 Series.
Averaging all episodes you get a mean score of 8.19 with a standard deviation of 0.84 - a *very* narrow range.
Context and Motivation matter more than scale in getting reusable scores out of rating systems. As I like to tell the folks here at Yahoo! – the person creating the rating has to get something out of the transaction other than just altruism.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://web.archive.org/web/20010207224515/http://people.delphi.com/mike100000/p5summary.html#summary&#34;&gt;F. Randall Farmer&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-08-11T09:49:28-07:00&#34;&gt;2006-08-11T09:49:28-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;I thought the issue, like Randy Farmer suggests, was that only positive discrete scales always tend to their maximum.  The problem then in rating systems, like you mentioned I think earlier, is that readers have to interpret what the real norm and limits are.  Even if every rating is in the upper quartile.  You may want to check out what I did in Playerep (http://www.playerep.com), where the rating system is based on election but not a fixed scale (always norming to zero without influence).   FWIW. Thanks.
Adam
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://tidehorion.blogspot.com&#34;&gt;Adam MacDonald&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-08-11T10:21:24-07:00&#34;&gt;2006-08-11T10:21:24-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;URL:
I like your classification scheme for iTunes. I&#39;m going to have to work on adapting my scale (which is definitely showing signs of skewing towards the high end).
One thing, though. You mention that you can&#39;t create smart playlists to show unchecked songs. It&#39;s true that you can&#39;t do it with only one playlist, but you can do so with 2:
#1) Any playlist (for example, let&#39;s say your smart playlist for 4 star, checked songs, so you have it set (I assume) for My Rating = 4, Match Only Checked Songs = checked)
#2) Use these criteria:  My Rating = 4, Playlist Is Not &lt;playlist #1&gt;. Leave match only checked songs unchecked, and enable live updating and you have a smart playlist that shows you your unchecked 4-star songs.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;#&#34;&gt;Andy Tinkham&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-08-16T13:55:43-07:00&#34;&gt;2006-08-16T13:55:43-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;URL:
&gt;&gt;Unfortunately, iTunes does not let you select only unchecked items, so I don&#39;t have a Smart Playlist for these; instead I keep them in a regular playlist.
You can:
Make a smart playlist called &#39;Checked&#39; with a rule that is always true (e.g. artist does not contain (leave the field emtpy), or bitrate &gt; 0). Create a smart playlist called &#39;Unchecked&#39; with the rule: &#39;Playlist is not Checked&#39;. There you have all your unchecked files.
From there you can make smart ones that use this Unchecked playlist.
I like your rating system. I have 40% on a 3star now, and around 25% for 2 and 4 stars. I might scale it down a little bit more.
hmm, after typing this I read the comments and see Adam McDonald already mentioned something similar about the unchecked items. I&#39;ll leave it here anyway...
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;#&#34;&gt;Tino&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-10-16T11:52:32-07:00&#34;&gt;2006-10-16T11:52:32-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;Christopher, I think some, if not all, of the issues with the 5-point rating system could be addressed with a better UI. For example, displaying a normal distribution curve skewed by accumulated rating to nudge raters away from the extremes. Of course, this doesn&#39;t work well if sample size is too small which usually happens on the far end of the long tail.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://www.docuverse.com/blog/donpark/&#34;&gt;Don Park&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-12-17T18:25:50-07:00&#34;&gt;2006-12-17T18:25:50-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;URL:
Thanks for writing this article. I found another view on 5 point music rating other than my own informational.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;#&#34;&gt;Jonathan&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2008-02-06T00:33:36-07:00&#34;&gt;2008-02-06T00:33:36-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;/footer&gt;
&lt;p class=&#34;previous&#34;&gt;&lt;a href=&#34;https://lifewithalacrity.github.io/previous/2006/08/using_5star_rat.html&#34; rel=&#34;syndication&#34;&gt;orginal layout&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Systems for Collective Choice</title>
      <link>https://lifewithalacrity.github.io/2005/12/systems_for_col.html</link>
      <pubDate>Thu, 01 Dec 2005 16:03:26 -0700</pubDate>
      
      <guid>https://lifewithalacrity.github.io/2005/12/systems_for_col.html</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-size: 0.7em;&#34;&gt;by Christopher Allen &amp;amp; Shannon Appelcline&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/about/staff/shannon_appelcline&#34;&gt;[Shannon Appelcline&lt;/a&gt; is a friend and colleague of mine at &lt;a href=&#34;http://www.skotos.net&#34;&gt;Skotos&lt;/a&gt;, an online game company. Over the last few years we&#39;ve had many discussions about how decisions are made, and how our society collectively makes choices. The origin of these discussions have varied from &amp;quot;what makes this board game work?&amp;quot;, to &amp;quot;how can we give our players more control of our online games?&amp;quot;, to &amp;quot;how do we make decisions in our company?&amp;quot;, and of course &amp;quot;how did we collectively make such a mess of decision making in America?&amp;quot;. This article, and some followup articles, summarize our thoughts on these topics, and will be &lt;a href=&#34;http://www.skotos.net/articles/TTnT_178.phtml&#34;&gt;jointly posted&lt;/a&gt; in Shannon&#39;s &lt;a href=&#34;http://www.skotos.net/articles/TTnT.shtml&#34;&gt;Trials, Triumphs &amp;amp; Trivialities&lt;/a&gt; online games column at Skotos.]&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img border=&#34;0&#34; alt=&#34;Peoplearoundthesun&#34; title=&#34;Peoplearoundthesun&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/peoplearoundthesun.gif&#34; style=&#34;margin: 0px 5px 5px 0px; float: left;&#34; /&gt; Collective choice systems have been around for a long time. Since at least the birth of democracy in ancient Greece people have made joint decisions about important issues, and since at least the knightly tournaments of the late Middle Age people have competed to be ranked against their peers. Today Western culture especially values diversity of input when implementing any type of choice, believing that wide input from a variety of people provides the fairest result.&lt;/p&gt;
&lt;p&gt;The Internet expands this long history of collective choice. However, as we bring collective choice systems onto the Internet, quantifying and programming them, we discover the need to be more analytical and more methodical in the techniques used. Thus we&#39;re beginning to learn that we don&#39;t know nearly as much about these collective choice systems as we should. There is a need to analyze and study them further, to understand their strengths and weaknesses, and to evaluate their social impact. Fortunately, the &lt;a href=&#34;https://lifewithalacrity.github.io/2004/10/tracing_the_evo.html&#34;&gt;social software&lt;/a&gt; and online games on the Internet provides the perfect &lt;em&gt;petri dish&lt;/em&gt; for doing so.&lt;/p&gt;
&lt;p&gt;Before any analysis can occur, however, there is a need for a categorization of systems and a definition of terms. That is the purpose of this article: to lay out at least some of the ways in which collective choices can be made, to organize them, to define them, and to briefly consider them. &lt;/p&gt;
&lt;p&gt;Broadly, there seem to be three methods of collective choice, divided by the intended result: &lt;em&gt;selection&lt;/em&gt;, &lt;em&gt;opinion&lt;/em&gt;, or &lt;em&gt;comparison&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;&lt;a name=&#34;selection_systems&#34;&gt;Selection Systems&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Selection systems allow for the purposeful choice between multiple items. There are many types of selection systems, but two in particular, &lt;em&gt;representative systems&lt;/em&gt;, &lt;em&gt;deliberative systems&lt;/em&gt;, and &lt;em&gt;consensus systems&lt;/em&gt; are worth noting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a onclick=&#34;window.open(this.href, &#39;_blank&#39;, &#39;width=369,height=393,scrollbars=no,resizable=no,toolbar=no,directories=no,location=no,menubar=no,status=no,left=0,top=0&#39;); return false&#34; href=&#34;http://lifewithalacrity.blogs.com/.shared/image.html?/photos/uncategorized/19thamend.jpg&#34;&gt;&lt;img width=&#34;150&#34; height=&#34;159&#34; border=&#34;0&#34; src=&#34;https://lifewithalacrity.github.io/images/19thamend.jpg&#34; title=&#34;19thamend&#34; alt=&#34;19thamend&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt; &lt;a name=&#34;representative_systems&#34;&gt;Representative Systems:&lt;/a&gt;&lt;/strong&gt; In a representative system, individuals cast a ballot for someone who will represent their interests. They&#39;re by definition &lt;em&gt;voting systems&lt;/em&gt; and the heart of any &lt;a href=&#34;http://en.wikipedia.org/wiki/Republicanism&#34;&gt;Republican&lt;/a&gt; system of government. When you&#39;re voting for a president, prime minister, senator, congressman, director, or board member, that&#39;s representative voting&lt;/p&gt;
&lt;p&gt;In most representative voting a winner is selected by &lt;a href=&#34;http://en.wikipedia.org/wiki/Plurality&#34;&gt;plurality&lt;/a&gt;, meaning the winner had more votes than any other candidate. This works well in a simple two-member election, but begins to fall apart if there are multiple candidates, because similar candidates can steal votes from each other, and thus allow a candidate with less popular ideas to be elected.&lt;/p&gt;
&lt;p&gt;The simplest solution is to require a &lt;a href=&#34;http://en.wikipedia.org/wiki/Majority&#34;&gt;majority&lt;/a&gt; victory, meaning that one winner must have at least 50% of the votes. Some places in the United States use this system for their representative elections, holding a first election, eliminating all but the two biggest vote-getters, then holding a new election between these two.&lt;/p&gt;
&lt;p&gt;Another solution is a &lt;a href=&#34;http://en.wikipedia.org/wiki/Primary_election&#34;&gt;primary-based election system&lt;/a&gt;, wherein all like-minded candidates compete against each other before participating in the real election. This requires buy-in from all like-minded candidates, however, and recent U.S. elections with third-party candidates like Ralph Nader and Ross Perot show the flaws in a voluntary primary system.&lt;/p&gt;
&lt;p&gt;Many other types of &lt;a href=&#34;http://en.wikipedia.org/wiki/Voting_systems&#34;&gt;voting systems&lt;/a&gt; are possible, most of which allow voters to select multiple candidates at the same time. These systems then eliminate the lowest ranked candidates and give their votes to others based upon those voter selections. &lt;/p&gt;
&lt;p&gt;&lt;img border=&#34;0&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/irv.gif&#34; title=&#34;Irv&#34; alt=&#34;Irv&#34; style=&#34;margin: 5px 5px 0px 0px; float: left;&#34; /&gt; &lt;a href=&#34;http://en.wikipedia.org/wiki/IRV&#34;&gt;Instant Runoff Voting&lt;/a&gt;, or IRV, is a fairly commonly used multiple candidate system (though not necessarily the best one). It&#39;s technically a &lt;a href=&#34;http://en.wikipedia.org/wiki/Single_Transferable_Vote&#34;&gt;single transferable vote&lt;/a&gt; preferential voting system. Wikipedia describes the process like this:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;Each voter ranks at least one candidate in order of preference.&lt;br /&gt; ...&lt;br /&gt; First choices are tallied. If no candidate has the support of a majority of voters, the candidate with the least support is eliminated. A second round of counting takes place, with the votes of supporters of the eliminated candidate now counting for their second choice candidate. After a candidate is eliminated, he or she may not receive any more votes.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This process of counting and eliminating is repeated until one candidate has over half the votes.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt;
&lt;p&gt;It&#39;s simple to understand, but also flawed. That&#39;s because every voting system ultimately has &lt;em&gt;some&lt;/em&gt; &lt;a href=&#34;http://en.wikipedia.org/wiki/Arrow&#39;s_impossibility_theorem&#34;&gt;flaw&lt;/a&gt; in it, as is evidenced by the fact that given the same conditions the different systems will often declare different winners. Different systems may also allow voters to &amp;quot;game&amp;quot; the system in different ways. This is often called &lt;a href=&#34;http://en.wikipedia.org/wiki/Tactical_voting&#34;&gt;tactical voting&lt;/a&gt; or strategic voting. Similar to analyzing various &amp;quot;attacks&amp;quot; when studying a cryptosystem, looking at which tactical voting approaches each voting system is vulnerable to helps you evaluate the voting systems.&lt;/p&gt;
&lt;p&gt;For instance, one type of tactical voting that can be used against an Instant Runoff Vote is the push-over strategy, which Wikipedia describes as this:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;Push-over is a type of strategic voting in which a voter ranks a perceived weak alternative higher, but not in the hopes of getting it elected. This primarily occurs in runoff voting when a voter already believes that his favored candidate will make it to the next round - the voter then ranks an unpreferred, but easily beatable, candidate higher so that his preferred candidate can win later. A United States analogy would be voters of one party crossing over to vote in the other party&#39;s primary to nominate a candidate who will be easy for their favorite to beat.&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For example, in an IRV election between Bush, Gore, and Nader, Democrats might rank Nader over Bush. The hope would be that this would give Nader enough votes to keep him from being eliminated, thus knocking Bush out instead in the first round. Afterward the Republican Bush votes transfer to the less progressive Gore, rather than the fringe Nader, allowing Gore to beat the &amp;quot;pushover&amp;quot; Nader when they couldn&#39;t have faced Bush in a straight-up fight.&lt;/p&gt;
&lt;p&gt;There several different tactical voting &amp;quot;attacks&amp;quot; against various representative voting systems. One of the technically better multiple-candidate voting methods is the &lt;a href=&#34;http://en.wikipedia.org/wiki/Condorcet_method&#34;&gt;condorcet method&lt;/a&gt; for voting. It&#39;s immune to most tactical voting strategies and more people would consider its result &amp;quot;correct&amp;quot;, but unfortunately it is much harder on the voters, who have to rank every single candidate. Maybe if we can create a better Internet user-interface to condorcet voting we can make this more sophisticated representative voting system more broadly available.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a name=&#34;deliberative_systems&#34;&gt;Deliberative Systems:&lt;/a&gt;&lt;/strong&gt; In a deliberative system, individuals directly make a decision, rather than selecting a representative to do so. Deliberative systems do &lt;em&gt;not&lt;/em&gt; have to include voting, and the subcategory of &lt;em&gt;consensus systems&lt;/em&gt; described below technically don&#39;t, however most modern deliberative sytems do. A deliberative system is the heart of &lt;a href=&#34;http://en.wikipedia.org/wiki/DEMOCRACY&#34;&gt;true democracy&lt;/a&gt;. Traditionally it&#39;s been relatively unfeasible because voters were not expected to be educated enough to make governmental decisions and because they didn&#39;t have the time or capability to regularly decide on issues. The spread of the Internet alleviates at least the latter problem, since millions of people can now simultaneously decide on any issue if they so desire.&lt;/p&gt;
&lt;p&gt;In the United States the best known deliberative system is the &lt;a href=&#34;http://en.wikipedia.org/wiki/Initiative&#34;&gt;initiative&lt;/a&gt; system found in some states, including California. It allows for issues to be put directly before the voters through the submission of sufficient signatures, and then allows the voters to pass or fail those issues, based on either &lt;a href=&#34;http://en.wikipedia.org/wiki/Plurality&#34;&gt;plurality&lt;/a&gt; (most votes), &lt;a href=&#34;http://en.wikipedia.org/wiki/Majority&#34;&gt;majority&lt;/a&gt; (at least 50% of votes), or else &lt;a href=&#34;http://en.wikipedia.org/wiki/Supermajority&#34;&gt;super majority&lt;/a&gt; (some percentage of votes in excess of 51%). In California, for example, 66% approval is required for new tax initiatives.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://lifewithalacrity.blogs.com/.shared/image.html?/photos/uncategorized/constitution_signing.jpg&#34; onclick=&#34;window.open(this.href, &#39;_blank&#39;, &#39;width=540,height=350,scrollbars=no,resizable=no,toolbar=no,directories=no,location=no,menubar=no,status=no,left=0,top=0&#39;); return false&#34;&gt;&lt;img width=&#34;200&#34; height=&#34;129&#34; border=&#34;0&#34; alt=&#34;Constitution_signing&#34; title=&#34;Constitution_signing&#34; src=&#34;https://lifewithalacrity.github.io/images/constitution_signing.jpg&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;The United States constitution defines a large and very complex deliberative system. It creates three bodies of government to support deliberation and voting, and uses a &lt;a href=&#34;http://en.wikipedia.org/wiki/Checks_and_Balances#Checks_and_balances&#34;&gt;checks and balances&lt;/a&gt; systems in order to allow different branches to have different effects upon a vote. The main voting is done by the legislature, which requires two &lt;em&gt;pluralities&lt;/em&gt; from two different groups of people to pass a vote. Then the executive branch has a singular opportunity to veto legislature, which then requires a &lt;em&gt;super majority&lt;/em&gt; (here, 66%) to override that veto. Once a law is established, the judicial department may by &lt;em&gt;plurality&lt;/em&gt; vote to declare that legislation unconstitutional, but that may be overcome by an even greater &lt;em&gt;super majority&lt;/em&gt; (typically, 66% of each legislature + 75% of the state governments) who want to &lt;a href=&#34;http://en.wikipedia.org/wiki/United_States_Constitution#Provisions_for_amendment&#34;&gt;amend the constitution&lt;/a&gt;.&amp;nbsp; &lt;/p&gt;
&lt;p&gt;The constitution also shows how deliberation can span beyond simple voting because of the fact that it includes specific rules for how to debate, when debate can be closed, etc. In today&#39;s very fractured congress, however, it&#39;s unclear if individuals ever are actually swayed by deliberations in the floor of the legislature, or if they&#39;ve already decided to follow their party lines or their specific interests, long before they entered the Capitol buildings. &lt;/p&gt;
&lt;p&gt;A smaller example of a true deliberative system, based on guiding discussions as much as holding votes, is found in &lt;a href=&#34;http://www.robertsrules.org/&#34;&gt;Robert&#39;s Rules of Order&lt;/a&gt;, a guide for conducting meetings. These rules detail explicit methods not just for voting, but also for the deliberation and discussion surrounding the voting. Various &lt;em&gt;majority&lt;/em&gt; and &lt;em&gt;minority&lt;/em&gt; votes can be taken to allow for certain actions.&lt;/p&gt;
&lt;p&gt;Because deciding directly upon ideas rather than just voting for representatives can have a greater effect upon a community, the deliberative systems may need to be more complex to avoid abuse, as evidenced by the complexities of the U.S. Government and Robert&#39;s Rule of Order. However, these very complexities can make these systems more prone to purposeful gaming. The benefits and deficits of more complex deliberative systems have not yet been fully studied, nor have there been as much analysis of &amp;quot;attacks&amp;quot; against them.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/consensus_1.jpg&#34;&gt;&lt;img width=&#34;150&#34; height=&#34;141&#34; border=&#34;0&#34; src=&#34;https://lifewithalacrity.github.io/images/consensus_1.jpg&#34; title=&#34;Consensus_1&#34; alt=&#34;Consensus_1&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;&lt;a name=&#34;consensus_systems&#34;&gt;&lt;/a&gt;&lt;strong&gt;&lt;a name=&#34;consensus_systems&#34;&gt;Consensus Systems:&lt;/a&gt;&lt;/strong&gt; In consensus systems people jointly come to a consensus as a group through group interactions. This sort of decision making theoretically avoids the &amp;quot;tyranny of the majority&amp;quot; and likewise can produce more informed decision making. It&#39;s a variant of the broader &lt;em&gt;deliberative systems&lt;/em&gt;, but one with more group and less individual power.&lt;/p&gt;
&lt;p&gt;One example of consensual selection is cabinet government as laid out under the &lt;a href=&#34;http://en.wikipedia.org/wiki/Westminster_system&#34;&gt;Westminster System&lt;/a&gt;. Wikipedia describes it as follows:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;em&gt; Members of the Cabinet are collectively seen as responsible for government policy. All Cabinet decisions are made by consensus, a vote is never taken in a Cabinet meeting. All ministers, whether senior and in the Cabinet, or junior ministers, must support the policy of the government publicly regardless of any private reservations. If a minister does not agree with a decision he, or she, can resign from the government; as did several British ministers over the 2003 Invasion of Iraq. This means that in the Westminster system of government the cabinet always collectively decides all decisions and all ministers are responsible for arguing in favour of any decision made by the cabinet.&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Consensus_decision-making#Quaker-based_consensus&#34;&gt;Quaker-based consensus&lt;/a&gt; offers a similar example. Herein a facilitator helps to identify disagreements and agreements to move a discussion forward until an end result is embraced by all individuals.&lt;/p&gt;
&lt;p&gt;As a final note, it&#39;s important to differentiate consensus from coercion. The end result of unanimity isn&#39;t the sole definition of a consensus system, nor is it entirely required. What is required is a more open and thoughtful selection process.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;&lt;a name=&#34;opinion_systems&#34;&gt;Opinion Systems&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img border=&#34;0&#34; alt=&#34;Poll&#34; title=&#34;Poll&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/poll.gif&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;Opinion systems are a clear subsidiary category to selection systems. An opinion system&#39;s main use is as a decision indicator, to show how people will decide or did decide in a representative system, a deliberative system, or both. Current opinion systems tend to be oriented toward actual votes, as opposed to more freeform selection systems (though the &lt;em&gt;delphic polling system&lt;/em&gt; shows a more freeform version of the category itself). Opinion systems tend to be push-based (meaning people are asked for their opinions rather than actively offering them), but this isn&#39;t required. &lt;/p&gt;
&lt;p&gt;All opinion systems tend to have the same general problem, which is figuring out how to use scientific means to determine the actual results of a decision. This means massaging respondent numbers to offset categories of people more or less likely to vote to try and generate the actual results. For example, one 1998 poll showed that 62% of Republicans were absolutely certain they were going to vote, while only 51% of Independents could say the same. This means that every Republican voter a poll contacted in that year might have been weighted about 1.2x over every Independent contacted. Of course the actual calculations are much more complex than that, since they tend to depend upon traditional voter turnout and lots of analysis, but the core idea is sound, which is that every polled individual should &lt;u&gt;not&lt;/u&gt; be considered equal.&lt;/p&gt;
&lt;p&gt;All opinion system results tend to be rated with &lt;a href=&#34;http://en.wikipedia.org/wiki/Margin_of_error&#34;&gt;margins of error&lt;/a&gt;. The margin of error is a percent spread which the poll is expected to be within, 90-99% of the time (depending on how conservative of a confidence rating is given). If a poll shows that a politician is expected to take 48% of the vote, for example, and the margin of error is 4%, that means he is expected to take 44-52% of the vote with 90-99% surety. Margins of error are typically given much greater importance in the modern media than they should, as they&#39;re calculated solely based upon the total number of respondents to a poll.&lt;/p&gt;
&lt;p&gt;There are two general categories of opinion systems: &lt;em&gt;pre-voting (subjective) polling systems&lt;/em&gt; and &lt;em&gt;post-voting (objective) polling systems&lt;/em&gt;. A different type of opinion system, &lt;em&gt;delphic polling&lt;/em&gt;, which could apply to either pre- or post-voting systems is also covered. Polling systems not directly related to selection systems are covered later, as &lt;em&gt;subjective rating systems&lt;/em&gt;, since they tend to have issues very different from other polling systems, as their goal isn&#39;t to try and match the &amp;quot;true&amp;quot; number of an actual vote.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a name=&#34;pre-voting_polling_systems&#34;&gt;Pre-voting Polling Systems:&lt;/a&gt;&lt;/strong&gt; These are polls made before a vote is cast. They&#39;re often called &amp;quot;opinion polls&amp;quot; and tend to be conducted via phone. They try and isolate &amp;quot;likely voters&amp;quot; and determine how they will vote. This question of voter likelihood is one of the first issues with a pre-voting system, because there&#39;s no guarantee that the polled people will actually later vote. Likewise, pre-voting systems have to accommodate &amp;quot;undecided voters&amp;quot; and the fact that no voter has ever truly made up their mind until they cast their final ballot. Unlike post-voting polling systems, pre-voting systems also have considerable more possibility for bias (which is not accounted for by margins of error), based upon how questions are asked, in what order, and with what additional text.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;img border=&#34;0&#34; alt=&#34;Exitpoll&#34; title=&#34;Exitpoll&#34; src=&#34;http://lifewithalacrity.blogs.com/photos/uncategorized/exitpoll.png&#34; style=&#34;margin: 5px 5px 0px 0px; float: left;&#34; /&gt; &lt;a name=&#34;post-voting_polling_systems&#34;&gt;Post-voting Polling Systems:&lt;/a&gt;&lt;/strong&gt; These are polls taken after a vote is cast. They&#39;re typically called &amp;quot;exit polls&amp;quot;, as most are conducted as people are leaving a &amp;quot;polling&amp;quot; station (where they cast a vote). One would expect these to be much more reliable than pre-voting polls, but as the &lt;a href=&#34;http://en.wikipedia.org/wiki/2004_U.S._presidential_election_controversy_and_irregularities#Exit_polls&#34;&gt;2004 U.S. Presidential Election&lt;/a&gt; showed, exit polls can be wildly inaccurate.&lt;/p&gt;
&lt;p&gt;One of the problems with post-voting polling systems, shared with pre-voting systems, is that the results must be manipulated to make sure that respondents to the poll match the percentages of those constituencies in the overall populations. For example, in the 2004 exit polls it appears that women were initially overrepresented in exit polls, and because of increased black turnout it appears that blacks were underrepresented in the exit polls. It can easily be seen how either of these misrepresentations could cause notable changes in an exit poll result.&lt;/p&gt;
&lt;p&gt;When conducted &amp;amp; matched correctly, exit polls are supposed to be &lt;a href=&#34;http://en.wikipedia.org/wiki/2004_U.S._presidential_election_controversy%2C_exit_polls#Reliability_of_Exit_Polls&#34;&gt;quite reliable&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://lifewithalacrity.blogs.com/.shared/image.html?/photos/uncategorized/delphiamphitheatre.jpg&#34; onclick=&#34;window.open(this.href, &#39;_blank&#39;, &#39;width=458,height=317,scrollbars=no,resizable=no,toolbar=no,directories=no,location=no,menubar=no,status=no,left=0,top=0&#39;); return false&#34;&gt;&lt;img width=&#34;200&#34; height=&#34;138&#34; border=&#34;0&#34; alt=&#34;Delphiamphitheatre&#34; title=&#34;Delphiamphitheatre&#34; src=&#34;https://lifewithalacrity.github.io/images/delphiamphitheatre.jpg&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt; &lt;a name=&#34;delphic_polling_systems&#34;&gt;Delphic Polling Systems:&lt;/a&gt;&lt;/strong&gt; An interesting polling method applicable to all sorts of opinion systems is the &amp;quot;delphi poll&amp;quot;. This is a specific method of polling which is &lt;em&gt;iterative&lt;/em&gt; and &lt;em&gt;anonymous&lt;/em&gt; and which supports &lt;em&gt;confidence ratings&lt;/em&gt; and &lt;em&gt;feedback&lt;/em&gt;. The general idea is that people are polled on a question using not just binary responses, but a full confidence rating (e.g., you would state that you are 60% sure that Bush would be elected, rather than stating that you think Bush would be elected). After polls are collected, the anonymous results--or at least a summary of those results--are shared with the participants, who then poll again. This iterative process continues until a consistent answer is settled upon. By incorporating feedback into the polling process there&#39;s the possibility for greatly increased reliability.&lt;/p&gt;
&lt;p&gt;In some ways delphic polling systems can be seen as an analogy to &lt;em&gt;consensus systems&lt;/em&gt;, since both involve more iterative processes that eventually result in a more commonly-held decision.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;&lt;a name=&#34;comparison_systems&#34;&gt;Comparison Systems&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Comparison systems allow individual items to be measured up against each other. There are three general categories: &lt;em&gt;comparison ranking systems&lt;/em&gt;, which are largely objective and which typically rank people; and &lt;em&gt;comparison rating systems&lt;/em&gt;, which more often mix subjective and objective opinions, and which more frequently rate things; and &lt;em&gt;reputation rating systems&lt;/em&gt;, which again tend to rank people, but also have a subject and objective mix. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a name=&#34;ranking_systems&#34;&gt;Comparison Ranking Systems:&lt;/a&gt;&lt;/strong&gt; In a ranking system, items in a hierarchy (most frequently people) rise or fall based upon specific, objective, and well-known rules. This is the heart of most multiplayer competitive systems.&lt;/p&gt;
&lt;p&gt;&lt;a onclick=&#34;window.open(this.href, &#39;_blank&#39;, &#39;width=500,height=336,scrollbars=no,resizable=no,toolbar=no,directories=no,location=no,menubar=no,status=no,left=0,top=0&#39;); return false&#34; href=&#34;http://lifewithalacrity.blogs.com/.shared/image.html?/photos/uncategorized/chess.jpg&#34;&gt;&lt;img width=&#34;200&#34; height=&#34;134&#34; border=&#34;0&#34; src=&#34;https://lifewithalacrity.github.io/images/chess.jpg&#34; title=&#34;Chess&#34; alt=&#34;Chess&#34; style=&#34;margin: 5px 5px 0px 0px; float: left;&#34; /&gt;&lt;/a&gt; The &lt;a href=&#34;http://en.wikipedia.org/wiki/ELO_rating_system&#34;&gt;ELO System&lt;/a&gt; is an example of a ranking system used for two-player games, and is used by the U.S. Chess Federation. Days of Wonder uses a &lt;a href=&#34;http://www.gangoffour.com/index.php?t=content&amp;amp;sub=ranking&#34;&gt;multiplayer variant of the ELO system&lt;/a&gt; for their online games. Each system builds a simple distribution of player ratings around a norm (typically 1500 points), then awards or deducts points based upon wins and losses, with the total sum of all points in the system staying constant. Players are then ranked according to their comparative scores. &lt;/p&gt;
&lt;p&gt;There are flaws in ranking systems like ELO. For example, two players could collude, with one purposefully throwing games so that his opponent could increase his ranking. Alternatively if a player gets a few lucky victories against good opponents, his rating might temporarily skyrocket above its normative value. However, these tend to be well-known and well-researched problems.&lt;/p&gt;
&lt;p&gt;These are numerous other ranking systems which are used for competitions, from double-elimination seeded tournaments (e.g., a tennis tournament) to ranked comparisons based upon win-loss ratios (e.g., baseball standings). Objective rankings are also (less commonly) used to rank items, such as a ranking of cars based upon safety ratings.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://lifewithalacrity.blogs.com/.shared/image.html?/photos/uncategorized/usmc_enlisted_rank_structure.jpg&#34; onclick=&#34;window.open(this.href, &#39;_blank&#39;, &#39;width=640,height=476,scrollbars=no,resizable=no,toolbar=no,directories=no,location=no,menubar=no,status=no,left=0,top=0&#39;); return false&#34;&gt;&lt;img width=&#34;200&#34; height=&#34;148&#34; border=&#34;0&#34; alt=&#34;Usmc_enlisted_rank_structure&#34; title=&#34;Usmc_enlisted_rank_structure&#34; src=&#34;https://lifewithalacrity.github.io/images/usmc_enlisted_rank_structure.jpg&#34; style=&#34;margin: 5px 5px 0px 0px; float: right;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Most ranking systems create a hierarchy of positive rankings (e.g., &amp;quot;best chess players ever&amp;quot;). However, a hierarchy of negative rankings may also most be used, most commonly based on a negative criteria (e.g., &amp;quot;biggest Player Killers (PKers)&amp;quot;). In addition, either direction of ranking can use threshold systems to mark positive or negative rankings that meet a certain criteria. A positive threshold might be a &amp;quot;Grand Master&amp;quot; ranking threshold for anyone with a Chess rating of 2700, while a negative threshold might be a &amp;quot;Player Killer&amp;quot; ranking threshold, for with sufficient &amp;quot;accidental&amp;quot; PKs.&lt;/p&gt;
&lt;p&gt;Ranking systems are somewhat removed from the other collective choice systems listed here, since there&#39;s isn&#39;t a collaborative decision, only a collective result. However their problems &amp;amp; results remain closely related to the more collective rating and reputation systems, hence their inclusion. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a onclick=&#34;window.open(this.href, &#39;_blank&#39;, &#39;width=373,height=239,scrollbars=no,resizable=no,toolbar=no,directories=no,location=no,menubar=no,status=no,left=0,top=0&#39;); return false&#34; href=&#34;http://lifewithalacrity.blogs.com/.shared/image.html?/photos/uncategorized/stars_1.gif&#34;&gt;&lt;img width=&#34;200&#34; height=&#34;128&#34; border=&#34;0&#34; src=&#34;https://lifewithalacrity.github.io/images/stars_1.gif&#34; title=&#34;Stars_1&#34; alt=&#34;Stars_1&#34; style=&#34;margin: 0px 5px 5px 0px; float: left;&#34; /&gt;&lt;/a&gt; &lt;a name=&#34;rating_systems&#34;&gt;Rating Systems:&lt;/a&gt;&lt;/strong&gt; In a rating system, the value of individual items (most frequently goods) rise or fall based upon the largely subjective judgment of individual users.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.amazon.com&#34;&gt;Amazon&lt;/a&gt; and &lt;a href=&#34;http://www.netflix.com&#34;&gt;Netflix&lt;/a&gt; are two examples of stores which provide subjective rating systems. Individual users rate items from 1 to 5 stars, then an average user rating is calculated. &lt;a href=&#34;http://www.boardgamegeek.com&#34;&gt;BoardGameGeek&lt;/a&gt; offers a slightly different example because it not only lets users rate individual items, but also ranks items against each other based upon those ratings.&lt;/p&gt;
&lt;p&gt;Flaws in these systems are similar to those in ranking systems: low numbers of ratings producing bad rankings, and individual users purposefully biasing ratings. Some mathematical methods may be used to smooth out these issues, among them &lt;em&gt;bayesian averages&lt;/em&gt;, which give ratings weight based upon total number of ratings for an item.&lt;/p&gt;
&lt;p&gt;The Stock Market offers an example of a different sort of rating system, because there&#39;s theoretically some objective basis to it. In a perfect Stock Market system, stock prices are based upon a solid cost analysis, such as a multiplier on yearly revenues or profits. However, as the Internet bubble of the late 1990s conclusively showed, there&#39;s also a high irrational component to stock purchases: thus subjective and objective views are combined in the rating (cost) of a stock.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a name=&#34;reputation_systems&#34;&gt;Reputation Systems:&lt;/a&gt;&lt;/strong&gt; Finally, reputation systems are very similar to ranking systems: items in a hierarchy (most frequently people) rise or fall based upon specific and well-known rules. However, unlike true ranking systems, reputation systems instead base their rules for rise and fall upon other user feedback. &lt;/p&gt;
&lt;p&gt;The goal of a reputation system is ultimately to create a trust metric that often allows different users access to different powers. We&#39;ll be covering reputation systems a bit more thoroughly in a couple of weeks. &lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;There are a variety of ways to measure the collective choices of a large group of people. We&#39;ve outline nine here: representative, deliberative, and consensus selection systems; ranking, rating, and reputation comparison systems; and three varieties of opinion systems. When developing social software it is important to understand the difference between these broad categories of systems and to use lessons already learned from the appropriate category in your own social software designs.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Related articles from this blog:&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://lifewithalacrity.github.io/2005/12/collective_choi.html&#34;&gt;2005-12: Collective Choice: Rating Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://lifewithalacrity.github.io/2006/01/ranking_systems.html&#34;&gt;2006-01: Collective Choice: Competitive Ranking Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://lifewithalacrity.github.io/2006/08/using_5star_rat.html&#34;&gt;2006-08: Using 5-Star Rating Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;https://lifewithalacrity.github.io/2007/01/collective_choi.html&#34;&gt;2007-01: Experimenting with Ratings&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;p&gt;Related articles from Shannon Appelcline&#39;s &lt;a href=&#34;http://www.skotos.net/articles/show-column.phtml?colname=TTnT_&#34;&gt;Trials, Triumphs &amp;amp; Trivialities:&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_192.phtml&#34;&gt;#192: Managing User Creativity, Part One&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_193.phtml&#34;&gt;#193: Managing User Creativity, Part Two&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_196.phtml&#34;&gt;#196: Collective Choice: Ratings, Who Do You Trust?&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_198.phtml&#34;&gt;#198: Collective Choice: More Thoughts About Ratings&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;footer&gt;&lt;h3&gt;Comments&lt;/h3&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;You seemed to leave out consensus methods such as the Quakers and facilitated decision making processes.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://www.imaginal.nl&#34;&gt;Jon Jenkins&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2005-12-02T00:08:19-07:00&#34;&gt;2005-12-02T00:08:19-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;My intent was more to outline the broad categories of collective choice, then to give all the possible examples within each category. Both consensus methods and facilitated decision making processes fall under the &#34;deliberation systems&#34; category.
It is my hope in future articles to dig deeper into each broad category and show more examples of types. Next week hopefully will be comparison rating systems.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://www.LifeWithAlacrity.com&#34;&gt;Christopher Allen&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2005-12-02T00:39:54-07:00&#34;&gt;2005-12-02T00:39:54-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;You should take a look at Jeff Vails&#39; writings on rhizome networks as counter weights to hierchical power structures. http://www.jeffvail.net/2005/08/rhizome-communication-and-our-one-time.html
There&#39;s a connection between your ideas and his that I can&#39;t quite articulate because my mind is focussed elsewhere at the moment. But I shall return.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://karavans.typepad.com/&#34;&gt;peter&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2005-12-07T17:17:04-07:00&#34;&gt;2005-12-07T17:17:04-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;I&#39;ve made some changes to this blog entry, adding consensus systems under deliberation systems, added reputation systems as a type of opinion system, simplifying some of the category names, and added some links.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://www.LifeWithAlacrity.com&#34;&gt;Christopher Allen&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2005-12-08T13:13:09-07:00&#34;&gt;2005-12-08T13:13:09-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;This is an excellent summary, but I feel like I want a comparison table of some kind.
Would &#34;information futures market&#34; or the like be a Delphic system? It sure seems more like a consensus measuring device to me.
One of the most interesting things about all of these systems is the side effects they have on the decisions made. Binary Votes polorize options. Information Futures Markets encourage results manipulation (by design!)
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://www.fudco.com/habitat&#34;&gt;F. Randall Farmer&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2005-12-08T14:24:25-07:00&#34;&gt;2005-12-08T14:24:25-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;This is a wonderful overview.  Thank you for putting it together!
I&#39;d like to suggest one small correction to the description of Condorcet voting, which says &#34;it is much harder on the voters, who have to rank every single candidate&#34;.  In a Condorcet system, the voters don&#39;t have to rank every candidate; they can rank as many or as few as they want (just like IRV), and they can even rank candidates equally (unlike IRV).  The Wikipedia article entitled &#34;Condorcet method&#34; describes this.
Regarding a comparison of these systems, i&#39;ve tried to illustrate the behaviour of plurality, approval, IRV, and Condorcet elections at http://zesty.ca/voting/ , and i hope you will find my method and results interesting.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://wolog.net/&#34;&gt;Ping&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2005-12-14T01:33:04-07:00&#34;&gt;2005-12-14T01:33:04-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;The problem of choice is outline very well in the book &#39;The Paradox of Choice&#39; by Bary Schwartz.  It provides a great description of some of the other challenges of multi-party elections. I found it very interesting when it got into the psychology of choice regret, and alternative choices.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://www.como.typepad.com&#34;&gt;Randal&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2005-12-14T08:48:12-07:00&#34;&gt;2005-12-14T08:48:12-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;I agree this is a very interesting article!  I notice an (intentional?) lack of coverage of most of the dominant and mature decision-making institutions in our society: markets in general (and specifically institutions such as exchanges -- mentioned briefly -- auctions, fixed-price retail stores vs. haggling bazaars, and so on), risk sharing institutions (e.g. insurance), hierarchical organizations based on the employer-employee and boss-subordinate relationships, the judiciary based on written law, opinion, and precedent, and so on.  Of course, it would be very ambitious to tackle these, but it would at least be interesting to try to categorize them according to your taxonomy.  Perhaps they are so complex they often fall into multiple categories.
The systems you describe abstract a vast amount of diffuse and often tacit information pertinent to a decision into a numerical result. Most economists (particularly Austrian school) would argue that at least on a large scale prices generated by auctions and exchanges generally do this better than other collective decision making systems.  On the other hand, many systems for collective choice don&#39;t work via an abstract numerical intermediary (e.g. the opinion and precedent system of the judiciary).
I suggest that the most important factor in these institutions is the costs involved in eliciting diffuse knowledge and lossily compressing it into a decision which is hopefully nevertheless a reasonably accurate reflection of that diffuse information.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://unenumerated.blogspot.com&#34;&gt;nick szabo&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2005-12-15T11:49:05-07:00&#34;&gt;2005-12-15T11:49:05-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;At least part of the reason why I didn&#39;t strongly highlight markets is that the definition of &#34;collective choice&#34;, at least as far as how many academics use it, is all the methods that groups make decisions EXCEPT markets. For instance, from http://www.bsos.umd.edu/umccc/ &#34;models of individual rational choice behavior to study non-market social phenomena&#34;.
However, another definition is http://www.artsci.wustl.edu/~polisci/calvert/PolSci505/ &#34;social choice theory and noncooperative game theory&#34;, and I might say that markets fall into a subset of noncooperative game theory.
Personally I&#39;m not sure why most academics exclude markets, thus I do briefly mention the Stock Market as a subcategory of comparison system. I hope to mention them more in other future articles, in particular, prediction markets as a subcategory of Polling Systems.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://www.LifeWithAlacrity.com&#34;&gt;Christopher Allen&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2005-12-15T12:20:55-07:00&#34;&gt;2005-12-15T12:20:55-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;Will you cover, in a future post the forced choice vote? I&#39;d welcome learning about more ways that a group can discuss alternatives (online or in person) then either vote in a priority way (first choice, second choice, etc.)
and/ or choose to vote in a first round for &#34;x&#34; number of options then limit the conversation to those options, then perhaps do a forced choice vote.
In other words what are some hybrid methods that you
(amazing) Christopher might craft that combine:
A. Thoughtful conversation to hear all ideas/opinions -
without the downsides of :
• being prolonged to the point it pains participants
and/or
• being dominated by one or two &#34;strong&#34; or verbose personalities
.....with
B. Stages of voting closure that enable all participants
to see &#39;we&#39; are moving to the top choice(s) of the group
- another fan of life with alacrity
p.s. In addition to the aforementioned, brilliant, The Paradox of Choice, read Smart Choices by Howard Raiffa
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://profile.typekey.com/kareanderson/&#34;&gt;Kare&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2008-12-30T13:37:36-07:00&#34;&gt;2008-12-30T13:37:36-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;/footer&gt;
&lt;p class=&#34;previous&#34;&gt;&lt;a href=&#34;https://lifewithalacrity.github.io/previous/2005/12/systems_for_col.html&#34; rel=&#34;syndication&#34;&gt;orginal layout&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>