<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics on Life With Alacrity</title>
    <link>http://www.lifewithalacrity.com/tags/statistics/index.xml</link>
    <description>Recent content in Statistics on Life With Alacrity</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://www.lifewithalacrity.com/tags/statistics/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Collective Choice: Experimenting with Ratings</title>
      <link>http://www.lifewithalacrity.com/2007/01/collective_choi.html</link>
      <pubDate>Mon, 01 Jan 2007 22:38:15 -0700</pubDate>
      
      <guid>http://www.lifewithalacrity.com/2007/01/collective_choi.html</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-size: 0.7em;&#34;&gt;by Christopher Allen &amp;amp; Shannon Appelcline&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;[This is the fourth in a series of articles on &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html&#34;&gt;collective choice&lt;/a&gt;, co-written by my collegue &lt;a href=&#34;http://www.skotos.net/about/staff/shannon_appelcline&#34;&gt;Shannon Appelcline&lt;/a&gt;. It will be &lt;a href=&#34;http://www.skotos.net/articles/TTnT_179.phtml&#34;&gt;jointly posted&lt;/a&gt; in Shannon&#39;s &lt;a href=&#34;http://www.skotos.net/articles/TTnT.shtml&#34;&gt;Trials, Triumphs &amp;amp; Trivialities&lt;/a&gt; online games column at &lt;a href=&#34;http://www.skotos.net/&#34;&gt;Skotos&lt;/a&gt;.]&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Last year in &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/collective_choi.html&#34;&gt;Collective Choice: Rating Systems&lt;/a&gt; we took a careful look at eBay and other websites that collect ratings, and used those systems as examples to highlight a number of theories about how to make rating systems more useful.&lt;/p&gt;
&lt;p&gt;We suggested three main methods for improving rating systems:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Granular Ratings:&lt;/strong&gt; Based on the clumping of ratings to high values, we believed that ratings could be made more useful by increasing the size of a rating scale. Most rating scales are 5-point ranges, so we suggested a 10-point range instead.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distinct Ratings:&lt;/strong&gt; Raters can be somewhat arbitrary in how they rate items, varying both from each other and even from themselves (usually over multiple sessions). Thus we believed that providing explicit statements of what each number meant could improve ratings.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistical Ratings:&lt;/strong&gt; Finally we stated that in low volumes ratings could be biased by various quirks of data entry, either malevolent or not, and that ratings could be improved with strong statistical methods being used to polish up data and automatically keep &amp;quot;bad&amp;quot; data in line with &amp;quot;good&amp;quot;.&lt;/p&gt;
&lt;p&gt;In the year since we wrote that article we&#39;ve decided to practice what we preach and have rolled out an entirely new rating system called &lt;a href=&#34;http://index.rpg.net&#34;&gt;The RPGnet Gaming Index&lt;/a&gt;. We&#39;ve applied all of the above theories and thus far it looks like they&#39;re not only working, but that they&#39;re actually providing better rating systems than previous ones we&#39;ve used at the RPGnet site.&lt;/p&gt;
&lt;p&gt;In this article we&#39;re going to step through the data we&#39;ve collected from this experience and see how it applies to our theory: first by looking at our previous RPGnet rating system, then by looking at the new system, and finally by by examining the data from these two systems and comparing their results. We&#39;ve also run into some unexpected troubles along the way, and we&#39;ll talk about that too.&lt;/p&gt;
&lt;h3&gt;The RPGnet Reviews System&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.rpg.net&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/rpgnetlogo_1.gif&#34; title=&#34;Rpgnetlogo_1&#34; alt=&#34;Rpgnetlogo_1&#34; style=&#34;margin: 0px 5px 5px 0px; float: left;&#34; /&gt;&lt;/a&gt;
RPGnet is our gaming site for tabletop roleplaying—games like &lt;em&gt;Dungeons &amp;amp; Dragons&lt;/em&gt; and &lt;em&gt;Vampire: The Masquerade&lt;/em&gt;. We purchased it in 2001 from the original owners. One of the benefits of RPGnet was that it had a very large community. As of today it sports one of the top-100 forums on the Internet, with over 1000 simultaneous users regularly logging in. However, because of its maturity, we also inherited many existing systems.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.rpg.net/reviews/archive/9/9971.phtml&#34;&gt;&lt;img border=&#34;0&#34; alt=&#34;Rpgnet_review_summary_1&#34; title=&#34;Rpgnet_review_summary_1&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/rpgnet_review_summary_1.jpg&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
One of these was the &lt;a href=&#34;http://www.rpg.net/reviews/&#34;&gt;RPGnet Reviews System&lt;/a&gt; which gave individual users the ability to review gaming products—mostly role-playing games, but also board games, books, DVDs, and a smattering of related products.&lt;/p&gt;
&lt;p&gt;Most of these reviews are submitted by average readers who just want to talk about a product that they like (or don&#39;t), though a fair percentage are instead submitted by staff reviewers. (Overall at least 26% of our reviews are based on publisher &amp;quot;comp&amp;quot; copies, and thus may be considered largely professional, while the other 74% may or may not be.) The large community size of RPGnet applies to the Reviews System as well: currently it features 8,505 published reviews.&lt;/p&gt;
&lt;p&gt;Looking at the RPGnet Reviews through our three filters we find the following:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Granularity.&lt;/strong&gt; The ratings from our existing reviews aren&#39;t as granular as we&#39;d like. We have a theoretical scale of 2-10, but that&#39;s based upon a Style rating of 1-5 and a Substance rating of 1-5.&lt;/p&gt;
&lt;table cellpadding=&#34;5&#34;&gt;
&lt;thead&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;&lt;strong&gt;Rating&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Style&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Substance&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;%&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;81&lt;/td&gt;
&lt;td&gt;225&lt;/td&gt;
&lt;td&gt;1.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;732&lt;/td&gt;
&lt;td&gt;651&lt;/td&gt;
&lt;td&gt;8.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;2364&lt;/td&gt;
&lt;td&gt;1777&lt;/td&gt;
&lt;td&gt;24.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;3618&lt;/td&gt;
&lt;td&gt;3525&lt;/td&gt;
&lt;td&gt;42.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1709&lt;/td&gt;
&lt;td&gt;2326&lt;/td&gt;
&lt;td&gt;23.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt; Approximately 90% of raters rate only with values of 3-5, and thus our scale is more limited than the 2-10 range would indicate. 42.9% of reviews further rate Style and Substance exactly the same, suggesting that not everyone sees a difference between these two elements. On the whole this scale isn&#39;t as a bad as a singular 5-point scale, but it also isn&#39;t a real 10-point scale, and the two orthogonal types of comparison don&#39;t necessarily provide a coherent description of a product.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distinctiveness.&lt;/strong&gt; Conversely, the review ratings are fairly distinct because the Review System provides an explanation of what each rating number means. For example the five Substance ratings are: I Wasted My Money (1); Sparse (2); Average (3); Meaty (4); Excellent(5). The descriptions could be better, but hopefully they connect to some users in meaningful ways, and help them to rate consistently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistics.&lt;/strong&gt; Our review ratings have no statistical basis. These values are used entirely unfiltered.&lt;/p&gt;
&lt;p&gt;On the whole, the existing RPGnet Reviews embodied slightly less than half of what we wanted to see in a rating systems: some improvement over a simple 5-point scale; some effort put into making individual ratings distinct; and nothing statistical.&lt;/p&gt;
&lt;p&gt;There is room for improvement, however, as we&#39;ll see when we analyze this system more fully.&lt;/p&gt;
&lt;h3&gt;The RPGnet Gaming Index&lt;/h3&gt;
&lt;p&gt;Our newer system is the &lt;a href=&#34;http://index.rpg.net/&#34;&gt;RPGnet Gaming Index&lt;/a&gt;. It doesn&#39;t supersede our Reviews, but instead offers a complementary look at the roleplaying field. The Index is essentially an RPG industry database. It contains individual entries for many different gamebooks—currently 5248—and allows registered users to rate each of them. Those ratings are then turned into averages by various mathematical formulas on a nightly basis and the roleplaying games in our index are then ranked.&lt;/p&gt;
&lt;p&gt;The large size of RPGnet has allowed us to very quickly turn our ideas of a Gaming Index into reality. Just six months after release we have:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;5248 well-written Index entries &lt;/li&gt;
&lt;li&gt; 5908 different editions&lt;/li&gt;
&lt;li&gt; 4240 authors&lt;/li&gt;
&lt;li&gt; 4478 covers&lt;/li&gt;
&lt;li&gt; 360 different game systems&lt;/li&gt;
&lt;li&gt; 345 series&lt;/li&gt;
&lt;li&gt; 10142 individual ratings&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;Most of the ratings are clumped around the best and worst games, with many less popular games unrated as of yet. Four different items have at least 80 ratings each (&lt;em&gt;Call of Cthulhu&lt;/em&gt;, &lt;em&gt;Exalted&lt;/em&gt;, &lt;em&gt;Nobilis&lt;/em&gt;, and &lt;em&gt;Unknown Armies&lt;/em&gt;). Our average rating is 6.79. Ratings above 7.82 are in the 99th
percentile, ratings above 7.21 are in the 90th percentile, and ratings
below 6.53 are beneath the 10th percentile.&lt;/p&gt;
&lt;p&gt;(For more info on the creation of the RPG Index, and how to encourage user generated content, see Shannon&#39;s articles, &amp;quot;Managing User Creativity&amp;quot;, &lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_192.phtml&#34;&gt;Part One&lt;/a&gt; and &lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_193.phtml&#34;&gt;Part Two&lt;/a&gt;.) &lt;/p&gt;
&lt;p&gt;The RPGnet Index also handles some unusual situations, such as when a game book contains other game books as part of an anthology or compilation. For instance, the 8-book compilation &lt;a href=&#34;http://index.rpg.net/display-entry.phtml?mainid=64&#34;&gt;In Search of Adventure&lt;/a&gt; has a composite rating of &lt;a href=&#34;http://index.rpg.net/display-entry-ratings.phtml?mainid=64&#34;&gt;6.57&lt;/a&gt; which is partially based upon the individual adventures that make it up.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Granularity:&lt;/strong&gt; The first thing we did was provide a 10-point scale for this new system.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distinctiveness:&lt;/strong&gt; We also made sure each point of the scale was clearly defined. Currently the points of our scale are: Worthless (1), Poor (2), Some Flaws (3), Almost Average (4), Average (5), Above Average (6), Good (7), Very Good (8), Outstanding (9), and One of the Best Ever (10).&lt;/p&gt;
&lt;p&gt;We made some mistakes in our original release of our &amp;quot;distinctive&amp;quot; titles, and we discovered this had real effects on the user input, telling us that these title labels &lt;em&gt;are&lt;/em&gt; meaningful to users.&lt;/p&gt;
&lt;p&gt;First, we initially labeled 6 as &amp;quot;average&amp;quot;, to mirror the rating system for our existing Reviews, rather than setting 5 to be average. But as we noted in our first article, people like to be nice, and thus they tend to rate on the good side of a scale. Changing the label for our definition of average from 6 to 5 has slowly started dropping the average of all ratings down as a result (providing more breadth, a topic we&#39;ll talk about more shortly).&lt;/p&gt;
&lt;p&gt;Second, two of our original distinctive titles were at odds with the others. Our original &amp;quot;2&amp;quot; value said that the game had &amp;quot;a few useful elements&amp;quot; and our original &amp;quot;9&amp;quot; value said that it was the &amp;quot;best of the year&amp;quot;. The 2 was much more specific than any of our other terms and the 9 created a comparative query that was very different from anything else. Overall our ratings conformed to a bell curve centered between 6 and 7, but we saw very clear dropouts in our curve at 2 and 9, telling us that we&#39;d made mistakes in those terms, and that people were less willing to use them as a result. Since we&#39;ve made the change to our current set of titles those two discontinuities have disappeared.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistics.&lt;/strong&gt; Finally, we fully integrated statistics into our new Index by using two main methods: bayesian weights and trust.&lt;/p&gt;
&lt;p&gt;We explained bayesian weights pretty fully in our previous article. Here&#39;s what we said then:&lt;/p&gt;
&lt;blockquote&gt;
&lt;table width=&#34;90%&#34; border=&#34;1&#34; cellpading=&#34;3&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;p&gt;
The idea behind a bayesian average is that you normalize ratings by pushing them toward the average rating for your site, and you do that more for items with fewer ratings than those with more ratings. The basic formula looks like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
b(r) = [ W(a) * a + W(r) * r ] / (W(a) + W(r)]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;r = average rating for an item&lt;/p&gt;
&lt;p&gt;W(r) = weight of that rating, which is the number of ratings&lt;/p&gt;
&lt;p&gt;a = average rating for your collection&lt;/p&gt;
&lt;p&gt;W(a) = weight of that average, which is an arbitrary number, but should be higher if you generally expect to have more ratings for your items; 100 is used here, for a database which expects many ratings per item&lt;/p&gt;
&lt;p&gt;b(r) = new bayesian rating &lt;/p&gt;
&lt;p&gt;Say three &amp;quot;shill&amp;quot; users had come onto your site and rated a brand new indie film a &amp;quot;10&amp;quot; because the producer asked them to. However, you use a bayesian average with a weight of 100, and thus 3 ratings won&#39;t move the movie very far from the average site rating of 6.50:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
b(r) = [100 * 6.50 + 3 * 10] / (100 + 3)&lt;/code&gt;&lt;code&gt;&lt;br /&gt;b(r) = 680 / 103&lt;/code&gt;&lt;code&gt;&lt;br /&gt;b(r) = 6.60
&lt;/code&gt;
&lt;/p&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/blockquote&gt;
&lt;p&gt;We implemented bayesian weights exactly as we&#39;d detailed, but with a lower weight of 25. Since then we&#39;ve accrued over 10,000 ratings in the database, and we can probably start thinking about cranking that weight up, another topic we&#39;ll return to.&lt;/p&gt;
&lt;p&gt;Our trust-based algorithms suggest that some ratings are better than others, and should thus be more trusted (and thus more weighted when we calculate the average rating of an item). Though bayesian weights have been used before, we&#39;re not aware of other systems that weight ratings based on trust. &lt;/p&gt;
&lt;p&gt;The calculation of trust is very simple:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
Weight = 0 if #ratings(user) &amp;lt;= 2&lt;/code&gt;&lt;code&gt;&lt;br /&gt;Otherwise Weight = #ratings(user) / 50 to a maximum of 2&lt;/code&gt;&lt;code&gt;&lt;br /&gt;Weight *= 2, to a maximum of 4, if the user included a comment
&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;This was based on the idea that the average good rater would rate 25 different items and the average great rater would rate at least 50. Additionally, we believed that ratings with comments were more likely to be thoughtful than those without.&lt;/p&gt;
&lt;p&gt;That, overall, is a quick picture of what we&#39;ve done with the RPGnet Gaming Index. Some of these ideas were laid out from the start, and others have been tuned as we progressed. &lt;/p&gt;
&lt;p&gt;So how did we do, particularly in comparison to our existing RPGnet Reviews System?&lt;/p&gt;
&lt;h3&gt;The Comparison&lt;/h3&gt;
&lt;p&gt;One of our goals in improving rating systems has been to widen the range of possible input. As we noted earlier we discovered that 90% of our RPGnet Reviews Ratings were in the 3-5 range, and only 10% in the 1-2 range.&lt;/p&gt;
&lt;p&gt;Generally, we can measure the success of widening a range by seeing whether the average rating of a database moves toward the &lt;em&gt;true&lt;/em&gt; average. For the purposes of a 10-point scale from 1-10, that&#39;s a desired value of 5.5. That generally means we&#39;re looking for our average rating to &lt;em&gt;decrease&lt;/em&gt; because people tend to rate high.&lt;/p&gt;
&lt;p&gt;The following table compares the average results of Reviews ratings and Index ratings.&lt;/p&gt;
&lt;center&gt;&lt;table cellpadding=&#34;3&#34; border=&#34;1&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Database&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Average&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Converted Reviews&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;7.25&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Massaged Reviews&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;7.29&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Unweighted Index&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;7.10&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Weighted Index&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;6.78&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;p&gt;Here&#39;s what the categories in the above chart represent:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Converted Reviews:&lt;/strong&gt; The Style + Substance of the Reviews, converted from its 2-10 scale to a 1-10 scale: &lt;/p&gt;
&lt;p&gt;&lt;code&gt;
$rating = avg($style) + avg($substance);&lt;/code&gt;&lt;br /&gt;&lt;code&gt;$rating = ($rating * 1.125) - 1.25;
&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Massaged Reviews:&lt;/strong&gt; The Style + Substance of the Reviews, with Substance given double weight over Style because we think that more closely reflects the intentions of the reviewer, converted from its 2-10 scale to a 1-10 scale:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
$rating = (average($style) + 2*average($substance))/1.5;&lt;/code&gt;&lt;br /&gt;&lt;code&gt;$rating = ($rating * 1.125) - 1.25;
&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unweighted Index:&lt;/strong&gt; Index ratings exactly as users have entered into our Gaming Index:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
$rating = average($index-rating);
&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Weighted Index:&lt;/strong&gt; Index ratings adjusted by the weight of each individual rating, which is based on user trust and inclusion of comments:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
$rating = average($index-rating*$index-weight)/average($index-weight);
&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;Our average rating—which is our criteria for success—decreased somewhat from the Reviews System to the Gaming Index and it decreased much more dramatically when we introduced our trust systems.&lt;/p&gt;
&lt;p&gt;The following chart shows the a typical example of how review and index ratings differ, using the venerable &lt;em&gt;Dungeons &amp;amp; Dragons Player&#39;s Handbook&lt;/em&gt; as an example:&lt;/p&gt;
&lt;center&gt;&lt;a href=&#34;http://index.rpg.net/display-entry-ratings.phtml?mainid=72&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/dd_players_handbook_rpgnet_reviews_only.jpg&#34; title=&#34;Dd_players_handbook_rpgnet_reviews_only&#34; alt=&#34;Dd_players_handbook_rpgnet_reviews_only&#34; /&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/dd_players_handbook_index_ratings_only.jpg&#34; title=&#34;Dd_players_handbook_index_ratings_only&#34; alt=&#34;Dd_players_handbook_index_ratings_only&#34; /&gt;&lt;/a&gt;&lt;/center&gt;&lt;p&gt;For this book the median ratings from reviews-only is 8, and the median from index-only is 7. A one-to-two point drop in median rating from reviews to index was consistent in all of our most-rated games other than those which were a rated a &amp;quot;10&amp;quot; in both places.&lt;/p&gt;
&lt;p&gt;We believe that this initial success of our unweighted Gaming Index can be attributed to the slightly better &lt;em&gt;granularity&lt;/em&gt;—a 10-point scale versus two 5-point scales—and our improved &lt;em&gt;distinctiviness&lt;/em&gt;—based on better naming of the rating levels. The veracity of this will ultimately be played out as the Index grows.&lt;/p&gt;
&lt;p&gt;However we have no doubt that our &lt;em&gt;statistical&lt;/em&gt; approach to the index data, when we moved from our unweighted Index to our weighted Index, is providing even better results. We had theorized that users who input more and who include comments would provide &amp;quot;better&amp;quot; data, and by our criteria of the average of the ratings moving toward 5.5 that seems to be borne out. The following table looks at the information a bit more precisely, by comparing average ratings as total number of ratings increases over several ranges:&lt;/p&gt;
&lt;center&gt;&lt;table cellpadding=&#34;3&#34; border=&#34;1&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;# of Ratings&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Average w/Comment&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Average w/o Comment&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;1-2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;8.55&lt;/td&gt;
&lt;td&gt;8.88&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;3-24&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;8.08&lt;/td&gt;
&lt;td&gt;8.16&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;25-49&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;7.32&lt;/td&gt;
&lt;td&gt;7.11&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;50-99&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;7.14&lt;/td&gt;
&lt;td&gt;7.03&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;100+&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;6.17&lt;/td&gt;
&lt;td&gt;6.99&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;p&gt;This table fairly definitively shows that base maxim: that the breadth of the ratings, and thus their quality, increases the more ratings a user makes. The improved quality of ratings with comments is less definitive. Among the vast mass of users the two values are pretty close, and sometimes the reverse of what we expect, but for the best and the worst users, ratings with comments seem to be better than those without. This latter point is another one that we&#39;ll have to continue to monitor as the Index grows beyond its current total of 10,000 ratings.&lt;/p&gt;
&lt;p&gt;The other major element of our &lt;em&gt;statistical&lt;/em&gt; approach to the Index is our bayesian weight. The following chart shows a top-ten chart for roleplaying games calculated via four different methodologies: our Reviews; our Index with no weighting; our Index with a 25 bayesian weighting (as it currently stands); and our Index with a 50 bayesian weighting:&lt;/p&gt;
&lt;center&gt;&lt;table cellpadding=&#34;3&#34; border=&#34;1&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Reviews-Only&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0-weight Index&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;25-weight Index&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;50-weight Index&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Delta Green: Countdown&lt;/td&gt;
&lt;td&gt;The Chronicles of Talislanta&lt;/td&gt;
&lt;td&gt;Delta Green: Countdown&lt;/td&gt;
&lt;td&gt;Delta Green&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Nobilis&lt;/td&gt;
&lt;td&gt;Wildside&lt;/td&gt;
&lt;td&gt;Spirit of the Century&lt;/td&gt;
&lt;td&gt;Delta Green: Countdown&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Castle Falkenstein&lt;/td&gt;
&lt;td&gt;Devil&#39;s Due&lt;/td&gt;
&lt;td&gt;Delta Green&lt;/td&gt;
&lt;td&gt;Unknown Armies&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Vimary Sourcebook&lt;/td&gt;
&lt;td&gt;Lodges: The Faithful&lt;/td&gt;
&lt;td&gt;Unknown Armies&lt;/td&gt;
&lt;td&gt;Call of Cthulhu&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Liber Servitorum&lt;/td&gt;
&lt;td&gt;Apocalypse&lt;/td&gt;
&lt;td&gt;Call of Cthulhu&lt;/td&gt;
&lt;td&gt;Nobilis&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Ork!&lt;/td&gt;
&lt;td&gt;Earthdawn Gamemaster&#39;s Compendium&lt;/td&gt;
&lt;td&gt;Nobilis&lt;/td&gt;
&lt;td&gt;Spirit of the Century&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;7&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;GURPS Russia&lt;/td&gt;
&lt;td&gt;Into the Badlands&lt;/td&gt;
&lt;td&gt;Pendragon&lt;/td&gt;
&lt;td&gt;Over the Edge&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;8&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;GURPS Reign of Steel&lt;/td&gt;
&lt;td&gt;Earthdawn Player&#39;s Compendium&lt;/td&gt;
&lt;td&gt;Over the Edge&lt;/td&gt;
&lt;td&gt;Pendragon&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;9&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Cudgel&#39;s Compendium&lt;/td&gt;
&lt;td&gt;Chronicle of the Black Labyrinth&lt;/td&gt;
&lt;td&gt;Mutants &amp;amp; Masterminds&lt;/td&gt;
&lt;td&gt;Mutants &amp;amp; Masterminds&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;10&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Corum&lt;/td&gt;
&lt;td&gt;The Spell Book&lt;/td&gt;
&lt;td&gt;Pulp Hero&lt;/td&gt;
&lt;td&gt;Vimary Sourcebook&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;p&gt;We actually &lt;em&gt;did&lt;/em&gt; do a little bit of statistical analysis on the Reviews because on our first try to produce this chart we got a random clump of reviews that were 5/5 from a much larger pool, so we further ordered them by descending total count of reviews, and as a result you&#39;re seeing a better selection of ranked reviews than a truly unstatistical sampling would allow. We did the same for the unweighted Index (which clumped a number of results at &amp;quot;10&amp;quot;), except we further ordered items at the same weight by decreasing number of views (another statistical decision).&lt;/p&gt;
&lt;p&gt;Clearly, deciding which of these lists is &amp;quot;right&amp;quot; is a much more subjective measure than the mathematical analysis we were able to apply to earlier problems. However, most roleplayers would tell you that the unweighted Reviews and Index lists are terrible. The top 5 items in the Reviews list actually aren&#39;t bad for a starting list of good games—but only because we did the aforementioned statistical ordering. Before that we just had a random listing of gaming items. Even with our attempts at quickie statistical analysis the unweighted Index is still quite bad, with only &lt;em&gt;Talislanta&lt;/em&gt; regularly showing up on other &amp;quot;best&amp;quot; lists.&lt;/p&gt;
&lt;p&gt;The problem is the ability of one person to come in and rate an item a &amp;quot;10&amp;quot; (or a &amp;quot;5&amp;quot;/&amp;quot;5&amp;quot;), thereby making that item more highly rated than &lt;em&gt;any&lt;/em&gt; item which has an actual consensus of ratings. Of our unweighted top Reviews only the top three had more than 2 reviews and the rest had 2. Not surprisingly those top three were the best fits to a typical top-ten list. Of the unweighted Index only the top three had more than 1 rating, and the rest had 1. Our single good pick was in those top three.&lt;/p&gt;
&lt;p&gt;Our 25-weight Index, which is what we currently use, has been generally accepted by the RPGnet community as a good marker of what&#39;s good and what&#39;s not. However there have been two items on it which some percentage of people disagree with: &lt;em&gt;Spirit of the Century&lt;/em&gt; and &lt;em&gt;Pulp Hero&lt;/em&gt;. It&#39;s instructive to see that when we increase to a 50-weight Index &lt;em&gt;Spirit of the Century&lt;/em&gt; drops (even more notably than depicted here, because its actual rating changes from .01 from first place to .16 from first place) and &lt;em&gt;Pulp Hero&lt;/em&gt; disappears entirely.&lt;/p&gt;
&lt;p&gt;The questions of &lt;em&gt;what&lt;/em&gt; to set your bayesian weight to, when to increase it, and what maximum value to set it to are all relatively unstudied and thus we don&#39;t have good answers to them. As we pass 10,000 ratings we&#39;re considering upping the bayesian value to 50. We expect that 100 will be our ultimate value when the Index is fully mature, however if we increase the weight too far an older, less rated game will never be able to get enough weight to get out of the doldrums.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We&#39;re by no means done with this ratings experiment. Though we&#39;ve pleased and impressed with the growth of the RPGnet Index thus far, by next year we hope that the Index will include the vast majority of all games in print (as opposed to somewhat less than half now) and that our 10,000 ratings will grow to 50,000 or more. This will allow us to offer even more definitive answers to our questions.&lt;/p&gt;
&lt;p&gt;In the meantime we&#39;re still mucking with our statistics and facing new problems.&amp;nbsp; Some of the newest:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;What to do about drive-by ratings:&lt;/strong&gt; Our trust algorithm does a good job of making drive-by ratings, where a publisher points his audience to an item in our site, mostly irrelevant, but there&#39;s some concern that they could have more effect in the long run.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How to incorporate our review ratings in our index ratings:&lt;/strong&gt; It seems a shame to waste the thousands of reviews that have been written—and indeed currently they&#39;re calculated into a composite rating we use in the Index—but we&#39;re realizing that people have very different purposes for writing reviews and inputing ratings, which may result in some of the upward skew we see on the review side of things. Ultimately we need to decide whether they&#39;re just too different or whether our statistical massaging is enough to incorporate those reviews into a composite Index rating.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How to pick some of our numbers:&lt;/strong&gt; As we already noted we don&#39;t have good formulas for when to choose which bayesian weights. Likewise we&#39;ve been guessing at which values to use for the trust-based weighting of our raters. Originally we set our desired rating count to 100 for good rater and 200 for great raters, but we&#39;ve since dropped those to 50 for good and 100 for great based upon the real numbers of ratings that users were making. Again, we&#39;d prefer to derive an actual formula for this type of calculation&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;Shannon has discussed some of these issues more in his recent article &lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_198.phtml&#34;&gt;More Thoughts Abour Ratings&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Despite unanswered questions, we still feel good about the basic ideas we laid out in our article last year. We have no doubt that giving our ratings a &lt;em&gt;statistical&lt;/em&gt; basis has dramatically improved them and evidence thus far suggests that both &lt;em&gt;granularity&lt;/em&gt; and &lt;em&gt;distinctiveness&lt;/em&gt; have been helpful as well.&lt;br /&gt;
&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Related articles from this blog:&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html&#34;&gt;2005-12: Systems for Collective Choice&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/collective_choi.html&#34;&gt;2005-12: Collective Choice: Rating Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2006/01/ranking_systems.html&#34;&gt;2006-01: Collective Choice: Competitive Ranking Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2006/08/using_5star_rat.html&#34;&gt;2006-08: Using 5-Star Rating Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;p&gt;Related articles from Shannon Appelcline&#39;s &lt;a href=&#34;http://www.skotos.net/articles/show-column.phtml?colname=TTnT_&#34;&gt;Trials, Triumphs &amp;amp; Trivialities:&lt;/a&gt;
&lt;/p&gt;&lt;blockquote&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_192.phtml&#34;&gt;#192: Managing User Creativity, Part One&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_193.phtml&#34;&gt;#193: Managing User Creativity, Part Two&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_196.phtml&#34;&gt;#196: Collective Choice: Ratings, Who Do You Trust?&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_198.phtml&#34;&gt;#198: Collective Choice: More Thoughts About Ratings&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;footer&gt;&lt;h3&gt;Comments&lt;/h3&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;URL:
0-10 scale is too large for most relevant human ratings systems. maybe i should pull the psych papers, but people tend to do a 1-5 or 0-5 scale better. it&#39;s a sign of immaturity to only use the endpoints.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;#&#34;&gt;Grumpy&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2007-12-18T10:24:24-07:00&#34;&gt;2007-12-18T10:24:24-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;/footer&gt;
&lt;p class=&#34;previous&#34;&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/previous/2007/01/collective_choi.html&#34; rel=&#34;syndication&#34; class=&#34;u-syndication&#34; &gt;orginal layout&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Collective Choice: Competitive Ranking Systems</title>
      <link>http://www.lifewithalacrity.com/2006/01/ranking_systems.html</link>
      <pubDate>Tue, 03 Jan 2006 23:37:09 -0700</pubDate>
      
      <guid>http://www.lifewithalacrity.com/2006/01/ranking_systems.html</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-size: 0.7em;&#34;&gt;by Christopher Allen &amp;amp; Shannon Appelcline&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;[This is the third in a series of articles on &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html&#34;&gt;collective choice&lt;/a&gt;, co-written by my collegue &lt;a href=&#34;http://www.skotos.net/about/staff/shannon_appelcline&#34;&gt;Shannon Appelcline&lt;/a&gt;. It will be &lt;a href=&#34;http://www.skotos.net/articles/TTnT_179.phtml&#34;&gt;jointly posted&lt;/a&gt; in Shannon&#39;s &lt;a href=&#34;http://www.skotos.net/articles/TTnT.shtml&#34;&gt;Trials, Triumphs &amp;amp; Trivialities&lt;/a&gt; online games column at &lt;a href=&#34;http://www.skotos.net/&#34;&gt;Skotos&lt;/a&gt;.]&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In our first article on &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html&#34;&gt;collective choice&lt;/a&gt; we outlined a number of different types of choice systems, among them voting, polling, rating, and ranking. Since then we&#39;ve been spending some time expanding upon the systems, with the goal being to create both a lexicon of and a dialogue about systems for collective choice.&lt;/p&gt;
&lt;p&gt;This time we&#39;re going to dig more into &lt;em&gt;comparison ranking systems&lt;/em&gt;, by focusing on &lt;em&gt;competitive&lt;/em&gt; rankings and looking more in depth at ELO Chess Ranking System and the other systems that we briefly mentioned previously. Our goal is to explicate these systems, to better address their flaws, to begin detailing the purposes of ranking systems, and to show how those purposes are critical in the design of ranking systems.&lt;/p&gt;
&lt;h3&gt;&lt;a name=&#34;subjective_vs_objective&#34;&gt;&lt;/a&gt;Subjective vs. Objective Rankings&lt;/h3&gt;
&lt;p&gt;In our original article we discussed &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/collective_choi.html&#34;&gt;rating systems&lt;/a&gt; as being largely subjective and ranking systems as being objective, but the situation isn&#39;t nearly as simple as that. In truth, there&#39;s a clear spectrum of ratings and rankings with varying amounts of subjectivity and objectivity in each collective choice system. &lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.bcsfootball.org/&#34;&gt;&lt;img border=&#34;0&#34; alt=&#34;Bcs_1&#34; title=&#34;Bcs_1&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/bcs_1.png&#34; style=&#34;margin: 0px 5px 5px 0px; float: left;&#34; /&gt;&lt;/a&gt;
&lt;a href=&#34;http://www.officialworldgolfranking.com/&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/golfrankings_1.png&#34; title=&#34;Golfrankings_1&#34; alt=&#34;Golfrankings_1&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
The &lt;a href=&#34;http://en.wikipedia.org/wiki/Bowl_Championship_Series&#34;&gt;Bowl Championship Series&lt;/a&gt; (BCS) for college football is a good example of a ranking system that explicitly allows a subjective component. It involves a complex mathematical formula that includes things like win/loss ratios, but also sportswriters&#39; and coaches&#39; ratings. &lt;/p&gt;
&lt;p&gt;However, public opinion continues to show that people don&#39;t necessarily like seeing true ranking systems having subjective components, because they expect them to be &amp;quot;fair&amp;quot;. The BCS formula has come under attack several times in the last few years precisely due to its subjective basis. Cal Berkeley was one of several teams denied a bowl position in 2004 when many felt that they were worthy.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;http://www.atptennis.com/en/players/raceleaderboard/rank_definitions.asp&#34;&gt;APL tennis rankings&lt;/a&gt; and the &lt;a href=&#34;http://en.wikipedia.org/wiki/Golf_rankings&#34;&gt;official world golf rankings&lt;/a&gt; also have a subjective component, but it is much more subtle. Each tournament is worth a certain number of points, and the allocation of those points is relatively arbitrary, based upon the &amp;quot;prestige&amp;quot; of each tournament and the quality of players who have traditionally played in it. The subjectivism isn&#39;t quite as near to the surface as that of the college bowls, but it&#39;s still something that can have a notable, and perhaps unwarranted, effect upon the final results.&lt;br /&gt; &lt;/p&gt;
&lt;h3&gt;&lt;a name=&#34;algorithmic_rankings&#34;&gt;&lt;/a&gt;Algorithmic Rankings&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.kosteniuk.com/&#34;&gt;&lt;img border=&#34;0&#34; alt=&#34;Wcsrating_2&#34; title=&#34;Wcsrating_2&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/wcsrating_2.jpg&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
This brings us back to the &lt;a href=&#34;http://en.wikipedia.org/wiki/ELO_rating_system&#34;&gt;ELO system&lt;/a&gt;, a ranking system originally designed for chess which is fairly well-known and well-understood. As we said in our overview article, &amp;quot;[ELO] builds a simple distribution of player ratings around a norm (typically 1500 points), then awards or deducts points based upon wins and losses, with the total sum of all points in the system staying constant. Players are then ranked according to their comparative scores.&amp;quot;&lt;/p&gt;
&lt;p&gt;The big difference between this and the previously discussed systems is that it&#39;s almost entirely objective; in fact it uses a statistical basis to create an underlying mathematical model for rankings, rather than allowing human subjectivity to get in the way.&lt;br /&gt;
&lt;/p&gt;
&lt;p&gt;The simplest formulation for an ELO rating looks like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span size=&#34;+2&#34;&gt;&lt;code&gt;R&#39; = R + K * (S - E)&lt;/code&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;R&#39;&lt;/code&gt; is the new rating&lt;br /&gt;
&lt;code&gt;R&lt;/code&gt; is the old rating&lt;br /&gt;
&lt;code&gt;K&lt;/code&gt; is a maximum value for increase or decrease of rating (16 or 32 for ELO)&lt;br /&gt;
&lt;code&gt;S&lt;/code&gt; is the score for a game&lt;br /&gt;
&lt;code&gt;E&lt;/code&gt; is the expected score for a game&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Much of the trick is in figuring out what the (&lt;code&gt;E&lt;/code&gt;)xpected score of a game is. ELO uses the following formulas for players A and B:&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;E(A) = 1 / [ 1 + 10 ^ ( [R(B) - R(A)] / 400 ) ]&lt;br /&gt;E(B) = 1 / [ 1 + 10 ^ ( [R(A) - R(B)] / 400 ) ]&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;It&#39;s a good model because, using the two formulas, it means that a great player gains little from beating an average player, but an average player gains a lot from beating a great player. Take the following example:&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;R(A) = 1900&lt;br /&gt;R(B) = 1500&lt;br /&gt;
E(A) = 1 / [ 1 + 10 ^ ( [1500 - 1900] / 400 ) ]&lt;br /&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp; = 1 / [ 1 + 10 ^ ( -400 / 400) ]&lt;br /&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp; = 1 / [ 1 + 10 ^ -4 / 4 ]&lt;br /&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp; = 1 / [ 1 + 10 ^ -1 ]&lt;br /&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp; = 1 / 1 + .1&lt;br /&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp; = .91&lt;br /&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp; = 91%&lt;br /&gt;&lt;p&gt;E(B) = 1 / [ 1 + 10 ^ ( [1900 - 1500] / 400) ]&lt;br /&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp; = 1 / [ 1 + 10 ^ ( 400 / 400 ) ]&lt;br /&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp; = 1 / [ 1 + 10 ^ 1 ]&lt;br /&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp; = 1 / 11&lt;br /&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp; = .09&lt;br /&gt;&amp;nbsp; &amp;nbsp;&amp;nbsp; = 9%&lt;/p&gt;&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;Player A is expected to score .91 in an average game, which is to say he should win 91% of the time, and will be punished accordingly if he loses to player B:&lt;/p&gt;
&lt;blockquote&gt;&lt;pre&gt;R&#39; = 1900 + 32 * (0 - .91)&lt;br /&gt;R&#39; = 1900 - 29.12&lt;br /&gt;R&#39; = 1871&lt;/pre&gt;&lt;/blockquote&gt;
&lt;p&gt;Conversely a win nets him very little:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;
R&#39; = 1900 + 32 * (1 - .91)&lt;br /&gt;R&#39; = 1900 + 32 * .09&lt;br /&gt;R&#39; = 1900 + 2.88&lt;br /&gt;R&#39; = 1903&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;ELO is almost entirely mathematical. Players can gain or lose different amounts of points based upon playing different players, but this is all part of the formula. The only slightly subjective element is the definition of &lt;code&gt;K&lt;/code&gt; -- how much a player can win or lose from a particular game. The most widely used ELO systems for Chess break &lt;code&gt;K&lt;/code&gt; down into two values: 16 for masters and 32 for everyone else. So there is a subjective decision that masters should vary their score less frequently than other players. &lt;/p&gt;
&lt;p&gt;That&#39;s a very minor element in an otherwise objective system, but as we&#39;ll see, more recent systems by Days of Wonder and Microsoft first reduce, then eliminate even this subjectivity.&lt;/p&gt;
&lt;h3&gt;&lt;a name=&#34;days_of_wonder&#34;&gt;&lt;/a&gt;Variations of a Theme: Days of Wonder&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.daysofwonder.com/&#34;&gt;&lt;img border=&#34;0&#34; alt=&#34;Dowlogo_1&#34; title=&#34;Dowlogo_1&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/dowlogo_1.gif&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
ELO is probably the most used ranking system in the world. You can find it in use for Go, &lt;em&gt;Tantrix&lt;/em&gt;, and many other games. Days of Wonder, producers of &lt;em&gt;Gang of Four&lt;/em&gt;, &lt;em&gt;Ticket to Ride&lt;/em&gt;, and many other games use a variant of the system which they describe on &lt;a href=&#34;http://www.gangoffour.com/index.php?t=content&amp;amp;sub=ranking&#34;&gt;their website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;They identify three core problems with ELO: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;New players can take a long time to ascend or descend to their correct levels.&lt;/li&gt;
&lt;li&gt;Highly ranked players can be hesitant to play with provisional players whose ranking might be much more uncertain.&lt;/li&gt;
&lt;li&gt;There are no allowances for games with more than two players.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Days of Wonder resolved the first problem by creating a new formula for provisional players, allowing them to rise and fall in the rankings much more quickly. &lt;/p&gt;
&lt;p&gt;Conversely when playing against provisional players, regular players can only lose a maximum of &lt;code&gt;K*n/20&lt;/code&gt; points, where n is the number of games that the provisional player has played--rather than the normal maximum loss of &lt;code&gt;K&lt;/code&gt;. For example, playing someone who has just played one game, can only result in a loss of 1/20th of the regular &lt;code&gt;K&lt;/code&gt; value, and so it really doesn&#39;t matter if the provisional player&#39;s ranking is wildly out of whack.&lt;/p&gt;
&lt;p&gt;Both of these new formulas are set up to converge toward a normal ELO formula as a provisional player&#39;s number of games approaches 20 (making them a normal player at Days of Wonder).&lt;/p&gt;
&lt;p&gt;(It should be pointed out that using the number &amp;quot;20&amp;quot; to define a provisional player, and making a player less provisional in clean 5% steps, inevitably offers yet another small, subjective element into this mathematical formula; as we&#39;ll see momentarily Microsoft has more recently incorporated the idea of provisional uncertainty into their core mathematical model, much as the whole ELO system originally turned subjective win and loss statistics into tighter mathematics.)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.skotos.net/games/wonder/TT/&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/ttrskotosrankings.png&#34; title=&#34;Ttrskotosrankings&#34; alt=&#34;Ttrskotosrankings&#34; style=&#34;margin: 0px 5px 5px 0px; float: left;&#34; /&gt;&lt;/a&gt;
Finally, to resolve the situation of multiple players, Days of Wonder considers each game to be a set of duels, as described here:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;
There are 4 players in a &lt;/em&gt;&lt;em&gt;Gang of Four game. Let&#39;s name A the winning player, B the second one, C the third one and D the last one. We consider that there were 6 duels: A won against B, C and D. B won against C and D. C won against D. We compute independently the new scores for each duel, and then we average the values for each player&lt;/em&gt;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It&#39;s a fairly elegant answer that not only rewards or penalizes all players separately, but also encourages playing for second place, or even third, if first isn&#39;t possible.&lt;/p&gt;
&lt;p&gt;There have been continued discussions of the Days of Wonder ELO variant in &lt;a href=&#34;http://www.daysofwonder.com/index.php?t=forums&#34;&gt;their forums&lt;/a&gt;, and the questions raised there are common to many different ranking systems. Some players wanted unranked games, while others thought that having unranked games would discourage people from playing good competitors &lt;em&gt;except&lt;/em&gt; in unranked games.&amp;nbsp; There has also been a lot of discussion regarding &lt;em&gt;Ticket to Ride&lt;/em&gt;, a strategy game that supports 2-5 people, and whether the ELO variant system discourages multiperson play.&lt;/p&gt;
&lt;p&gt;The various lessons learned at Days of Wonder underline two basic ideas about rankings. First, even with a well-studied system like ELO, there&#39;s still a lot to understand, and, second, any ranking system needs to reflect the specifics of what it&#39;s ranking -- and what its purpose is.&lt;/p&gt;
&lt;h3&gt;&lt;a name=&#34;xbox_360_live&#34;&gt;&lt;/a&gt;Variations on a Theme: XBox 360 Live&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.research.microsoft.com/mlp/trueskill/default.aspx&#34;&gt;&lt;img border=&#34;0&#34; alt=&#34;Trueskillxbox360&#34; title=&#34;Trueskillxbox360&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/trueskillxbox360.jpg&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
An even more recent large-scale ranking system is the &lt;a href=&#34;http://research.microsoft.com/displayArticle.aspx?id=1361&#34;&gt;TrueSkill&lt;/a&gt; system developed by Microsoft for use with the XBox 360. It appears to be an expanded variant of the &lt;a href=&#34;http://math.bu.edu/people/mg/glicko/glicko.doc/glicko.html&#34;&gt;glicko&lt;/a&gt; ranking system used by the &lt;a href=&#34;http://www.freechess.org/&#34;&gt;free internet chess server&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Many of the problems identified by Microsoft were the same as those already noted by Days of Wonder and others, including: the uncertainty of provisional ratings and the need to rank players in multiplayer games. However, the TrueSkill system notably expands both issues. Ranking uncertainty is now defined as a mathematical concept and the rankings now support not just multiple players, but also multiple teams.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://lifewithalacrity.blogs.com/.shared/image.html?/photos/uncategorized/trueskill.gif&#34; onclick=&#34;window.open(this.href, &#39;_blank&#39;, &#39;width=430,height=324,scrollbars=no,resizable=no,toolbar=no,directories=no,location=no,menubar=no,status=no,left=0,top=0&#39;); return false&#34;&gt;&lt;img width=&#34;200&#34; height=&#34;150&#34; border=&#34;0&#34; alt=&#34;Trueskill&#34; title=&#34;Trueskill&#34; src=&#34;http://www.lifewithalacrity.com/previous/images/trueskill.gif&#34; style=&#34;margin: 0px 5px 5px 0px; float: left;&#34; /&gt;&lt;/a&gt;TrueSkill explicitly includes two values in any ranking: a skill level and an uncertainty level. The first, like the more common ELO ranking, tells how good a player is. The second states how sure that ranking is. The uncertainty rating is effectively a margin of error, similar to those we saw in &lt;em&gt;polling systems&lt;/em&gt;. If a first-time player has a skill rating of 25 with an uncertainty rating of 8.3 that means that his skill is probably somewhere in the range of 16.7 to 33.3, a pretty wide range, but then this is a totally untested player. According to benchmarks that Microsoft produced, 99.99% of actual skill levels were within 3x of the uncertainty rating, and 100% were within 4x.&lt;/p&gt;
&lt;p&gt;The rest of TrueSkill&#39;s innovations are built around this model of uncertainty. All players win or lose skill points, based upon how many players they beat or lose to, and they also decrease their uncertainty rating as they play more games. However, uncertainty is decreased more for players toward the middle of a pack within a game than those around the edges (because on the edges the players could actually be much better or much worse than it is possible to see from a specific game). In addition, TrueSkill is only a zero-sum ranking system for players at the exact same level of uncertainty. The more uncertainty that an opponent possesses, the smaller the weighting of any gain or loss (much like the simpler system that Days of Wonder uses, which bases weightings of games against provisional players as &lt;code&gt;n/20&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Overall TrueSkill is a somewhat complex system that is described more fully at Microsoft&#39;s &lt;a href=&#34;http://www.research.microsoft.com/mlp/trueskill/Details.aspx#How_to_Update_Skills&#34;&gt;web site&lt;/a&gt;. Some of their expansions had already been considered by others, but still their system is notably innovative in two ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Expanding a competitive ranking system to include concepts of teams.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Incorporating the uncertainty of ratings further into the core mathematical model, rather than using a somewhat more subjective model such as that described by Days of Wonder for provisional players.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;http://www.research.microsoft.com/mlp/trueskill/RankCalculator.aspx&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/trueskillcalculator_1.png&#34; title=&#34;Trueskillcalculator_1&#34; alt=&#34;Trueskillcalculator_1&#34; class=&#34;image-full&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
The TrueSkill calculations are a bit complex. In general, that&#39;s not a problem for a computer-based ranking model because you can have a computer doing all the computations, and players only need to understand the results. However the two-part ranking system used by TrueSkill, which notes both skill level and uncertainty, does offer a potential problem on this latter point. &lt;em&gt;Can players understand it?&lt;/em&gt; In general, the concept of uncertainty will not be understood by people other that statisticians, thus raising a real user-interface question with the TrueSkill system -- and the exact sort of thing that designers of new ranking systems will need to consider.&lt;/p&gt;
&lt;h3&gt;&lt;a name=&#34;a_tale_in_the_desert&#34;&gt;&lt;/a&gt;Variations on a Theme: A Tale in the Desert&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.atid.com&#34;&gt;&lt;img border=&#34;0&#34; alt=&#34;A_tale_in_the_desert_logo_1&#34; title=&#34;A_tale_in_the_desert_logo_1&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/a_tale_in_the_desert_logo_1.jpg&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
The online game, &lt;a href=&#34;http://www.atitd.com&#34;&gt;A Tale in the Desert&lt;/a&gt;, identified a different problem with the ELO system: &lt;em&gt;cheating&lt;/em&gt;. This is a uniquely Internet-based problem, because there users can create fake accounts, then defeat those accounts to win points. This can also be done more subtly, by having multiple additional accounts build up the rating of that fake account before the fake account is defeated. So a totally new ranking system, called the &lt;a href=&#34;http://wiki.atitd.net/tale1/EGenesis_Ranking_System&#34;&gt;eGenesis Ranking System&lt;/a&gt;, was created.&lt;/p&gt;
&lt;p&gt;Each player is ranked through a 256-bit vector, half of which is initially set to 0 and half of which is set to 1 (therefore creating an average ranking of 128). Whenever a match occurs between players a hash function based on the players&#39; names mathematically selects 32 of those bits, 8 of which are then randomly selected. Among those bits, any 1s in the loser&#39;s vector which correspond to 0s in the winner&#39;s vector are &amp;quot;transferred&amp;quot;.&lt;/p&gt;
&lt;p&gt;This simple design corresponds in some ways to ELO&#39;s more complex formula. A good player will have more 1s and thus more to lose, and he will lose correspondingly more to a poor player who has more 0s in his vector. &lt;/p&gt;
&lt;p&gt;However, the system also prevents the collusion earlier noted. Statistically, a single player will only ever gain 8 ranking points from another new player, since out of the 32 bit hash only eight of those will, on average, be in the correct 0-1 configuration. Expanding a group of players expands the number of points that can potentially be gained, but within real limits.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2005/10/dunbar_group_co.html&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/wowsocialmap_1.png&#34; title=&#34;Wowsocialmap_1&#34; alt=&#34;Wowsocialmap_1&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
In fact, the eGenesis system prevents cheating by measuring the size of social networks, then limiting the number of ranking points that can be earned within a social network. It&#39;s not necessarily the only way to measure social network size, but its methodology points toward social software as an interesting area for additional study of ranking systems.&lt;/p&gt;
&lt;p&gt;As with XBox&#39;s TrueSkill, the eGenesis algorithms are overall fairly sophisticated and confusing, perhaps more so than TrueSkill itself. However, unlike TrueSkill the output is very simple: a skill number between 0 and 255. The intricacies are hidden by the system.&lt;/p&gt;
&lt;h3&gt;&lt;a name=&#34;goals&#34;&gt;&lt;/a&gt;Competitive Ranking Goals&lt;/h3&gt;
&lt;p&gt;Ultimately, as we mentioned when discussing Days of Wonder, any ranking system has to be measured by what it&#39;s trying to do and how well it does that. ELO and similar numerical, long-term ranking systems, are most likely trying to achieve one of three goals:&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;heirarchy&#34;&gt;&lt;/a&gt;&lt;strong&gt;Hierarchy:&lt;/strong&gt; Players are divided into hierarchies of success, giving players goals to constantly strive for and ways to measure their success (or failure).&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;matching&#34;&gt;&lt;/a&gt;&lt;strong&gt;Matching:&lt;/strong&gt; Players can play with other players at their same skill level, rather than having to play beginners or experts who are much better than they are. This generally increases everyone&#39;s enjoyment. For computer games, the complexity of a matching system can be largely moderated by the computer, thus ensuring better competition.&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;handicapping&#34;&gt;&lt;/a&gt;&lt;strong&gt;Handicapping:&lt;/strong&gt; If players do play against others of different skill levels, the better players can be handicapped in automatic, appropriate ways for the game in question, again increasing the fairness of games and everyone&#39;s enjoyment. For instance, someone ranked 3-kyu in &lt;a href=&#34;http://en.wikipedia.org/wiki/Go_%28board_game%29&#34;&gt;Go&lt;/a&gt; playing a less experienced 7-kyu player would give him a starting &lt;a href=&#34;http://en.wikipedia.org/wiki/Go_handicap&#34;&gt;4 stone advantage&lt;/a&gt; to make for better competition.&lt;/p&gt;
&lt;p&gt;The ELO system may be a good matching system, which allows players to easily find other players of their same skill level and play against them. However it doesn&#39;t provide any way to handicap players, nor would the ELO method necessarily be a good one to analyze handicaps (and conversely a &lt;a href=&#34;http://en.wikipedia.org/wiki/Golf_handicap&#34;&gt;golf handicap&lt;/a&gt; might not do a good job of finding like players nor measuring players&#39; ability in a hierarchy). &lt;/p&gt;
&lt;p&gt;More recently the XBox system has stated that it&#39;s explicitly for matchmaking, with the goal being to always try and match up players at nearly the same skill level. It&#39;s also used for hierarchy (or &amp;quot;leaderboards&amp;quot; as it&#39;s described in the TrueSkill docs), but that&#39;s clearly a subsidiary purpose.&lt;/p&gt;
&lt;p&gt;All of these systems would be ineffective for measuring a winner in a live event, which is a very different goal:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tourney:&lt;/strong&gt; A single player is listed as an absolute winner, the &amp;quot;King of the Hill&amp;quot;. Often, second, third, and fourth place winners are measured too.&lt;/p&gt;
&lt;p&gt;And, the systems we&#39;ve discussed thus far may not be useful for measuring privileges, yet another goal:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Threshold:&lt;/strong&gt; The best ranks of players can be given special privileges, including the ability to create games and form tournaments. Alternatively, they can be given privileges totally outside the game, again giving them something extra to strive for.&lt;/p&gt;
&lt;p&gt;For each of these additional goals we may need to consider very different ranking systems, not just variations of ELO.&lt;/p&gt;
&lt;h3&gt;&lt;a name=&#34;tourneys&#34;&gt;&lt;/a&gt;Different Themes: Tourneys&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www2.kumc.edu/itc/staff/rknight/tourneys.htm&#34;&gt;&lt;img border=&#34;0&#34; alt=&#34;Tournament_1&#34; title=&#34;Tournament_1&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/tournament_1.png&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
There are a number of well-known tournament types which can be used to create a &amp;quot;King of the Hill&amp;quot; ranking.&lt;/p&gt;
&lt;p&gt;The simplest is the &lt;a href=&#34;http://en.wikipedia.org/wiki/Single-elimination_tournament&#34;&gt;single-elimination tournament,&lt;/a&gt; where the winner of each competition moves on to compete with other winners, until there is only one. However, this style of tournament is quite cut-throat and is not suited very well to events where the competition may result in a draw, or where chance is a notable factor in the competition. It also has a very subjective factor in the initial &lt;a href=&#34;http://en.wikipedia.org/wiki/Single-elimination_tournament#Seeding&#34;&gt;seeding&lt;/a&gt; of the rounds. The single-elimination tournament also does not rank the losers. However, by having the losers compete with each other in a &lt;a href=&#34;http://en.wikipedia.org/wiki/Swiss_system_tournament&#34;&gt;Swiss-style tournament&lt;/a&gt;, the relative strengths of the players can be ranked.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://jsoo.home.mindspring.com/articles/formats/&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/pseudodoubleelimination_1.png&#34; title=&#34;Pseudodoubleelimination_1&#34; alt=&#34;Pseudodoubleelimination_1&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
An improvement is the &lt;a href=&#34;http://en.wikipedia.org/wiki/Double-elimination&#34;&gt;double-elimination tournament&lt;/a&gt; which is now one of the best known tournament systems in sports. Players compete in series of two-player matches, and a player has to lose twice before he&#39;s eliminated. This is done through a system of winner and loser brackets, wherein people drop from the winners&#39; brackets to the losers&#39; brackets when they lose once, and drop out altogether when they lose twice.&lt;/p&gt;
&lt;p&gt;One problem with standard double-elimination is that there are unusual situations where a significantly inferior player can still make it to the final round, or the last player to remain undefeated can lose only once and still be eliminated. These can be addressed through variants such as face-off (requiring the last two remaining competitors to compete &lt;em&gt;again&lt;/em&gt; if the undefeated team is defeated for the first time in the finals) or by &lt;a href=&#34;http://jsoo.home.mindspring.com/articles/formats/&#34;&gt;reconfiguring the loser&#39;s brackets&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.wscgames.com/&#34;&gt;&lt;img border=&#34;0&#34; alt=&#34;Wsc_1&#34; title=&#34;Wsc_1&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/wsc_1.png&#34; style=&#34;margin: 0px 5px 5px 0px; float: left;&#34; /&gt;&lt;/a&gt;
&lt;a href=&#34;http://en.wikipedia.org/wiki/Round-robin_tournament&#34;&gt;Round-robin tournaments&lt;/a&gt;, such as official &lt;a href=&#34;http://www.wscgames.com/2005/build/standing/1/topten.html&#34;&gt;Scrabble Tournaments&lt;/a&gt; involve every player playing a set number of games (24 in the 2005 World Scrabble Championship), facing opponents with similar win-lose records. They then ultimately rank players by their win-lose ratios.&lt;/p&gt;
&lt;p&gt;The advantage of these sorts of tournament over an ELO-style ranking is that they&#39;re easily understandable and seem fair. In addition, they measure ranking in a much more topical manner: how well someone is playing during a singular instant, rather than over a longer career. As a result they work much better for a live tournament.&lt;/p&gt;
&lt;h3&gt;&lt;a name=&#34;thresholds&#34;&gt;&lt;/a&gt;Different Themes: Thresholds&lt;/h3&gt;
&lt;p&gt;As we discussed in our original article on Collective Choice, thresholds are ranking barriers above which members get a special ability--or alternatively levels below which members lose a special ability. They can also act as another goal for a ranking system.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.usgo.org/&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/gosmall.jpg&#34; title=&#34;Gosmall&#34; alt=&#34;Gosmall&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
In the game of Go there are both &lt;a href=&#34;http://en.wikipedia.org/wiki/Go_ranks_and_ratings#Pro_and_amateur_ranks&#34;&gt;amateur&lt;/a&gt; and &lt;a href=&#34;http://en.wikipedia.org/wiki/Go_ranks_and_ratings#Professional_strength&#34;&gt;professional&lt;/a&gt; players. Although they aren&#39;t technically in the same &lt;a href=&#34;http://www.cns.nyu.edu/~mechner/go/kyu.html&#34;&gt;hierarchy of rankings&lt;/a&gt;, the highest Go amateur ranking&amp;nbsp; (7 dan) is approximately equal to the lowest Go professional ranking (1 dan), forming a de facto threshold.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.uschess.org/&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/uscf_1.png&#34; title=&#34;Uscf_1&#34; alt=&#34;Uscf_1&#34; style=&#34;margin: 0px 5px 5px 0px; float: left;&#34; /&gt;&lt;/a&gt;
Likewise the United States Chess Federation uses their ELO rankings to denote &lt;a href=&#34;http://en.wikipedia.org/wiki/Chess_master&#34;&gt;Chess Masters&lt;/a&gt;. Anyone who achieves 2200 UCSF is given a National Master threshold ranking and anyone who maintains it for 300 games is given a Life Master threshold ranking.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.acbl.org/&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/acblopt2_1.gif&#34; title=&#34;Acblopt2_1&#34; alt=&#34;Acblopt2_1&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
The &lt;a href=&#34;http://www.acbl.org/&#34;&gt;American Contract Bridge Association&lt;/a&gt; uses a threshold system where you have to win a certain number of tournaments and thus earn &lt;a href=&#34;http://www.acbl.org/about/honorTitles.html&#34;&gt;masterpoints&lt;/a&gt; in order achieve official rankings such as &amp;quot;section master&amp;quot;. Furthermore, players may earn different &amp;quot;colors&amp;quot; of masterpoints depending the difficulty of the tournament, and some ranks require that you earn at least some specific colored masterpoints in order to meet the requirements for the next threshold.&lt;/p&gt;
&lt;p&gt;These thresholds are fairly explicitly based on other hierarchical ranking systems, but this doesn&#39;t need to be the case. Since determining the purpose of a ranking system is often the first step in designing it, as we delve further into the area of thresholds we may well find that systems specifically dedicated toward measuring thresholds are more likely to do so well.&lt;/p&gt;
&lt;p&gt;In our next article we&#39;ll consider among other things the Avogadro reputation system, which manages thresholds in such a way as to prevent cheating.&lt;/p&gt;
&lt;h3&gt;&lt;a name=&#34;conclusion&#34;&gt;&lt;/a&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;There&#39;s actually a lot of variety in ranking systems, and even though we&#39;d like them to be totally objective, various subjective elements often creep into these systems. In addition, there&#39;s a lot of variety in what ranking systems can do. For competitive systems, hierarchy, privilege, matching, and handicapping are some of the top purposes of ranking. Determining what a ranking system is going to do is a necessary first step in designing the system, as different systems will accomplish various goals to a better or worse degree.&lt;/p&gt;
&lt;p&gt;ELO, in several variants, is the best studied and most used competitive ranking system. It works particularly well as a matching system. However, even ELO has flaws in it, among them: issues with new player rankings; its core two-player basis; its lack of provisions for teams; a few minor subjective elements; and problems with cheaters. New systems continue to be rolled out on the Internet to resolve these issues, and overall, it&#39;s an area of interesting new study.&lt;/p&gt;
&lt;p&gt;Tournament systems and threshold systems offer a few good examples of competitive ranking systems with very different purposes, underlying the need to understand what you&#39;re doing before you do it.&lt;/p&gt;
&lt;p&gt;Ranking systems also lay very near yet another type of Collective Choice: reputation systems. We briefly addressed reputation systems when talking about threshold systems and will return to this in our next article.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Related articles from this blog:&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html&#34;&gt;2005-12: Systems for Collective Choice&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/collective_choi.html&#34;&gt;2005-12: Collective Choice: Rating Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2006/08/using_5star_rat.html&#34;&gt;2006-08: Using 5-Star Rating Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2007/01/collective_choi.html&#34;&gt;2007-01: Experimenting with Ratings&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;p&gt;Related articles from Shannon Appelcline&#39;s &lt;a href=&#34;http://www.skotos.net/articles/show-column.phtml?colname=TTnT_&#34;&gt;Trials, Triumphs &amp;amp; Trivialities:&lt;/a&gt;
&lt;/p&gt;&lt;blockquote&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_192.phtml&#34;&gt;#192: Managing User Creativity, Part One&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_193.phtml&#34;&gt;#193: Managing User Creativity, Part Two&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_196.phtml&#34;&gt;#196: Collective Choice: Ratings, Who Do You Trust?&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_198.phtml&#34;&gt;#198: Collective Choice: More Thoughts About Ratings&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;footer&gt;&lt;h3&gt;Comments&lt;/h3&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;A data point on ELO cheating for you: Yahoo! Games uses ELO rankings for several their two-player games. Before recent abuse mitigation changes, some people used robot to accumulate scores in excess of 6,000,000 points. The abuse-the-ranking game had become a totally seperate competition.
For now, Yahoo! has capped the ELO scores at 3,000 (I think.) This removed most of the cheating incentive.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://www.fudco.com/habitat&#34;&gt;F. Randall Farmer&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-01-05T13:46:18-07:00&#34;&gt;2006-01-05T13:46:18-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;In the slashdot discussion of this blog entry http://games.slashdot.org/comments.pl?sid=173988&amp;threshold=0&amp;mode=flat   there was an interesting link to beatpaths.com, which is sort of a tourney system for when you can&#39;t complete a full set of round-robin or double-elimination competitions, as what happens in NFL during the fall season.
What is also interesting to me is that it introduces a goal for ranking systems that I&#39;d not thought of before -- prediction. The purpose of the beatpath systems is actually focused on predicting the outcome the next set of the weekend games.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://www.LifeWithAlacrity.com&#34;&gt;Christopher Allen&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-01-14T14:31:30-07:00&#34;&gt;2006-01-14T14:31:30-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;No mention of Glicko and Glicko2?
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://swaits.com/&#34;&gt;Stephen Waits&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-01-15T08:57:38-07:00&#34;&gt;2006-01-15T08:57:38-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;There&#39;s a ranking scheme that Mark Newman from the U of Michigan did for college football, which Yahoo Search turned up a pointer to here:
http://www.sciencenews.org/articles/20051112/mathtrek.asp
quoting:
Physicists Juyong Park and M.E.J. Newman aimed for a ranking system that&#39;s fast and easily understood by fans (in contrast to the cumbersome, opaque BCS formula). They based their ranking method on the notion that, if A beats B, and B beats C, then A also beats C, even if it may not actually play C. Hence, the method counts both direct wins by beating a team and indirect wins by beating a team that beat another team.
(end quote)
You end up with a directed graph of teams as nodes and games as edges, and compute a function of that graph that is related to a centrality metric.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://vielmetti.typepad.com&#34;&gt;Edward Vielmetti&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-02-02T22:18:39-07:00&#34;&gt;2006-02-02T22:18:39-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;This article is a great survey of the subject!  I&#39;m working on the PvP ranking system for another game with some unique problems, because the teams in a PvP match can be of disparate size and widely differing power levels, which are then balanced so that wildly mismatched teams (ignoring relative rank at PvP for the moment) are actually reasonably matched.
It starts here: http://ncanson.livejournal.com/3938.html
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://ncanson.livejournal.com/&#34;&gt;Matthew Weigel&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2008-01-31T09:55:58-07:00&#34;&gt;2008-01-31T09:55:58-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;/footer&gt;
&lt;p class=&#34;previous&#34;&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/previous/2006/01/ranking_systems.html&#34; rel=&#34;syndication&#34; class=&#34;u-syndication&#34; &gt;orginal layout&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>