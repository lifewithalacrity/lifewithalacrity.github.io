<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayesian on Life With Alacrity</title>
    <link>http://www.lifewithalacrity.com/tags/bayesian/index.xml</link>
    <description>Recent content in Bayesian on Life With Alacrity</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://www.lifewithalacrity.com/tags/bayesian/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Collective Choice: Experimenting with Ratings</title>
      <link>http://www.lifewithalacrity.com/2007/01/collective_choi.html</link>
      <pubDate>Mon, 01 Jan 2007 22:38:15 -0700</pubDate>
      
      <guid>http://www.lifewithalacrity.com/2007/01/collective_choi.html</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-size: 0.7em;&#34;&gt;by Christopher Allen &amp;amp; Shannon Appelcline&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;[This is the fourth in a series of articles on &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html&#34;&gt;collective choice&lt;/a&gt;, co-written by my collegue &lt;a href=&#34;http://www.skotos.net/about/staff/shannon_appelcline&#34;&gt;Shannon Appelcline&lt;/a&gt;. It will be &lt;a href=&#34;http://www.skotos.net/articles/TTnT_179.phtml&#34;&gt;jointly posted&lt;/a&gt; in Shannon&#39;s &lt;a href=&#34;http://www.skotos.net/articles/TTnT.shtml&#34;&gt;Trials, Triumphs &amp;amp; Trivialities&lt;/a&gt; online games column at &lt;a href=&#34;http://www.skotos.net/&#34;&gt;Skotos&lt;/a&gt;.]&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Last year in &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/collective_choi.html&#34;&gt;Collective Choice: Rating Systems&lt;/a&gt; we took a careful look at eBay and other websites that collect ratings, and used those systems as examples to highlight a number of theories about how to make rating systems more useful.&lt;/p&gt;
&lt;p&gt;We suggested three main methods for improving rating systems:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Granular Ratings:&lt;/strong&gt; Based on the clumping of ratings to high values, we believed that ratings could be made more useful by increasing the size of a rating scale. Most rating scales are 5-point ranges, so we suggested a 10-point range instead.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distinct Ratings:&lt;/strong&gt; Raters can be somewhat arbitrary in how they rate items, varying both from each other and even from themselves (usually over multiple sessions). Thus we believed that providing explicit statements of what each number meant could improve ratings.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistical Ratings:&lt;/strong&gt; Finally we stated that in low volumes ratings could be biased by various quirks of data entry, either malevolent or not, and that ratings could be improved with strong statistical methods being used to polish up data and automatically keep &amp;quot;bad&amp;quot; data in line with &amp;quot;good&amp;quot;.&lt;/p&gt;
&lt;p&gt;In the year since we wrote that article we&#39;ve decided to practice what we preach and have rolled out an entirely new rating system called &lt;a href=&#34;http://index.rpg.net&#34;&gt;The RPGnet Gaming Index&lt;/a&gt;. We&#39;ve applied all of the above theories and thus far it looks like they&#39;re not only working, but that they&#39;re actually providing better rating systems than previous ones we&#39;ve used at the RPGnet site.&lt;/p&gt;
&lt;p&gt;In this article we&#39;re going to step through the data we&#39;ve collected from this experience and see how it applies to our theory: first by looking at our previous RPGnet rating system, then by looking at the new system, and finally by by examining the data from these two systems and comparing their results. We&#39;ve also run into some unexpected troubles along the way, and we&#39;ll talk about that too.&lt;/p&gt;
&lt;h3&gt;The RPGnet Reviews System&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.rpg.net&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/rpgnetlogo_1.gif&#34; title=&#34;Rpgnetlogo_1&#34; alt=&#34;Rpgnetlogo_1&#34; style=&#34;margin: 0px 5px 5px 0px; float: left;&#34; /&gt;&lt;/a&gt;
RPGnet is our gaming site for tabletop roleplaying—games like &lt;em&gt;Dungeons &amp;amp; Dragons&lt;/em&gt; and &lt;em&gt;Vampire: The Masquerade&lt;/em&gt;. We purchased it in 2001 from the original owners. One of the benefits of RPGnet was that it had a very large community. As of today it sports one of the top-100 forums on the Internet, with over 1000 simultaneous users regularly logging in. However, because of its maturity, we also inherited many existing systems.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.rpg.net/reviews/archive/9/9971.phtml&#34;&gt;&lt;img border=&#34;0&#34; alt=&#34;Rpgnet_review_summary_1&#34; title=&#34;Rpgnet_review_summary_1&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/rpgnet_review_summary_1.jpg&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
One of these was the &lt;a href=&#34;http://www.rpg.net/reviews/&#34;&gt;RPGnet Reviews System&lt;/a&gt; which gave individual users the ability to review gaming products—mostly role-playing games, but also board games, books, DVDs, and a smattering of related products.&lt;/p&gt;
&lt;p&gt;Most of these reviews are submitted by average readers who just want to talk about a product that they like (or don&#39;t), though a fair percentage are instead submitted by staff reviewers. (Overall at least 26% of our reviews are based on publisher &amp;quot;comp&amp;quot; copies, and thus may be considered largely professional, while the other 74% may or may not be.) The large community size of RPGnet applies to the Reviews System as well: currently it features 8,505 published reviews.&lt;/p&gt;
&lt;p&gt;Looking at the RPGnet Reviews through our three filters we find the following:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Granularity.&lt;/strong&gt; The ratings from our existing reviews aren&#39;t as granular as we&#39;d like. We have a theoretical scale of 2-10, but that&#39;s based upon a Style rating of 1-5 and a Substance rating of 1-5.&lt;/p&gt;
&lt;table cellpadding=&#34;5&#34;&gt;
&lt;thead&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;&lt;strong&gt;Rating&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Style&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Substance&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;%&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;81&lt;/td&gt;
&lt;td&gt;225&lt;/td&gt;
&lt;td&gt;1.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;732&lt;/td&gt;
&lt;td&gt;651&lt;/td&gt;
&lt;td&gt;8.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;2364&lt;/td&gt;
&lt;td&gt;1777&lt;/td&gt;
&lt;td&gt;24.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;3618&lt;/td&gt;
&lt;td&gt;3525&lt;/td&gt;
&lt;td&gt;42.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1709&lt;/td&gt;
&lt;td&gt;2326&lt;/td&gt;
&lt;td&gt;23.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt; Approximately 90% of raters rate only with values of 3-5, and thus our scale is more limited than the 2-10 range would indicate. 42.9% of reviews further rate Style and Substance exactly the same, suggesting that not everyone sees a difference between these two elements. On the whole this scale isn&#39;t as a bad as a singular 5-point scale, but it also isn&#39;t a real 10-point scale, and the two orthogonal types of comparison don&#39;t necessarily provide a coherent description of a product.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distinctiveness.&lt;/strong&gt; Conversely, the review ratings are fairly distinct because the Review System provides an explanation of what each rating number means. For example the five Substance ratings are: I Wasted My Money (1); Sparse (2); Average (3); Meaty (4); Excellent(5). The descriptions could be better, but hopefully they connect to some users in meaningful ways, and help them to rate consistently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistics.&lt;/strong&gt; Our review ratings have no statistical basis. These values are used entirely unfiltered.&lt;/p&gt;
&lt;p&gt;On the whole, the existing RPGnet Reviews embodied slightly less than half of what we wanted to see in a rating systems: some improvement over a simple 5-point scale; some effort put into making individual ratings distinct; and nothing statistical.&lt;/p&gt;
&lt;p&gt;There is room for improvement, however, as we&#39;ll see when we analyze this system more fully.&lt;/p&gt;
&lt;h3&gt;The RPGnet Gaming Index&lt;/h3&gt;
&lt;p&gt;Our newer system is the &lt;a href=&#34;http://index.rpg.net/&#34;&gt;RPGnet Gaming Index&lt;/a&gt;. It doesn&#39;t supersede our Reviews, but instead offers a complementary look at the roleplaying field. The Index is essentially an RPG industry database. It contains individual entries for many different gamebooks—currently 5248—and allows registered users to rate each of them. Those ratings are then turned into averages by various mathematical formulas on a nightly basis and the roleplaying games in our index are then ranked.&lt;/p&gt;
&lt;p&gt;The large size of RPGnet has allowed us to very quickly turn our ideas of a Gaming Index into reality. Just six months after release we have:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;5248 well-written Index entries &lt;/li&gt;
&lt;li&gt; 5908 different editions&lt;/li&gt;
&lt;li&gt; 4240 authors&lt;/li&gt;
&lt;li&gt; 4478 covers&lt;/li&gt;
&lt;li&gt; 360 different game systems&lt;/li&gt;
&lt;li&gt; 345 series&lt;/li&gt;
&lt;li&gt; 10142 individual ratings&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;Most of the ratings are clumped around the best and worst games, with many less popular games unrated as of yet. Four different items have at least 80 ratings each (&lt;em&gt;Call of Cthulhu&lt;/em&gt;, &lt;em&gt;Exalted&lt;/em&gt;, &lt;em&gt;Nobilis&lt;/em&gt;, and &lt;em&gt;Unknown Armies&lt;/em&gt;). Our average rating is 6.79. Ratings above 7.82 are in the 99th
percentile, ratings above 7.21 are in the 90th percentile, and ratings
below 6.53 are beneath the 10th percentile.&lt;/p&gt;
&lt;p&gt;(For more info on the creation of the RPG Index, and how to encourage user generated content, see Shannon&#39;s articles, &amp;quot;Managing User Creativity&amp;quot;, &lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_192.phtml&#34;&gt;Part One&lt;/a&gt; and &lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_193.phtml&#34;&gt;Part Two&lt;/a&gt;.) &lt;/p&gt;
&lt;p&gt;The RPGnet Index also handles some unusual situations, such as when a game book contains other game books as part of an anthology or compilation. For instance, the 8-book compilation &lt;a href=&#34;http://index.rpg.net/display-entry.phtml?mainid=64&#34;&gt;In Search of Adventure&lt;/a&gt; has a composite rating of &lt;a href=&#34;http://index.rpg.net/display-entry-ratings.phtml?mainid=64&#34;&gt;6.57&lt;/a&gt; which is partially based upon the individual adventures that make it up.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Granularity:&lt;/strong&gt; The first thing we did was provide a 10-point scale for this new system.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distinctiveness:&lt;/strong&gt; We also made sure each point of the scale was clearly defined. Currently the points of our scale are: Worthless (1), Poor (2), Some Flaws (3), Almost Average (4), Average (5), Above Average (6), Good (7), Very Good (8), Outstanding (9), and One of the Best Ever (10).&lt;/p&gt;
&lt;p&gt;We made some mistakes in our original release of our &amp;quot;distinctive&amp;quot; titles, and we discovered this had real effects on the user input, telling us that these title labels &lt;em&gt;are&lt;/em&gt; meaningful to users.&lt;/p&gt;
&lt;p&gt;First, we initially labeled 6 as &amp;quot;average&amp;quot;, to mirror the rating system for our existing Reviews, rather than setting 5 to be average. But as we noted in our first article, people like to be nice, and thus they tend to rate on the good side of a scale. Changing the label for our definition of average from 6 to 5 has slowly started dropping the average of all ratings down as a result (providing more breadth, a topic we&#39;ll talk about more shortly).&lt;/p&gt;
&lt;p&gt;Second, two of our original distinctive titles were at odds with the others. Our original &amp;quot;2&amp;quot; value said that the game had &amp;quot;a few useful elements&amp;quot; and our original &amp;quot;9&amp;quot; value said that it was the &amp;quot;best of the year&amp;quot;. The 2 was much more specific than any of our other terms and the 9 created a comparative query that was very different from anything else. Overall our ratings conformed to a bell curve centered between 6 and 7, but we saw very clear dropouts in our curve at 2 and 9, telling us that we&#39;d made mistakes in those terms, and that people were less willing to use them as a result. Since we&#39;ve made the change to our current set of titles those two discontinuities have disappeared.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistics.&lt;/strong&gt; Finally, we fully integrated statistics into our new Index by using two main methods: bayesian weights and trust.&lt;/p&gt;
&lt;p&gt;We explained bayesian weights pretty fully in our previous article. Here&#39;s what we said then:&lt;/p&gt;
&lt;blockquote&gt;
&lt;table width=&#34;90%&#34; border=&#34;1&#34; cellpading=&#34;3&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;p&gt;
The idea behind a bayesian average is that you normalize ratings by pushing them toward the average rating for your site, and you do that more for items with fewer ratings than those with more ratings. The basic formula looks like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
b(r) = [ W(a) * a + W(r) * r ] / (W(a) + W(r)]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;r = average rating for an item&lt;/p&gt;
&lt;p&gt;W(r) = weight of that rating, which is the number of ratings&lt;/p&gt;
&lt;p&gt;a = average rating for your collection&lt;/p&gt;
&lt;p&gt;W(a) = weight of that average, which is an arbitrary number, but should be higher if you generally expect to have more ratings for your items; 100 is used here, for a database which expects many ratings per item&lt;/p&gt;
&lt;p&gt;b(r) = new bayesian rating &lt;/p&gt;
&lt;p&gt;Say three &amp;quot;shill&amp;quot; users had come onto your site and rated a brand new indie film a &amp;quot;10&amp;quot; because the producer asked them to. However, you use a bayesian average with a weight of 100, and thus 3 ratings won&#39;t move the movie very far from the average site rating of 6.50:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
b(r) = [100 * 6.50 + 3 * 10] / (100 + 3)&lt;/code&gt;&lt;code&gt;&lt;br /&gt;b(r) = 680 / 103&lt;/code&gt;&lt;code&gt;&lt;br /&gt;b(r) = 6.60
&lt;/code&gt;
&lt;/p&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/blockquote&gt;
&lt;p&gt;We implemented bayesian weights exactly as we&#39;d detailed, but with a lower weight of 25. Since then we&#39;ve accrued over 10,000 ratings in the database, and we can probably start thinking about cranking that weight up, another topic we&#39;ll return to.&lt;/p&gt;
&lt;p&gt;Our trust-based algorithms suggest that some ratings are better than others, and should thus be more trusted (and thus more weighted when we calculate the average rating of an item). Though bayesian weights have been used before, we&#39;re not aware of other systems that weight ratings based on trust. &lt;/p&gt;
&lt;p&gt;The calculation of trust is very simple:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
Weight = 0 if #ratings(user) &amp;lt;= 2&lt;/code&gt;&lt;code&gt;&lt;br /&gt;Otherwise Weight = #ratings(user) / 50 to a maximum of 2&lt;/code&gt;&lt;code&gt;&lt;br /&gt;Weight *= 2, to a maximum of 4, if the user included a comment
&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;This was based on the idea that the average good rater would rate 25 different items and the average great rater would rate at least 50. Additionally, we believed that ratings with comments were more likely to be thoughtful than those without.&lt;/p&gt;
&lt;p&gt;That, overall, is a quick picture of what we&#39;ve done with the RPGnet Gaming Index. Some of these ideas were laid out from the start, and others have been tuned as we progressed. &lt;/p&gt;
&lt;p&gt;So how did we do, particularly in comparison to our existing RPGnet Reviews System?&lt;/p&gt;
&lt;h3&gt;The Comparison&lt;/h3&gt;
&lt;p&gt;One of our goals in improving rating systems has been to widen the range of possible input. As we noted earlier we discovered that 90% of our RPGnet Reviews Ratings were in the 3-5 range, and only 10% in the 1-2 range.&lt;/p&gt;
&lt;p&gt;Generally, we can measure the success of widening a range by seeing whether the average rating of a database moves toward the &lt;em&gt;true&lt;/em&gt; average. For the purposes of a 10-point scale from 1-10, that&#39;s a desired value of 5.5. That generally means we&#39;re looking for our average rating to &lt;em&gt;decrease&lt;/em&gt; because people tend to rate high.&lt;/p&gt;
&lt;p&gt;The following table compares the average results of Reviews ratings and Index ratings.&lt;/p&gt;
&lt;center&gt;&lt;table cellpadding=&#34;3&#34; border=&#34;1&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Database&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Average&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Converted Reviews&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;7.25&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Massaged Reviews&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;7.29&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Unweighted Index&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;7.10&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Weighted Index&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;6.78&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;p&gt;Here&#39;s what the categories in the above chart represent:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Converted Reviews:&lt;/strong&gt; The Style + Substance of the Reviews, converted from its 2-10 scale to a 1-10 scale: &lt;/p&gt;
&lt;p&gt;&lt;code&gt;
$rating = avg($style) + avg($substance);&lt;/code&gt;&lt;br /&gt;&lt;code&gt;$rating = ($rating * 1.125) - 1.25;
&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Massaged Reviews:&lt;/strong&gt; The Style + Substance of the Reviews, with Substance given double weight over Style because we think that more closely reflects the intentions of the reviewer, converted from its 2-10 scale to a 1-10 scale:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
$rating = (average($style) + 2*average($substance))/1.5;&lt;/code&gt;&lt;br /&gt;&lt;code&gt;$rating = ($rating * 1.125) - 1.25;
&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unweighted Index:&lt;/strong&gt; Index ratings exactly as users have entered into our Gaming Index:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
$rating = average($index-rating);
&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Weighted Index:&lt;/strong&gt; Index ratings adjusted by the weight of each individual rating, which is based on user trust and inclusion of comments:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
$rating = average($index-rating*$index-weight)/average($index-weight);
&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;Our average rating—which is our criteria for success—decreased somewhat from the Reviews System to the Gaming Index and it decreased much more dramatically when we introduced our trust systems.&lt;/p&gt;
&lt;p&gt;The following chart shows the a typical example of how review and index ratings differ, using the venerable &lt;em&gt;Dungeons &amp;amp; Dragons Player&#39;s Handbook&lt;/em&gt; as an example:&lt;/p&gt;
&lt;center&gt;&lt;a href=&#34;http://index.rpg.net/display-entry-ratings.phtml?mainid=72&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/dd_players_handbook_rpgnet_reviews_only.jpg&#34; title=&#34;Dd_players_handbook_rpgnet_reviews_only&#34; alt=&#34;Dd_players_handbook_rpgnet_reviews_only&#34; /&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/dd_players_handbook_index_ratings_only.jpg&#34; title=&#34;Dd_players_handbook_index_ratings_only&#34; alt=&#34;Dd_players_handbook_index_ratings_only&#34; /&gt;&lt;/a&gt;&lt;/center&gt;&lt;p&gt;For this book the median ratings from reviews-only is 8, and the median from index-only is 7. A one-to-two point drop in median rating from reviews to index was consistent in all of our most-rated games other than those which were a rated a &amp;quot;10&amp;quot; in both places.&lt;/p&gt;
&lt;p&gt;We believe that this initial success of our unweighted Gaming Index can be attributed to the slightly better &lt;em&gt;granularity&lt;/em&gt;—a 10-point scale versus two 5-point scales—and our improved &lt;em&gt;distinctiviness&lt;/em&gt;—based on better naming of the rating levels. The veracity of this will ultimately be played out as the Index grows.&lt;/p&gt;
&lt;p&gt;However we have no doubt that our &lt;em&gt;statistical&lt;/em&gt; approach to the index data, when we moved from our unweighted Index to our weighted Index, is providing even better results. We had theorized that users who input more and who include comments would provide &amp;quot;better&amp;quot; data, and by our criteria of the average of the ratings moving toward 5.5 that seems to be borne out. The following table looks at the information a bit more precisely, by comparing average ratings as total number of ratings increases over several ranges:&lt;/p&gt;
&lt;center&gt;&lt;table cellpadding=&#34;3&#34; border=&#34;1&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;# of Ratings&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Average w/Comment&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Average w/o Comment&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;1-2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;8.55&lt;/td&gt;
&lt;td&gt;8.88&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;3-24&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;8.08&lt;/td&gt;
&lt;td&gt;8.16&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;25-49&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;7.32&lt;/td&gt;
&lt;td&gt;7.11&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;50-99&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;7.14&lt;/td&gt;
&lt;td&gt;7.03&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;100+&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;6.17&lt;/td&gt;
&lt;td&gt;6.99&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;p&gt;This table fairly definitively shows that base maxim: that the breadth of the ratings, and thus their quality, increases the more ratings a user makes. The improved quality of ratings with comments is less definitive. Among the vast mass of users the two values are pretty close, and sometimes the reverse of what we expect, but for the best and the worst users, ratings with comments seem to be better than those without. This latter point is another one that we&#39;ll have to continue to monitor as the Index grows beyond its current total of 10,000 ratings.&lt;/p&gt;
&lt;p&gt;The other major element of our &lt;em&gt;statistical&lt;/em&gt; approach to the Index is our bayesian weight. The following chart shows a top-ten chart for roleplaying games calculated via four different methodologies: our Reviews; our Index with no weighting; our Index with a 25 bayesian weighting (as it currently stands); and our Index with a 50 bayesian weighting:&lt;/p&gt;
&lt;center&gt;&lt;table cellpadding=&#34;3&#34; border=&#34;1&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Reviews-Only&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0-weight Index&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;25-weight Index&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;50-weight Index&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Delta Green: Countdown&lt;/td&gt;
&lt;td&gt;The Chronicles of Talislanta&lt;/td&gt;
&lt;td&gt;Delta Green: Countdown&lt;/td&gt;
&lt;td&gt;Delta Green&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Nobilis&lt;/td&gt;
&lt;td&gt;Wildside&lt;/td&gt;
&lt;td&gt;Spirit of the Century&lt;/td&gt;
&lt;td&gt;Delta Green: Countdown&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Castle Falkenstein&lt;/td&gt;
&lt;td&gt;Devil&#39;s Due&lt;/td&gt;
&lt;td&gt;Delta Green&lt;/td&gt;
&lt;td&gt;Unknown Armies&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Vimary Sourcebook&lt;/td&gt;
&lt;td&gt;Lodges: The Faithful&lt;/td&gt;
&lt;td&gt;Unknown Armies&lt;/td&gt;
&lt;td&gt;Call of Cthulhu&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Liber Servitorum&lt;/td&gt;
&lt;td&gt;Apocalypse&lt;/td&gt;
&lt;td&gt;Call of Cthulhu&lt;/td&gt;
&lt;td&gt;Nobilis&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Ork!&lt;/td&gt;
&lt;td&gt;Earthdawn Gamemaster&#39;s Compendium&lt;/td&gt;
&lt;td&gt;Nobilis&lt;/td&gt;
&lt;td&gt;Spirit of the Century&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;7&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;GURPS Russia&lt;/td&gt;
&lt;td&gt;Into the Badlands&lt;/td&gt;
&lt;td&gt;Pendragon&lt;/td&gt;
&lt;td&gt;Over the Edge&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;8&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;GURPS Reign of Steel&lt;/td&gt;
&lt;td&gt;Earthdawn Player&#39;s Compendium&lt;/td&gt;
&lt;td&gt;Over the Edge&lt;/td&gt;
&lt;td&gt;Pendragon&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;9&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Cudgel&#39;s Compendium&lt;/td&gt;
&lt;td&gt;Chronicle of the Black Labyrinth&lt;/td&gt;
&lt;td&gt;Mutants &amp;amp; Masterminds&lt;/td&gt;
&lt;td&gt;Mutants &amp;amp; Masterminds&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;10&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Corum&lt;/td&gt;
&lt;td&gt;The Spell Book&lt;/td&gt;
&lt;td&gt;Pulp Hero&lt;/td&gt;
&lt;td&gt;Vimary Sourcebook&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;p&gt;We actually &lt;em&gt;did&lt;/em&gt; do a little bit of statistical analysis on the Reviews because on our first try to produce this chart we got a random clump of reviews that were 5/5 from a much larger pool, so we further ordered them by descending total count of reviews, and as a result you&#39;re seeing a better selection of ranked reviews than a truly unstatistical sampling would allow. We did the same for the unweighted Index (which clumped a number of results at &amp;quot;10&amp;quot;), except we further ordered items at the same weight by decreasing number of views (another statistical decision).&lt;/p&gt;
&lt;p&gt;Clearly, deciding which of these lists is &amp;quot;right&amp;quot; is a much more subjective measure than the mathematical analysis we were able to apply to earlier problems. However, most roleplayers would tell you that the unweighted Reviews and Index lists are terrible. The top 5 items in the Reviews list actually aren&#39;t bad for a starting list of good games—but only because we did the aforementioned statistical ordering. Before that we just had a random listing of gaming items. Even with our attempts at quickie statistical analysis the unweighted Index is still quite bad, with only &lt;em&gt;Talislanta&lt;/em&gt; regularly showing up on other &amp;quot;best&amp;quot; lists.&lt;/p&gt;
&lt;p&gt;The problem is the ability of one person to come in and rate an item a &amp;quot;10&amp;quot; (or a &amp;quot;5&amp;quot;/&amp;quot;5&amp;quot;), thereby making that item more highly rated than &lt;em&gt;any&lt;/em&gt; item which has an actual consensus of ratings. Of our unweighted top Reviews only the top three had more than 2 reviews and the rest had 2. Not surprisingly those top three were the best fits to a typical top-ten list. Of the unweighted Index only the top three had more than 1 rating, and the rest had 1. Our single good pick was in those top three.&lt;/p&gt;
&lt;p&gt;Our 25-weight Index, which is what we currently use, has been generally accepted by the RPGnet community as a good marker of what&#39;s good and what&#39;s not. However there have been two items on it which some percentage of people disagree with: &lt;em&gt;Spirit of the Century&lt;/em&gt; and &lt;em&gt;Pulp Hero&lt;/em&gt;. It&#39;s instructive to see that when we increase to a 50-weight Index &lt;em&gt;Spirit of the Century&lt;/em&gt; drops (even more notably than depicted here, because its actual rating changes from .01 from first place to .16 from first place) and &lt;em&gt;Pulp Hero&lt;/em&gt; disappears entirely.&lt;/p&gt;
&lt;p&gt;The questions of &lt;em&gt;what&lt;/em&gt; to set your bayesian weight to, when to increase it, and what maximum value to set it to are all relatively unstudied and thus we don&#39;t have good answers to them. As we pass 10,000 ratings we&#39;re considering upping the bayesian value to 50. We expect that 100 will be our ultimate value when the Index is fully mature, however if we increase the weight too far an older, less rated game will never be able to get enough weight to get out of the doldrums.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We&#39;re by no means done with this ratings experiment. Though we&#39;ve pleased and impressed with the growth of the RPGnet Index thus far, by next year we hope that the Index will include the vast majority of all games in print (as opposed to somewhat less than half now) and that our 10,000 ratings will grow to 50,000 or more. This will allow us to offer even more definitive answers to our questions.&lt;/p&gt;
&lt;p&gt;In the meantime we&#39;re still mucking with our statistics and facing new problems.&amp;nbsp; Some of the newest:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;What to do about drive-by ratings:&lt;/strong&gt; Our trust algorithm does a good job of making drive-by ratings, where a publisher points his audience to an item in our site, mostly irrelevant, but there&#39;s some concern that they could have more effect in the long run.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How to incorporate our review ratings in our index ratings:&lt;/strong&gt; It seems a shame to waste the thousands of reviews that have been written—and indeed currently they&#39;re calculated into a composite rating we use in the Index—but we&#39;re realizing that people have very different purposes for writing reviews and inputing ratings, which may result in some of the upward skew we see on the review side of things. Ultimately we need to decide whether they&#39;re just too different or whether our statistical massaging is enough to incorporate those reviews into a composite Index rating.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How to pick some of our numbers:&lt;/strong&gt; As we already noted we don&#39;t have good formulas for when to choose which bayesian weights. Likewise we&#39;ve been guessing at which values to use for the trust-based weighting of our raters. Originally we set our desired rating count to 100 for good rater and 200 for great raters, but we&#39;ve since dropped those to 50 for good and 100 for great based upon the real numbers of ratings that users were making. Again, we&#39;d prefer to derive an actual formula for this type of calculation&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;Shannon has discussed some of these issues more in his recent article &lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_198.phtml&#34;&gt;More Thoughts Abour Ratings&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Despite unanswered questions, we still feel good about the basic ideas we laid out in our article last year. We have no doubt that giving our ratings a &lt;em&gt;statistical&lt;/em&gt; basis has dramatically improved them and evidence thus far suggests that both &lt;em&gt;granularity&lt;/em&gt; and &lt;em&gt;distinctiveness&lt;/em&gt; have been helpful as well.&lt;br /&gt;
&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Related articles from this blog:&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html&#34;&gt;2005-12: Systems for Collective Choice&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/collective_choi.html&#34;&gt;2005-12: Collective Choice: Rating Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2006/01/ranking_systems.html&#34;&gt;2006-01: Collective Choice: Competitive Ranking Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2006/08/using_5star_rat.html&#34;&gt;2006-08: Using 5-Star Rating Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;p&gt;Related articles from Shannon Appelcline&#39;s &lt;a href=&#34;http://www.skotos.net/articles/show-column.phtml?colname=TTnT_&#34;&gt;Trials, Triumphs &amp;amp; Trivialities:&lt;/a&gt;
&lt;/p&gt;&lt;blockquote&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_192.phtml&#34;&gt;#192: Managing User Creativity, Part One&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_193.phtml&#34;&gt;#193: Managing User Creativity, Part Two&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_196.phtml&#34;&gt;#196: Collective Choice: Ratings, Who Do You Trust?&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_198.phtml&#34;&gt;#198: Collective Choice: More Thoughts About Ratings&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;footer&gt;&lt;h3&gt;Comments&lt;/h3&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;URL:
0-10 scale is too large for most relevant human ratings systems. maybe i should pull the psych papers, but people tend to do a 1-5 or 0-5 scale better. it&#39;s a sign of immaturity to only use the endpoints.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;#&#34;&gt;Grumpy&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2007-12-18T10:24:24-07:00&#34;&gt;2007-12-18T10:24:24-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;/footer&gt;
&lt;p class=&#34;previous&#34;&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/previous/2007/01/collective_choi.html&#34; rel=&#34;syndication&#34; class=&#34;u-syndication&#34; &gt;orginal layout&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Collective Choice: Rating Systems</title>
      <link>http://www.lifewithalacrity.com/2005/12/collective_choi.html</link>
      <pubDate>Mon, 12 Dec 2005 17:58:34 -0700</pubDate>
      
      <guid>http://www.lifewithalacrity.com/2005/12/collective_choi.html</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-size: 0.7em;&#34;&gt;by Christopher Allen &amp;amp; Shannon Appelcline&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;[This is the second of a series of articles on &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html&#34;&gt;collective choice&lt;/a&gt;, co-written by my collegue &lt;a href=&#34;http://www.skotos.net/about/staff/shannon_appelcline&#34;&gt;Shannon Appelcline&lt;/a&gt;. It will be &lt;a href=&#34;http://www.skotos.net/articles/TTnT_179.phtml&#34;&gt;jointly posted&lt;/a&gt; in Shannon&#39;s &lt;a href=&#34;http://www.skotos.net/articles/TTnT.shtml&#34;&gt;Trials, Triumphs &amp;amp; Trivialities&lt;/a&gt; online games column at &lt;a href=&#34;http://www.skotos.net/&#34;&gt;Skotos&lt;/a&gt;.]&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In our previous article we talked about the many systems available for &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html&#34;&gt;collective choice&lt;/a&gt;. There are &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html#selection_systems&#34;&gt;selection systems&lt;/a&gt;, which are primarily centered on voting and deliberation, &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html#opinion_systems&#34;&gt;opinion systems&lt;/a&gt;, which represent how voting could occur, and finally &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html#comparison_systems&#34;&gt;comparison systems&lt;/a&gt;, which rank or rate different people or things in a simple, comparative manner.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://lifewithalacrity.blogs.com/.shared/image.html?/photos/uncategorized/stars_1.gif&#34; onclick=&#34;window.open(this.href, &#39;_blank&#39;, &#39;width=373,height=239,scrollbars=no,resizable=no,toolbar=no,directories=no,location=no,menubar=no,status=no,left=0,top=0&#39;); return false&#34;&gt;&lt;img width=&#34;200&#34; height=&#34;128&#34; border=&#34;0&#34; alt=&#34;Stars_1&#34; title=&#34;Stars_1&#34; src=&#34;http://www.lifewithalacrity.com/previous/images/stars_1.gif&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;One purpose of our previous article was to create a dictionary of terms for talking about these related, but clearly different, systems. Another was to start offering analyses of these systems, many of which had not been well studied before their introduction onto the Internet. &lt;/p&gt;
&lt;p&gt;However at best our previous article provided an overview of what should be further investigated in each system. This article provides more in-depth coverage of one of the systems we previously outlined: &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html#rating_systems&#34;&gt;rating systems&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As we wrote in our previous article, in comparison rating systems &lt;em&gt;&amp;quot;the value of individual items (most frequently goods) rise or fall based upon the largely subjective judgment of individual users.&amp;quot;&lt;/em&gt; Ratings systems should be clearly differentiated from the closely related &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html#ranking_systems&#34;&gt;ranking systems&lt;/a&gt;. Ratings systems have a more subjective component, while ranking systems are largely objective. Amazon, Netflix, BoardGameGeek, and even the Stock Market were offered up as examples of ratings systems. Another example of a comparison rating system, and one of the earliest that appeared on the modern Internet, is eBay. The techniques they use are now beginning to show their age.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;eBay: A Failed Rating Experiment&lt;/h3&gt;
&lt;p&gt;&lt;img border=&#34;0&#34; alt=&#34;Ebaysales&#34; title=&#34;Ebaysales&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/ebaysales.jpg&#34; style=&#34;margin: 0px 5px 5px 0px; float: left;&#34; /&gt;Most rating systems center around rating content, often user-contributed content, and they frequently help apply community values and acclaim to that content. However, the idea of ratings can go far beyond that narrow niche (though that will doubtless be its greatest use as the Internet continues to expand). Early Internet site, &lt;a href=&#34;http://www.ebay.com&#34;&gt;eBay&lt;/a&gt;, was one of the first to widely use user-submitted ratings, and it used them for a different manner: to determine the good traders on their auction site.&lt;/p&gt;
&lt;p&gt;Unfortunately, as one of the first in this field, eBay made many mistakes which now leave their ratings system only slightly helpful. However, its failures can also provide us with insights in creating new rating systems on the Internet.&lt;/p&gt;
&lt;p&gt;eBay allows you to leave positive, negative, or (more recently) neutral feedback for each transaction you conduct in their society. These are aggregated into two numbers. &amp;quot;Feedback Score&amp;quot; is calculated as unique positive feedback received minus unique negative feedback received, and results in a whole number like &amp;quot;32&amp;quot; or &amp;quot;10,302&amp;quot;. &amp;quot;Positive Feedback&amp;quot; is calculated as positive feedback received divided by all feedback received, and results in a percentage like &amp;quot;100%&amp;quot; or &amp;quot;99.8%&amp;quot;.&lt;/p&gt;
&lt;p&gt;Unfortunately, for reasons discussed below, almost all feedback is positive, and thus the Feedback Score acts almost entirely as a track record of how many trades someone has made. The Feedback Score could be largely replaced by that single number. You can look at a score of &amp;quot;27&amp;quot;, and say, &amp;quot;That&#39;s an amateur trader, or someone just getting started&amp;quot;, at a score of &amp;quot;3&amp;quot;, and say, &amp;quot;That person may or may not know what they&#39;re doing&amp;quot;, at a score of &amp;quot;10,302&amp;quot;, and say, &amp;quot;That person has done a lot of trades.&amp;quot; But you still don&#39;t know how good the trader is.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/ebayprofile.jpg&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/ebayprofile.jpg&#34; title=&#34;Ebayprofile&#34; alt=&#34;Ebayprofile&#34; class=&#34;image-full&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;Theoretically, the Positive Feedback percentage should give a more meaningful number, but people so infrequently give bad ratings that, even when they do appear, they look like noise. Does a percentage of &amp;quot;99.8%&amp;quot; on a user with a score of &amp;quot;1,762&amp;quot; mean that the seller has a genuine problem or not? Do those 3 unhappy customers really represent another 30 who were unwilling to actually click the negative feedback? And, did those people have slightly bad experience or really bad experiences? It&#39;s pretty hard to say.&lt;/p&gt;
&lt;p&gt;Overall, eBay has a few major problems with their rating system:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It&#39;s &lt;strong&gt;non-granular&lt;/strong&gt;, with only two options (positive/negative), or more recently three (positive/negative/neutral).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It&#39;s &lt;strong&gt;non-distinct&lt;/strong&gt;, with no useful guidelines on what behaviors should result in each rating.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It&#39;s &lt;strong&gt;non-statistical&lt;/strong&gt;, and thus ends up showing only a gross number of sales, not a real subjective measure.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It&#39;s &lt;strong&gt;bilateral&lt;/strong&gt;, with buyers and sellers rating each other simultaneously, and thus people are afraid to give bad ratings lest they get them in return.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It&#39;s &lt;strong&gt;meaningless&lt;/strong&gt;, because there are no good tools to control who bids on an auction based on Feedback numbers. (Technically it may be legitimate to ban low feedback bidders from an auction, then cancel their bids if they enter the auction, but this is neither obvious, automatic, nor simple.)
&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;We&#39;re going to address each of these issues in turn, to offer insight into creating new comparison rating systems. The first three topics--granularity, distinction, and a statistical basis--are the most important elements of a good comparison rating system. Bilateral &amp;amp; meaningfulness issues will only be relevant on certain sites.&lt;/p&gt;
&lt;p&gt;(As a final caveat: in some ways eBay falls closer in ultimate result to a &lt;em&gt;reputation system&lt;/em&gt;, a topic which we&#39;ll be covering more in a few articles down the road, but its lessons learned are still entirely accurate for rating systems of all sorts.)&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;Granular Ratings&lt;/h3&gt;
&lt;p&gt;&lt;img border=&#34;0&#34; alt=&#34;Smiley&#34; title=&#34;Smiley&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/smiley.png&#34; style=&#34;margin: 0px 5px 5px 0px; float: left;&#34; /&gt;
In general, &lt;em&gt;people want to be nice&lt;/em&gt;. There are exceptions to that rule, perhaps even great numbers of them, but the average, well-adjusted person would prefer to make other people happy, not sad.&lt;/p&gt;
&lt;p&gt;This has a notable effect on any comparison rating system, because it means that people are less likely to use the bottom half of any rating scale. If you did a statistical run on eBay, you&#39;d certainly find that more than 99 out of every 100 ratings are positive. This is largely influenced by concerns of bilateral revenge, as discussed below, and the fact that eBay suggests other means of dispute resolution when you try and leave negative feedback. However, &lt;a href=&#34;http://www.rpg.net&#34;&gt;RPGnet&lt;/a&gt;, a roleplaying site which reviews games, comics, books, movies, and more shows a similar trend despite the lack of bilaterality.&lt;/p&gt;
&lt;p&gt;RPGnet uses two 5-points scales for reviews, resulting in a total rating of 2-10. Of all the ratings at RPGnet, 6,983 reviews have a total that&#39;s above average, a total rating of 6 or more, and 795 have a total that&#39;s below average, a total rating of 5 or less. Perhaps there are more people who sit down to write a review because they really like a game than those who do so because they really hated it, but the result of ~90% of reviews being above average is still stunning.&lt;/p&gt;
&lt;p&gt;The following table shows all the ratings for each of the two categories that RPGnet uses, &amp;quot;Style&amp;quot; and &amp;quot;Substance&amp;quot;:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.rpg.net/reviews/archive/9/9971.phtml&#34;&gt;&lt;img border=&#34;0&#34; alt=&#34;Rpgnetsettlersreview&#34; title=&#34;Rpgnetsettlersreview&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/rpgnetsettlersreview.png&#34; style=&#34;margin: 0px 0px 15px 5px; float: right;&#34; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;table cellpadding=&#34;5&#34;&gt;
&lt;thead&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;&lt;strong&gt;Rating&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Style&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Substance&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;%&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;73&lt;/td&gt;
&lt;td&gt;210&lt;/td&gt;
&lt;td&gt;1.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;687&lt;/td&gt;
&lt;td&gt;590&lt;/td&gt;
&lt;td&gt;8.2%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;2127&lt;/td&gt;
&lt;td&gt;1583&lt;/td&gt;
&lt;td&gt;23.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;3337&lt;/td&gt;
&lt;td&gt;3242&lt;/td&gt;
&lt;td&gt;42.2%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1554&lt;/td&gt;
&lt;td&gt;2153&lt;/td&gt;
&lt;td&gt;23.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;This evidence confirms what we&#39;d already suspected. Only 10% of raters use the bottom two ratings in a 5-point scale, and only 2% use the bottom rating. The median of the 5-point scale is actually the fourth point, with a neat bell curve arranged around it.&lt;/p&gt;
&lt;p&gt;Because users are innately unwilling to give bad ratings, as evidenced here, useful comparison ratings truly come about only through fractional differences between good ratings. In this case, the difference between &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, and &amp;quot;5&amp;quot; is meaningful, and becomes more meaningful as more ratings are entered. Eventually you can look at a ranked list of ratings and see that &amp;quot;4.2&amp;quot; is a good rating while &amp;quot;3.5&amp;quot; is not.&lt;/p&gt;
&lt;p&gt;In order to do this, however, you need enough levels of good ratings to be able to distinguish between them. eBay, only offering one positive rating, does not provide enough differentiation. RPGnet, with its three positive ratings, might. However, sites that offer a 10-point scale are the ones that really seem to be able to produce meaningful statistics. On those sites we can expect that 90% of users will choose between six different numbers, from &amp;quot;5&amp;quot; to &amp;quot;10&amp;quot;, and as the number of ratings builds up, this will produce enough differentiation to be meaningful. If you have already adopted a 5-point scale, consider allowing users to select the half-points, giving users a greater ability to differentiate their ratings.&lt;br /&gt; &lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;Distinct Ratings&lt;/h3&gt;
&lt;p&gt;No two users are ever going to rate the same; different rating numbers will mean different things to each person. This can introduce minor discrepencies into ratings, if a single individual rates particularly low or high. However, because most ratings are eventually used for comparisons, if that low- or high-rater rates many different things, the ratings equalize. &amp;quot;Item A&amp;quot; is rated low by this person, but so is &amp;quot;Item B&amp;quot;, and so they end up in the correct positions &lt;em&gt;in relation to each other&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A bigger problem occurs when an individual is inconsistent in his ratings over time. If an individual rates everything low for a while, then rates everything high, then he has a greater chance of biasing the overall rating pool. Worse, his individual ratings aren&#39;t meaningful, because you can&#39;t look at two items, see that one is a &amp;quot;6&amp;quot; and another is an &amp;quot;8&amp;quot;, and truly believe that he likes the &amp;quot;8&amp;quot; a fair amount more than the &amp;quot;6&amp;quot;. This reduces the usability of an individual recommendation system or a friends system where one user might look at what other users thought about products, because their unaggregated numbers are not accurate.&lt;/p&gt;
&lt;p&gt;You thus want to help individuals to stay consistent, and the best way to do that is to make the criteria for your ratings distinct. &lt;a href=&#34;http://www.boardgamegeek.com&#34;&gt;BoardGameGeek&lt;/a&gt;, a board game web site that supports a 10-point rating system for games, does a good job of offering distinction in its ratings. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;a href=&#34;http://www.boardgamegeek.com/game/13&#34;&gt;&lt;img border=&#34;0&#34; alt=&#34;Settlers_rating_1&#34; title=&#34;Settlers_rating_1&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/settlers_rating_1.png&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;10&lt;/strong&gt; - Outstanding. Always want to play and expect this will never change.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;9 &lt;/strong&gt;- Excellent game. Always want to play it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;8 &lt;/strong&gt;- Very good game. I like to play. Probably I&#39;ll suggest it and will never turn down a game.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;7 &lt;/strong&gt;- Good game, usually willing to play.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;6 &lt;/strong&gt;- Ok game, some fun or challenge at least, will play sporadically if in the right mood.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;5 &lt;/strong&gt;- Average game, slightly boring, take it or leave it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;4 &lt;/strong&gt;- Not so good, it doesn&#39;t get me but could be talked into it on occasion.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;3 &lt;/strong&gt;- Likely won&#39;t play this again although could be convinced. Bad.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2 &lt;/strong&gt;- Extremely annoying game, won&#39;t play this ever again.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1 -&lt;/strong&gt; Defies description of a game. You won&#39;t catch me dead playing this. Clearly broken.&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;
&lt;p&gt;If you offer a distinct rating listing like this, some users will still come up with their own rating ideas, but if they do, they&#39;re more likely to remember what each number means to them. For everyone else, a very clear, s rating system is the most likely to produce meaningful and consistent results. As long as users aren&#39;t puzzled by the distinction, they&#39;ll be consistent in picking the same numbers for the same rating every time.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;Statistical Ratings&lt;/h3&gt;
&lt;p&gt;The last big topic that you have to think about in creating most comparison rating systems is whether they&#39;re statistically sound. &lt;/p&gt;
&lt;p&gt;The best way to make your ratings statistically sound is with volume. If you can manage thousands or tens of thousands of ratings for each item, any anomolies are going to become noise. However, the fewer ratings you have, the more likely it is that your ratings are inaccurate in relationship to your database of ratings as a whole. (And thus one of the failures for eBay is that it tries to claim meaningfulness for users with very few ratings, where there&#39;s clearly no statistical basis.)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Thomas_Bayes&#34;&gt;&lt;img border=&#34;0&#34; alt=&#34;Bayes&#34; title=&#34;Bayes&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/bayes.jpeg&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
Ideally what you want to do is give items with fewer ratings among your collection less weight, and those with more ratings higher weight. One simple way to do this is to apply a &lt;em&gt;bayesian average&lt;/em&gt;. Variants of this are used by the aforementioned BoardGameGeek and by &lt;a href=&#34;http://www.imdb.com&#34;&gt;IMDB&lt;/a&gt;. RPGnet is using it for some unreleased software as well.&lt;/p&gt;
&lt;p&gt;The idea behind a bayesian average is that you normalize ratings by pushing them toward the average rating for your site, and you do that more for items with fewer ratings than those with more ratings. The basic formula looks like this:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;code&gt; b(r) = [ W(a) * a + W(r) * r ] / (W(a) + W(r)]&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;&lt;code&gt;r&lt;/code&gt; = average rating for an item&lt;br /&gt;&lt;code&gt;W(r)&lt;/code&gt; = weight of that rating, which is the number of ratings&lt;br /&gt;&lt;code&gt; a&lt;/code&gt; = average rating for your collection&lt;br /&gt; &lt;code&gt;W(a)&lt;/code&gt; = weight of that average, which is an arbitrary number, but should be higher if you generally expect to have more ratings for your items; 100 is used here, for a database which expects many ratings per item&lt;br /&gt;&lt;code&gt;b(r)&lt;/code&gt; = new bayesian rating &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Say three &amp;quot;shill&amp;quot; users had come onto your site and rated a brand new indie film a &amp;quot;10&amp;quot; because the producer asked them to. However, you use a bayesian average with a weight of 100, and thus 3 ratings won&#39;t move the movie very far from the average site rating of 6.50:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;code&gt;b(r) = [100 * 6.50 + 3 * 10] / (100 + 3)&lt;br /&gt; b(r) = 680 / 103&lt;br /&gt; b(r) = 6.60&lt;/code&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;WowWebDesigns uses a similar model and even offers a &lt;a href=&#34;http://www.wowwebdesigns.com/formula.php&#34;&gt;good explanation of their methods&lt;/a&gt; on their web site.&lt;a href=&#34;http://www.wowwebdesigns.com/designs/id_242/ratings/&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/wowwebdesignsrating.png&#34; title=&#34;Wowwebdesignsrating&#34; alt=&#34;Wowwebdesignsrating&#34; class=&#34;image-full&#34; halign=&#34;middle&#34; style=&#34;margin: 5px;&#34; /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;With everything that&#39;s been described thus far, including granularity and distinction, a bayesian average (or some other similiar method) will probably be enough to give your ratings a good, sound statistical basis. However, sites with low volume of ratings may still be concerned with &amp;quot;shills&amp;quot; or &amp;quot;crappers&amp;quot; who come in to your site just to put &amp;quot;10&amp;quot;s on their favorite items on &amp;quot;1&amp;quot;s on their least favorite. RPGnet&#39;s reviews are an example of a site that could experience this issue, because only a few people are going to ever write reviews for an individual item, and this small number of reviews could compromise the nature of any comparisons generated by the ratings sytems.&lt;/p&gt;
&lt;p&gt;In short summary the following additional methods may help with this issue:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Rate the Raters:&lt;/strong&gt; Reviews are low volume, but presumably readers of those reviews are high volume, and you can take advantage of that to then have your readers rate the reviews. &lt;a href=&#34;http://www.amazon.com&#34;&gt;Amazon&lt;/a&gt; and &lt;a href=&#34;http://www.netflix.com&#34;&gt;Netflix&lt;/a&gt; are two examples of sites which use this method by asking &amp;quot;how many readers found this helpful&amp;quot;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Altruistic Punishment:&lt;/strong&gt; An alternative method for rating raters is to use &lt;a href=&#34;http://www.lifewithalacrity.com/2005/03/dunbar_altruist.html#altruistic_punishment&#34;&gt;altruistic punishment&lt;/a&gt;. Herein users can punish someone who does contribute to the community, but at a cost to themselves. So, a reader could flag a poor rating or a poor review at some minor cost to their own rating. Though this method may seems somewhat paradoxical, &lt;a href=&#34;http://en.wikipedia.org/wiki/Game_theory&#34;&gt;game theory&lt;/a&gt;&amp;nbsp; suggests that it is a generally effective technique for improving the commons.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Adjust Ratings Based on Ratings:&lt;/strong&gt; Ratings can be self-adjusted based upon the rater&#39;s own behavior. The simplest method here is to map a rater&#39;s average rating to the average rating for a site. For example, if the average rating of a site is 6.50 and a shill&#39;s average rating is 10.0, then those 10s should be treated as 6.50s. This has the possibility for some intensive calculations, however, and may lead to additional bias in your rating pool if shills figure out the methods you use to adjust ratings.
&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Allow Editorial Fiat:&lt;/strong&gt; Another method is to allow editorial fiat, where editors are expected to come in and remove bad ratings (or proactively not release them). This clearly results in time issues, but they may not be major since only sites with small numbers of ratings/item will have to do this type of adjusting. Further, automated systems could flag &amp;quot;suspicious&amp;quot; rating patterns which are outside the norms for average, speed of rating, etc. (RPGnet supports editorial fiat by requiring editorial release of all reviews.)&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;The idea of adjusting ratings based on ratings bears a bit of additional discussion because it&#39;s somewhat similar to another well-knowing rating system: slashdot. Herein you have both ratings and meta-ratings. People can rate threads and articles, then other people can agree or disagree with those ratings, which in turn makes it more or less likely that the original rater will be allowed to rate in the future (depending on if people agree or disagree with his ratings). Under a more general classification, this is probably a &lt;em&gt;meta-rating system based on a reputation system&lt;/em&gt;, so it&#39;s something we&#39;ll look at further a couple of articles down the road.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;Other Issues: Bilateralism &amp;amp; Usefulness&lt;/h3&gt;
&lt;p&gt;90% of the rating issues that sites will face are covered by the above. However eBay in particular raised two other issues -- bilateralism and usefulness -- that aren&#39;t as generally relevant but do deserve some consideration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/ebayfeedback.jpg&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/ebayfeedback.jpg&#34; title=&#34;Ebayfeedback&#34; alt=&#34;Ebayfeedback&#34; class=&#34;image-full&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
Bilateralism:&lt;/strong&gt; One of the reasons that eBay&#39;s ratings fall apart is that they&#39;re bilateral. Buyers and sellers rate each other simultaneously and thus there&#39;s the fear of revenge if you rate someone badly. It&#39;s a sufficient issue that eBay has a &lt;a href=&#34;http://pages.ebay.com/help/feedback/questions/retaliatory-feedback.html&#34;&gt;FAQ on the topic&lt;/a&gt;, though they don&#39;t offer any good answers. &lt;/p&gt;
&lt;p&gt;The following solution would address some issues of bilateral revenge:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Put a time limit on bilateral ratings&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Release bilateral ratings simultaneously at the end of the time limit&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don&#39;t allow additional ratings after the time period&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This would work well on an eBay, where you&#39;re unlikely to conclude an additional deal with someone you rated badly, and thus there&#39;s no possibility ever for rating revenge. On a game site, however, where people are arbitrarily put into games with each other, and thus you could end up in a game with someone you rated poorly, there might be room for later revenge, down the road. This would have to be addressed to truly feel comfortable with bilateral ratings. &lt;/p&gt;
&lt;p&gt;Additional investigation might reveal more variations of this method, or offer good answers for alternatives, like &lt;em&gt;anonymous ratings&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In addition, good privacy restrictions are really needed to make bilateral ratings work, as well as Terms of Service that protect users from lawsuits for ratings. There have already been cases of physical threats based upon eBay ratings. eBay has also produced cases where people threatened slander or libel lawsuits for bad ratings, and this even further chills the possibility of true ratings appearing on the eBay server.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Usefulness:&lt;/strong&gt; Finally, you want to make sure your ratings are useful at your sites. Rankings are a good way to achieve this. You can see the &amp;quot;best games ever&amp;quot; ranked, or you can see the most interesting user content rise to the top of a long listing, and the least interesting sink to the bottom.&lt;/p&gt;
&lt;p&gt;eBay offers a counter example of frustration with the usefulness of ratings. As already mentioned, you can theoretically ban &amp;quot;bad&amp;quot; users from bidding on your items, and then cancel bids from these users if they appear. However, there are multiple issues with this approach. First, how do you define &amp;quot;bad&amp;quot; users on eBay? Insufficient feedback? Too much negative feedback? Too high a percentage of negative feedback? Second, there is no automated method for doing this, so you must remain ever vigilant on your auctions to make sure that &amp;quot;bad&amp;quot; users aren&#39;t involved. Third, there&#39;s no way to keep a bad bidder from returning after you&#39;ve cancelled his bid. Fourth, these bad bids and cancellations have the possibility of corrupting your auction, as you could lose other bidders who came in, saw the higher bid when the bad bidder was involved, then left before the bid was reduced by his removal. Finally, greed is a powerful motivator on eBay, which might lead to the retention of bad users.&lt;/p&gt;
&lt;p&gt;You also need to be careful with your user interface for ratings. Here is an example of a poor UI:&lt;/p&gt;
&lt;p&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/uselessratingui.jpg&#34; title=&#34;Uselessratingui&#34; alt=&#34;Uselessratingui&#34; /&gt;
&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Comparison ratings are going to be an increasingly important force as the Internet continues to mature. To produce meaningful comparison ratings for your site, you need to concentrate on four important factors: granularity, specifity, sound statistics, and usefulness. And, if you offer bilateral ratings, make sure you understand the subtleties of that as well.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Related articles from this blog:&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html&#34;&gt;2005-12: Systems for Collective Choice&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2006/01/ranking_systems.html&#34;&gt;2006-01: Collective Choice: Competitive Ranking Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2006/08/using_5star_rat.html&#34;&gt;2006-08: Using 5-Star Rating Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2007/01/collective_choi.html&#34;&gt;2007-01: Experimenting with Ratings&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;p&gt;Related articles from Shannon Appelcline&#39;s &lt;a href=&#34;http://www.skotos.net/articles/show-column.phtml?colname=TTnT_&#34;&gt;Trials, Triumphs &amp;amp; Trivialities:&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_192.phtml&#34;&gt;#192: Managing User Creativity, Part One&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_193.phtml&#34;&gt;#193: Managing User Creativity, Part Two&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_196.phtml&#34;&gt;#196: Collective Choice: Ratings, Who Do You Trust?&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_198.phtml&#34;&gt;#198: Collective Choice: More Thoughts About Ratings&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;footer&gt;&lt;h3&gt;Comments&lt;/h3&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;I&#39;ve ran into my fair share of voting systems, but by far the worst I&#39;ve ever seen is the one used at www.animenfo.com, a database of anime products with user ratings. When giving a product a rating, a user is required to also write a constructive review on the product. However, the reviews are moderated and, often, removed from the database as &#34;unconstructive&#34; by the limited number of moderators. The problem with this approach is that the moderators are inevitably biased. A submitted, bad rating on a product, if reviewed by a moderator who personally likes the show being rated, is often removed from the database with the claim that the review was not constructive. Perhaps not entirely related, but an example of ratings-gone-awry, all the same.
-Kalle.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://www.livejournal.com/userinfo.bml?user=kallewooof&#34;&gt;Kalle&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2005-12-13T10:42:58-07:00&#34;&gt;2005-12-13T10:42:58-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;It&#39;s rarely used, but asking people to rank from -5 to +5 can be better than ranking from 0 to 10.   You make it very clear that &#34;0&#34; means average/neutral.   You won&#39;t entirely eliminate the positive bias in ratings but you can reduce it.  And you bring everybody closer to the same mean.
I&#39;ve concluded over time that in fact the response to a negative feedback on eBay should be disabled, as it contains no information.   Who, after all, is going to give a positive feedback to somebody after they have slapped a negative on you?  The likely choices there are no-feedback or neutral from a charitable person and of course a revenge negative.
So they could make it simpler.  If the first person gives a negative, the transaction is considered sour.  In your stats it would list the number of first-negatives you left for others.  (The number of negs you leave for others is already visible but harder to find.)  Ideally split buyer/seller.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://ideas.4brad.com&#34;&gt;Brad Templeton&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2005-12-13T13:11:31-07:00&#34;&gt;2005-12-13T13:11:31-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;An interesting point missing from this good summary:
Many huge ratings sites see a clear bimodal distribution of scores when rating products(movies, cds, books, etc.)  with a simple scoring scheme, such as a single 5-star scale: The nodes are at 1 and 5 stars, with the love-or-hate scores making up more than 80% of all scores.
Clearly, some of that is a function of the intrinsic cost vs. incentive structures of completing ratings and reviews. When there is a clear direct benifit to the user (such as Netflix, Slashdot, or Yahoo! Music&#39;s Launchcast) ratings tend to distribute a bit more evenly (but just a bit.)
My point is that the actual end-user application of the rating has a large effect on the nature of the scores that will be created.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://www.fudco.com/habitat&#34;&gt;F. Randall Farmer&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2005-12-14T16:13:25-07:00&#34;&gt;2005-12-14T16:13:25-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;I find when people give a bad rating on anything from eBay to those hotel and restaurant review sites its usually as a result of an extremely negative personal experience therefore the rating tends to be exagerated and totally not objective.
A blind system like the porposed solution to eBay bilateralism issue is probably best.
I&#39;ve been a mystery shopper for a handful of restaurants and the system works quite well. In this system the purpose is to behave exactly like a regular consumer. You sit down, ask about the specials, take mental note of everything from the bathroom sink to the temperature of the food (and a hundred other details), but you don&#39;t behave in any way that would alter the staff you are a mystery shopper (for example writing things down is strickly prohibited).
Upon returning home you complete a lengthy questionnare about all the details you were supposed to notice. In this system the rewards is for a thorough assesment, regardless of its positive or negative slant. The more detail you provide the better assignments you get next time around. Even if you totally slam a place, but you provide substantial detail, you will be &#34;promoted&#34; to a &#34;bar visit&#34; or something much more rewarding than the typical lunch visit. So, I also agree with this articles suggestion to rate the raters, akin to what Amazon does but without the bias towards volume.
I would also addd that there is no middle ground on such ratings. Whether the choices range from Strongly Agree to Strongly Dissagree, or from &#34;shortest wait&#34; to &#34;longest wait&#34; there is no &#39;neutral&#39; choice, nothing is &#34;just right&#34; as in the story about the three bears and the porridge. Even with numerical choices they are always even, not odd, so there is not chance of ranking in the middle. I like that. I think it pushes you to be more realistic. Having a middle ground leaves the undecided the opportunity to not make a decision. A &#34;middle&#34; rating is of no use to anyone.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://www.jobmachine.net/shally/&#34;&gt;Shally&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2005-12-21T16:23:26-07:00&#34;&gt;2005-12-21T16:23:26-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;Researcher Paul Resnick has been looking at eBay, too. His blog is at http://www.livejournal.com/users/presnick/ and has posts like eBay Live Trip Report:
http://www.livejournal.com/users/presnick/8121.html
There are a few relevant articles as well, at http://www.si.umich.edu/~presnick/#publications
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://seb.notlong.com&#34;&gt;Seb&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2005-12-29T05:53:29-07:00&#34;&gt;2005-12-29T05:53:29-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;Good work there, and very thorough. Can we distinguish the social/transactional from the object/informational aspects of a rating situation? I&#39;m just wondering aloud. On the one hand is the fact that publishing one&#39;s rating of a transaction will reflect on the transaction partner, on oneself, and so is social. On the other hand is the fact one can indeed rate an experience with some objectivity, assuming that the soft stuff has been bracketed out. I dont see how we can bracket out the social though, at least in a way that users will be able to participate in.
There&#39;s a thing about sincerity as a type of interaction: it&#39;s an attribute of expression that cannot be stated explicitly (say that you&#39;re being sincere and you throw your sincerity into quesion immediately). Ratings may fall into that kind of category of linguistic and metalinguistic acts..
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://www.gravity7.com/blog/media/&#34;&gt;adrian Chan&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-01-04T09:47:53-07:00&#34;&gt;2006-01-04T09:47:53-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;URL:
Wow, thorough and cogent.
Of all of the points, one missed opportunity occured to me while reading, and that is on the EBay statistical validity criticism. The phrase &#39;thus one of the failures for eBay is that it tries to claim meaningfulness for users with very few ratings, where there&#39;s clearly no statistical basis&#39; struck me as off.
Human transactions are not mechanical actions - they involve situations that often matter. Hence a particular negative may really be cause for serious concern, even if there are not yet many ratings. Also, beginners are not inherantly less important than experienced traders. So though difficult, credit, or blame, is rightly awarded even on the basis of just a few transactions. Exactly how is a subject of further refinement.
I have read some of the EBay dissection papers a while back. This post is far more comprehensive, that is wide ranging, and yet not missing clarity, accuracy or nuance. Master work.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;#&#34;&gt;Brian Hamlin&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-05-30T00:43:49-07:00&#34;&gt;2006-05-30T00:43:49-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;I have an interesting brainstorm for you about the netflix challenge:
http://www.thinksketchdesign.com/2008/05/03/design/coding/does-the-netflix-challenge-have-it-backwards
cheers -
thinksketch
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://thinksketchdesign.com&#34;&gt;thinksketch&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2009-03-03T18:56:13-07:00&#34;&gt;2009-03-03T18:56:13-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;/footer&gt;
&lt;p class=&#34;previous&#34;&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/previous/2005/12/collective_choi.html&#34; rel=&#34;syndication&#34; class=&#34;u-syndication&#34; &gt;orginal layout&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>