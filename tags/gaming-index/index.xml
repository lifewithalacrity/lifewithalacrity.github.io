<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gaming Index on Life With Alacrity</title>
    <link>http://www.lifewithalacrity.com/tags/gaming-index/index.xml</link>
    <description>Recent content in Gaming Index on Life With Alacrity</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://www.lifewithalacrity.com/tags/gaming-index/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Collective Choice: Experimenting with Ratings</title>
      <link>http://www.lifewithalacrity.com/2007/01/collective_choi.html</link>
      <pubDate>Mon, 01 Jan 2007 22:38:15 -0700</pubDate>
      
      <guid>http://www.lifewithalacrity.com/2007/01/collective_choi.html</guid>
      <description>&lt;p&gt;&lt;span style=&#34;font-size: 0.7em;&#34;&gt;by Christopher Allen &amp;amp; Shannon Appelcline&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;[This is the fourth in a series of articles on &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html&#34;&gt;collective choice&lt;/a&gt;, co-written by my collegue &lt;a href=&#34;http://www.skotos.net/about/staff/shannon_appelcline&#34;&gt;Shannon Appelcline&lt;/a&gt;. It will be &lt;a href=&#34;http://www.skotos.net/articles/TTnT_179.phtml&#34;&gt;jointly posted&lt;/a&gt; in Shannon&#39;s &lt;a href=&#34;http://www.skotos.net/articles/TTnT.shtml&#34;&gt;Trials, Triumphs &amp;amp; Trivialities&lt;/a&gt; online games column at &lt;a href=&#34;http://www.skotos.net/&#34;&gt;Skotos&lt;/a&gt;.]&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Last year in &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/collective_choi.html&#34;&gt;Collective Choice: Rating Systems&lt;/a&gt; we took a careful look at eBay and other websites that collect ratings, and used those systems as examples to highlight a number of theories about how to make rating systems more useful.&lt;/p&gt;
&lt;p&gt;We suggested three main methods for improving rating systems:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Granular Ratings:&lt;/strong&gt; Based on the clumping of ratings to high values, we believed that ratings could be made more useful by increasing the size of a rating scale. Most rating scales are 5-point ranges, so we suggested a 10-point range instead.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distinct Ratings:&lt;/strong&gt; Raters can be somewhat arbitrary in how they rate items, varying both from each other and even from themselves (usually over multiple sessions). Thus we believed that providing explicit statements of what each number meant could improve ratings.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistical Ratings:&lt;/strong&gt; Finally we stated that in low volumes ratings could be biased by various quirks of data entry, either malevolent or not, and that ratings could be improved with strong statistical methods being used to polish up data and automatically keep &amp;quot;bad&amp;quot; data in line with &amp;quot;good&amp;quot;.&lt;/p&gt;
&lt;p&gt;In the year since we wrote that article we&#39;ve decided to practice what we preach and have rolled out an entirely new rating system called &lt;a href=&#34;http://index.rpg.net&#34;&gt;The RPGnet Gaming Index&lt;/a&gt;. We&#39;ve applied all of the above theories and thus far it looks like they&#39;re not only working, but that they&#39;re actually providing better rating systems than previous ones we&#39;ve used at the RPGnet site.&lt;/p&gt;
&lt;p&gt;In this article we&#39;re going to step through the data we&#39;ve collected from this experience and see how it applies to our theory: first by looking at our previous RPGnet rating system, then by looking at the new system, and finally by by examining the data from these two systems and comparing their results. We&#39;ve also run into some unexpected troubles along the way, and we&#39;ll talk about that too.&lt;/p&gt;
&lt;h3&gt;The RPGnet Reviews System&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.rpg.net&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/rpgnetlogo_1.gif&#34; title=&#34;Rpgnetlogo_1&#34; alt=&#34;Rpgnetlogo_1&#34; style=&#34;margin: 0px 5px 5px 0px; float: left;&#34; /&gt;&lt;/a&gt;
RPGnet is our gaming site for tabletop roleplaying—games like &lt;em&gt;Dungeons &amp;amp; Dragons&lt;/em&gt; and &lt;em&gt;Vampire: The Masquerade&lt;/em&gt;. We purchased it in 2001 from the original owners. One of the benefits of RPGnet was that it had a very large community. As of today it sports one of the top-100 forums on the Internet, with over 1000 simultaneous users regularly logging in. However, because of its maturity, we also inherited many existing systems.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.rpg.net/reviews/archive/9/9971.phtml&#34;&gt;&lt;img border=&#34;0&#34; alt=&#34;Rpgnet_review_summary_1&#34; title=&#34;Rpgnet_review_summary_1&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/rpgnet_review_summary_1.jpg&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
One of these was the &lt;a href=&#34;http://www.rpg.net/reviews/&#34;&gt;RPGnet Reviews System&lt;/a&gt; which gave individual users the ability to review gaming products—mostly role-playing games, but also board games, books, DVDs, and a smattering of related products.&lt;/p&gt;
&lt;p&gt;Most of these reviews are submitted by average readers who just want to talk about a product that they like (or don&#39;t), though a fair percentage are instead submitted by staff reviewers. (Overall at least 26% of our reviews are based on publisher &amp;quot;comp&amp;quot; copies, and thus may be considered largely professional, while the other 74% may or may not be.) The large community size of RPGnet applies to the Reviews System as well: currently it features 8,505 published reviews.&lt;/p&gt;
&lt;p&gt;Looking at the RPGnet Reviews through our three filters we find the following:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Granularity.&lt;/strong&gt; The ratings from our existing reviews aren&#39;t as granular as we&#39;d like. We have a theoretical scale of 2-10, but that&#39;s based upon a Style rating of 1-5 and a Substance rating of 1-5.&lt;/p&gt;
&lt;table cellpadding=&#34;5&#34;&gt;
&lt;thead&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;&lt;strong&gt;Rating&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Style&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Substance&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;%&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;81&lt;/td&gt;
&lt;td&gt;225&lt;/td&gt;
&lt;td&gt;1.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;732&lt;/td&gt;
&lt;td&gt;651&lt;/td&gt;
&lt;td&gt;8.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;2364&lt;/td&gt;
&lt;td&gt;1777&lt;/td&gt;
&lt;td&gt;24.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;3618&lt;/td&gt;
&lt;td&gt;3525&lt;/td&gt;
&lt;td&gt;42.0%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr align=&#34;right&#34;&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1709&lt;/td&gt;
&lt;td&gt;2326&lt;/td&gt;
&lt;td&gt;23.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt; Approximately 90% of raters rate only with values of 3-5, and thus our scale is more limited than the 2-10 range would indicate. 42.9% of reviews further rate Style and Substance exactly the same, suggesting that not everyone sees a difference between these two elements. On the whole this scale isn&#39;t as a bad as a singular 5-point scale, but it also isn&#39;t a real 10-point scale, and the two orthogonal types of comparison don&#39;t necessarily provide a coherent description of a product.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distinctiveness.&lt;/strong&gt; Conversely, the review ratings are fairly distinct because the Review System provides an explanation of what each rating number means. For example the five Substance ratings are: I Wasted My Money (1); Sparse (2); Average (3); Meaty (4); Excellent(5). The descriptions could be better, but hopefully they connect to some users in meaningful ways, and help them to rate consistently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistics.&lt;/strong&gt; Our review ratings have no statistical basis. These values are used entirely unfiltered.&lt;/p&gt;
&lt;p&gt;On the whole, the existing RPGnet Reviews embodied slightly less than half of what we wanted to see in a rating systems: some improvement over a simple 5-point scale; some effort put into making individual ratings distinct; and nothing statistical.&lt;/p&gt;
&lt;p&gt;There is room for improvement, however, as we&#39;ll see when we analyze this system more fully.&lt;/p&gt;
&lt;h3&gt;The RPGnet Gaming Index&lt;/h3&gt;
&lt;p&gt;Our newer system is the &lt;a href=&#34;http://index.rpg.net/&#34;&gt;RPGnet Gaming Index&lt;/a&gt;. It doesn&#39;t supersede our Reviews, but instead offers a complementary look at the roleplaying field. The Index is essentially an RPG industry database. It contains individual entries for many different gamebooks—currently 5248—and allows registered users to rate each of them. Those ratings are then turned into averages by various mathematical formulas on a nightly basis and the roleplaying games in our index are then ranked.&lt;/p&gt;
&lt;p&gt;The large size of RPGnet has allowed us to very quickly turn our ideas of a Gaming Index into reality. Just six months after release we have:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;5248 well-written Index entries &lt;/li&gt;
&lt;li&gt; 5908 different editions&lt;/li&gt;
&lt;li&gt; 4240 authors&lt;/li&gt;
&lt;li&gt; 4478 covers&lt;/li&gt;
&lt;li&gt; 360 different game systems&lt;/li&gt;
&lt;li&gt; 345 series&lt;/li&gt;
&lt;li&gt; 10142 individual ratings&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;Most of the ratings are clumped around the best and worst games, with many less popular games unrated as of yet. Four different items have at least 80 ratings each (&lt;em&gt;Call of Cthulhu&lt;/em&gt;, &lt;em&gt;Exalted&lt;/em&gt;, &lt;em&gt;Nobilis&lt;/em&gt;, and &lt;em&gt;Unknown Armies&lt;/em&gt;). Our average rating is 6.79. Ratings above 7.82 are in the 99th
percentile, ratings above 7.21 are in the 90th percentile, and ratings
below 6.53 are beneath the 10th percentile.&lt;/p&gt;
&lt;p&gt;(For more info on the creation of the RPG Index, and how to encourage user generated content, see Shannon&#39;s articles, &amp;quot;Managing User Creativity&amp;quot;, &lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_192.phtml&#34;&gt;Part One&lt;/a&gt; and &lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_193.phtml&#34;&gt;Part Two&lt;/a&gt;.) &lt;/p&gt;
&lt;p&gt;The RPGnet Index also handles some unusual situations, such as when a game book contains other game books as part of an anthology or compilation. For instance, the 8-book compilation &lt;a href=&#34;http://index.rpg.net/display-entry.phtml?mainid=64&#34;&gt;In Search of Adventure&lt;/a&gt; has a composite rating of &lt;a href=&#34;http://index.rpg.net/display-entry-ratings.phtml?mainid=64&#34;&gt;6.57&lt;/a&gt; which is partially based upon the individual adventures that make it up.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Granularity:&lt;/strong&gt; The first thing we did was provide a 10-point scale for this new system.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distinctiveness:&lt;/strong&gt; We also made sure each point of the scale was clearly defined. Currently the points of our scale are: Worthless (1), Poor (2), Some Flaws (3), Almost Average (4), Average (5), Above Average (6), Good (7), Very Good (8), Outstanding (9), and One of the Best Ever (10).&lt;/p&gt;
&lt;p&gt;We made some mistakes in our original release of our &amp;quot;distinctive&amp;quot; titles, and we discovered this had real effects on the user input, telling us that these title labels &lt;em&gt;are&lt;/em&gt; meaningful to users.&lt;/p&gt;
&lt;p&gt;First, we initially labeled 6 as &amp;quot;average&amp;quot;, to mirror the rating system for our existing Reviews, rather than setting 5 to be average. But as we noted in our first article, people like to be nice, and thus they tend to rate on the good side of a scale. Changing the label for our definition of average from 6 to 5 has slowly started dropping the average of all ratings down as a result (providing more breadth, a topic we&#39;ll talk about more shortly).&lt;/p&gt;
&lt;p&gt;Second, two of our original distinctive titles were at odds with the others. Our original &amp;quot;2&amp;quot; value said that the game had &amp;quot;a few useful elements&amp;quot; and our original &amp;quot;9&amp;quot; value said that it was the &amp;quot;best of the year&amp;quot;. The 2 was much more specific than any of our other terms and the 9 created a comparative query that was very different from anything else. Overall our ratings conformed to a bell curve centered between 6 and 7, but we saw very clear dropouts in our curve at 2 and 9, telling us that we&#39;d made mistakes in those terms, and that people were less willing to use them as a result. Since we&#39;ve made the change to our current set of titles those two discontinuities have disappeared.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistics.&lt;/strong&gt; Finally, we fully integrated statistics into our new Index by using two main methods: bayesian weights and trust.&lt;/p&gt;
&lt;p&gt;We explained bayesian weights pretty fully in our previous article. Here&#39;s what we said then:&lt;/p&gt;
&lt;blockquote&gt;
&lt;table width=&#34;90%&#34; border=&#34;1&#34; cellpading=&#34;3&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;p&gt;
The idea behind a bayesian average is that you normalize ratings by pushing them toward the average rating for your site, and you do that more for items with fewer ratings than those with more ratings. The basic formula looks like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
b(r) = [ W(a) * a + W(r) * r ] / (W(a) + W(r)]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;r = average rating for an item&lt;/p&gt;
&lt;p&gt;W(r) = weight of that rating, which is the number of ratings&lt;/p&gt;
&lt;p&gt;a = average rating for your collection&lt;/p&gt;
&lt;p&gt;W(a) = weight of that average, which is an arbitrary number, but should be higher if you generally expect to have more ratings for your items; 100 is used here, for a database which expects many ratings per item&lt;/p&gt;
&lt;p&gt;b(r) = new bayesian rating &lt;/p&gt;
&lt;p&gt;Say three &amp;quot;shill&amp;quot; users had come onto your site and rated a brand new indie film a &amp;quot;10&amp;quot; because the producer asked them to. However, you use a bayesian average with a weight of 100, and thus 3 ratings won&#39;t move the movie very far from the average site rating of 6.50:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
b(r) = [100 * 6.50 + 3 * 10] / (100 + 3)&lt;/code&gt;&lt;code&gt;&lt;br /&gt;b(r) = 680 / 103&lt;/code&gt;&lt;code&gt;&lt;br /&gt;b(r) = 6.60
&lt;/code&gt;
&lt;/p&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;/blockquote&gt;
&lt;p&gt;We implemented bayesian weights exactly as we&#39;d detailed, but with a lower weight of 25. Since then we&#39;ve accrued over 10,000 ratings in the database, and we can probably start thinking about cranking that weight up, another topic we&#39;ll return to.&lt;/p&gt;
&lt;p&gt;Our trust-based algorithms suggest that some ratings are better than others, and should thus be more trusted (and thus more weighted when we calculate the average rating of an item). Though bayesian weights have been used before, we&#39;re not aware of other systems that weight ratings based on trust. &lt;/p&gt;
&lt;p&gt;The calculation of trust is very simple:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
Weight = 0 if #ratings(user) &amp;lt;= 2&lt;/code&gt;&lt;code&gt;&lt;br /&gt;Otherwise Weight = #ratings(user) / 50 to a maximum of 2&lt;/code&gt;&lt;code&gt;&lt;br /&gt;Weight *= 2, to a maximum of 4, if the user included a comment
&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;This was based on the idea that the average good rater would rate 25 different items and the average great rater would rate at least 50. Additionally, we believed that ratings with comments were more likely to be thoughtful than those without.&lt;/p&gt;
&lt;p&gt;That, overall, is a quick picture of what we&#39;ve done with the RPGnet Gaming Index. Some of these ideas were laid out from the start, and others have been tuned as we progressed. &lt;/p&gt;
&lt;p&gt;So how did we do, particularly in comparison to our existing RPGnet Reviews System?&lt;/p&gt;
&lt;h3&gt;The Comparison&lt;/h3&gt;
&lt;p&gt;One of our goals in improving rating systems has been to widen the range of possible input. As we noted earlier we discovered that 90% of our RPGnet Reviews Ratings were in the 3-5 range, and only 10% in the 1-2 range.&lt;/p&gt;
&lt;p&gt;Generally, we can measure the success of widening a range by seeing whether the average rating of a database moves toward the &lt;em&gt;true&lt;/em&gt; average. For the purposes of a 10-point scale from 1-10, that&#39;s a desired value of 5.5. That generally means we&#39;re looking for our average rating to &lt;em&gt;decrease&lt;/em&gt; because people tend to rate high.&lt;/p&gt;
&lt;p&gt;The following table compares the average results of Reviews ratings and Index ratings.&lt;/p&gt;
&lt;center&gt;&lt;table cellpadding=&#34;3&#34; border=&#34;1&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Database&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Average&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Converted Reviews&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;7.25&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Massaged Reviews&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;7.29&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Unweighted Index&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;7.10&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Weighted Index&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;6.78&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;p&gt;Here&#39;s what the categories in the above chart represent:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Converted Reviews:&lt;/strong&gt; The Style + Substance of the Reviews, converted from its 2-10 scale to a 1-10 scale: &lt;/p&gt;
&lt;p&gt;&lt;code&gt;
$rating = avg($style) + avg($substance);&lt;/code&gt;&lt;br /&gt;&lt;code&gt;$rating = ($rating * 1.125) - 1.25;
&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Massaged Reviews:&lt;/strong&gt; The Style + Substance of the Reviews, with Substance given double weight over Style because we think that more closely reflects the intentions of the reviewer, converted from its 2-10 scale to a 1-10 scale:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
$rating = (average($style) + 2*average($substance))/1.5;&lt;/code&gt;&lt;br /&gt;&lt;code&gt;$rating = ($rating * 1.125) - 1.25;
&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unweighted Index:&lt;/strong&gt; Index ratings exactly as users have entered into our Gaming Index:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
$rating = average($index-rating);
&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Weighted Index:&lt;/strong&gt; Index ratings adjusted by the weight of each individual rating, which is based on user trust and inclusion of comments:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;
$rating = average($index-rating*$index-weight)/average($index-weight);
&lt;/code&gt;
&lt;/p&gt;
&lt;p&gt;Our average rating—which is our criteria for success—decreased somewhat from the Reviews System to the Gaming Index and it decreased much more dramatically when we introduced our trust systems.&lt;/p&gt;
&lt;p&gt;The following chart shows the a typical example of how review and index ratings differ, using the venerable &lt;em&gt;Dungeons &amp;amp; Dragons Player&#39;s Handbook&lt;/em&gt; as an example:&lt;/p&gt;
&lt;center&gt;&lt;a href=&#34;http://index.rpg.net/display-entry-ratings.phtml?mainid=72&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/dd_players_handbook_rpgnet_reviews_only.jpg&#34; title=&#34;Dd_players_handbook_rpgnet_reviews_only&#34; alt=&#34;Dd_players_handbook_rpgnet_reviews_only&#34; /&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/dd_players_handbook_index_ratings_only.jpg&#34; title=&#34;Dd_players_handbook_index_ratings_only&#34; alt=&#34;Dd_players_handbook_index_ratings_only&#34; /&gt;&lt;/a&gt;&lt;/center&gt;&lt;p&gt;For this book the median ratings from reviews-only is 8, and the median from index-only is 7. A one-to-two point drop in median rating from reviews to index was consistent in all of our most-rated games other than those which were a rated a &amp;quot;10&amp;quot; in both places.&lt;/p&gt;
&lt;p&gt;We believe that this initial success of our unweighted Gaming Index can be attributed to the slightly better &lt;em&gt;granularity&lt;/em&gt;—a 10-point scale versus two 5-point scales—and our improved &lt;em&gt;distinctiviness&lt;/em&gt;—based on better naming of the rating levels. The veracity of this will ultimately be played out as the Index grows.&lt;/p&gt;
&lt;p&gt;However we have no doubt that our &lt;em&gt;statistical&lt;/em&gt; approach to the index data, when we moved from our unweighted Index to our weighted Index, is providing even better results. We had theorized that users who input more and who include comments would provide &amp;quot;better&amp;quot; data, and by our criteria of the average of the ratings moving toward 5.5 that seems to be borne out. The following table looks at the information a bit more precisely, by comparing average ratings as total number of ratings increases over several ranges:&lt;/p&gt;
&lt;center&gt;&lt;table cellpadding=&#34;3&#34; border=&#34;1&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;# of Ratings&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Average w/Comment&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Average w/o Comment&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;1-2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;8.55&lt;/td&gt;
&lt;td&gt;8.88&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;3-24&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;8.08&lt;/td&gt;
&lt;td&gt;8.16&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;25-49&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;7.32&lt;/td&gt;
&lt;td&gt;7.11&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;50-99&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;7.14&lt;/td&gt;
&lt;td&gt;7.03&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;100+&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;6.17&lt;/td&gt;
&lt;td&gt;6.99&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;p&gt;This table fairly definitively shows that base maxim: that the breadth of the ratings, and thus their quality, increases the more ratings a user makes. The improved quality of ratings with comments is less definitive. Among the vast mass of users the two values are pretty close, and sometimes the reverse of what we expect, but for the best and the worst users, ratings with comments seem to be better than those without. This latter point is another one that we&#39;ll have to continue to monitor as the Index grows beyond its current total of 10,000 ratings.&lt;/p&gt;
&lt;p&gt;The other major element of our &lt;em&gt;statistical&lt;/em&gt; approach to the Index is our bayesian weight. The following chart shows a top-ten chart for roleplaying games calculated via four different methodologies: our Reviews; our Index with no weighting; our Index with a 25 bayesian weighting (as it currently stands); and our Index with a 50 bayesian weighting:&lt;/p&gt;
&lt;center&gt;&lt;table cellpadding=&#34;3&#34; border=&#34;1&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Reviews-Only&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0-weight Index&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;25-weight Index&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;50-weight Index&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Delta Green: Countdown&lt;/td&gt;
&lt;td&gt;The Chronicles of Talislanta&lt;/td&gt;
&lt;td&gt;Delta Green: Countdown&lt;/td&gt;
&lt;td&gt;Delta Green&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Nobilis&lt;/td&gt;
&lt;td&gt;Wildside&lt;/td&gt;
&lt;td&gt;Spirit of the Century&lt;/td&gt;
&lt;td&gt;Delta Green: Countdown&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Castle Falkenstein&lt;/td&gt;
&lt;td&gt;Devil&#39;s Due&lt;/td&gt;
&lt;td&gt;Delta Green&lt;/td&gt;
&lt;td&gt;Unknown Armies&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Vimary Sourcebook&lt;/td&gt;
&lt;td&gt;Lodges: The Faithful&lt;/td&gt;
&lt;td&gt;Unknown Armies&lt;/td&gt;
&lt;td&gt;Call of Cthulhu&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Liber Servitorum&lt;/td&gt;
&lt;td&gt;Apocalypse&lt;/td&gt;
&lt;td&gt;Call of Cthulhu&lt;/td&gt;
&lt;td&gt;Nobilis&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Ork!&lt;/td&gt;
&lt;td&gt;Earthdawn Gamemaster&#39;s Compendium&lt;/td&gt;
&lt;td&gt;Nobilis&lt;/td&gt;
&lt;td&gt;Spirit of the Century&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;7&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;GURPS Russia&lt;/td&gt;
&lt;td&gt;Into the Badlands&lt;/td&gt;
&lt;td&gt;Pendragon&lt;/td&gt;
&lt;td&gt;Over the Edge&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;8&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;GURPS Reign of Steel&lt;/td&gt;
&lt;td&gt;Earthdawn Player&#39;s Compendium&lt;/td&gt;
&lt;td&gt;Over the Edge&lt;/td&gt;
&lt;td&gt;Pendragon&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;9&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Cudgel&#39;s Compendium&lt;/td&gt;
&lt;td&gt;Chronicle of the Black Labyrinth&lt;/td&gt;
&lt;td&gt;Mutants &amp;amp; Masterminds&lt;/td&gt;
&lt;td&gt;Mutants &amp;amp; Masterminds&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;10&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Corum&lt;/td&gt;
&lt;td&gt;The Spell Book&lt;/td&gt;
&lt;td&gt;Pulp Hero&lt;/td&gt;
&lt;td&gt;Vimary Sourcebook&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;p&gt;We actually &lt;em&gt;did&lt;/em&gt; do a little bit of statistical analysis on the Reviews because on our first try to produce this chart we got a random clump of reviews that were 5/5 from a much larger pool, so we further ordered them by descending total count of reviews, and as a result you&#39;re seeing a better selection of ranked reviews than a truly unstatistical sampling would allow. We did the same for the unweighted Index (which clumped a number of results at &amp;quot;10&amp;quot;), except we further ordered items at the same weight by decreasing number of views (another statistical decision).&lt;/p&gt;
&lt;p&gt;Clearly, deciding which of these lists is &amp;quot;right&amp;quot; is a much more subjective measure than the mathematical analysis we were able to apply to earlier problems. However, most roleplayers would tell you that the unweighted Reviews and Index lists are terrible. The top 5 items in the Reviews list actually aren&#39;t bad for a starting list of good games—but only because we did the aforementioned statistical ordering. Before that we just had a random listing of gaming items. Even with our attempts at quickie statistical analysis the unweighted Index is still quite bad, with only &lt;em&gt;Talislanta&lt;/em&gt; regularly showing up on other &amp;quot;best&amp;quot; lists.&lt;/p&gt;
&lt;p&gt;The problem is the ability of one person to come in and rate an item a &amp;quot;10&amp;quot; (or a &amp;quot;5&amp;quot;/&amp;quot;5&amp;quot;), thereby making that item more highly rated than &lt;em&gt;any&lt;/em&gt; item which has an actual consensus of ratings. Of our unweighted top Reviews only the top three had more than 2 reviews and the rest had 2. Not surprisingly those top three were the best fits to a typical top-ten list. Of the unweighted Index only the top three had more than 1 rating, and the rest had 1. Our single good pick was in those top three.&lt;/p&gt;
&lt;p&gt;Our 25-weight Index, which is what we currently use, has been generally accepted by the RPGnet community as a good marker of what&#39;s good and what&#39;s not. However there have been two items on it which some percentage of people disagree with: &lt;em&gt;Spirit of the Century&lt;/em&gt; and &lt;em&gt;Pulp Hero&lt;/em&gt;. It&#39;s instructive to see that when we increase to a 50-weight Index &lt;em&gt;Spirit of the Century&lt;/em&gt; drops (even more notably than depicted here, because its actual rating changes from .01 from first place to .16 from first place) and &lt;em&gt;Pulp Hero&lt;/em&gt; disappears entirely.&lt;/p&gt;
&lt;p&gt;The questions of &lt;em&gt;what&lt;/em&gt; to set your bayesian weight to, when to increase it, and what maximum value to set it to are all relatively unstudied and thus we don&#39;t have good answers to them. As we pass 10,000 ratings we&#39;re considering upping the bayesian value to 50. We expect that 100 will be our ultimate value when the Index is fully mature, however if we increase the weight too far an older, less rated game will never be able to get enough weight to get out of the doldrums.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We&#39;re by no means done with this ratings experiment. Though we&#39;ve pleased and impressed with the growth of the RPGnet Index thus far, by next year we hope that the Index will include the vast majority of all games in print (as opposed to somewhat less than half now) and that our 10,000 ratings will grow to 50,000 or more. This will allow us to offer even more definitive answers to our questions.&lt;/p&gt;
&lt;p&gt;In the meantime we&#39;re still mucking with our statistics and facing new problems.&amp;nbsp; Some of the newest:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;What to do about drive-by ratings:&lt;/strong&gt; Our trust algorithm does a good job of making drive-by ratings, where a publisher points his audience to an item in our site, mostly irrelevant, but there&#39;s some concern that they could have more effect in the long run.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How to incorporate our review ratings in our index ratings:&lt;/strong&gt; It seems a shame to waste the thousands of reviews that have been written—and indeed currently they&#39;re calculated into a composite rating we use in the Index—but we&#39;re realizing that people have very different purposes for writing reviews and inputing ratings, which may result in some of the upward skew we see on the review side of things. Ultimately we need to decide whether they&#39;re just too different or whether our statistical massaging is enough to incorporate those reviews into a composite Index rating.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How to pick some of our numbers:&lt;/strong&gt; As we already noted we don&#39;t have good formulas for when to choose which bayesian weights. Likewise we&#39;ve been guessing at which values to use for the trust-based weighting of our raters. Originally we set our desired rating count to 100 for good rater and 200 for great raters, but we&#39;ve since dropped those to 50 for good and 100 for great based upon the real numbers of ratings that users were making. Again, we&#39;d prefer to derive an actual formula for this type of calculation&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;Shannon has discussed some of these issues more in his recent article &lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_198.phtml&#34;&gt;More Thoughts Abour Ratings&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Despite unanswered questions, we still feel good about the basic ideas we laid out in our article last year. We have no doubt that giving our ratings a &lt;em&gt;statistical&lt;/em&gt; basis has dramatically improved them and evidence thus far suggests that both &lt;em&gt;granularity&lt;/em&gt; and &lt;em&gt;distinctiveness&lt;/em&gt; have been helpful as well.&lt;br /&gt;
&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Related articles from this blog:&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html&#34;&gt;2005-12: Systems for Collective Choice&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/collective_choi.html&#34;&gt;2005-12: Collective Choice: Rating Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2006/01/ranking_systems.html&#34;&gt;2006-01: Collective Choice: Competitive Ranking Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2006/08/using_5star_rat.html&#34;&gt;2006-08: Using 5-Star Rating Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;p&gt;Related articles from Shannon Appelcline&#39;s &lt;a href=&#34;http://www.skotos.net/articles/show-column.phtml?colname=TTnT_&#34;&gt;Trials, Triumphs &amp;amp; Trivialities:&lt;/a&gt;
&lt;/p&gt;&lt;blockquote&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_192.phtml&#34;&gt;#192: Managing User Creativity, Part One&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_193.phtml&#34;&gt;#193: Managing User Creativity, Part Two&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_196.phtml&#34;&gt;#196: Collective Choice: Ratings, Who Do You Trust?&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_198.phtml&#34;&gt;#198: Collective Choice: More Thoughts About Ratings&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;footer&gt;&lt;h3&gt;Comments&lt;/h3&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;URL:
0-10 scale is too large for most relevant human ratings systems. maybe i should pull the psych papers, but people tend to do a 1-5 or 0-5 scale better. it&#39;s a sign of immaturity to only use the endpoints.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;#&#34;&gt;Grumpy&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2007-12-18T10:24:24-07:00&#34;&gt;2007-12-18T10:24:24-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;/footer&gt;
&lt;p class=&#34;previous&#34;&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/previous/2007/01/collective_choi.html&#34; rel=&#34;syndication&#34; class=&#34;u-syndication&#34; &gt;orginal layout&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using 5-Star Rating Systems</title>
      <link>http://www.lifewithalacrity.com/2006/08/using_5star_rat.html</link>
      <pubDate>Fri, 11 Aug 2006 08:49:14 -0700</pubDate>
      
      <guid>http://www.lifewithalacrity.com/2006/08/using_5star_rat.html</guid>
      <description>&lt;p&gt;In &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/collective_choi.html&#34;&gt;Collective Choice: Rating Systems&lt;/a&gt; I discuss ratings scales of various sorts, from eBay&#39;s 3-point scale to RPGnet&#39;s double 5-point scale, and BoardGame Geek&#39;s 10-point scale.&lt;/p&gt;
&lt;p&gt;&lt;a onclick=&#34;window.open(this.href, &#39;_blank&#39;, &#39;width=373,height=239,scrollbars=no,resizable=no,toolbar=no,directories=no,location=no,menubar=no,status=no,left=0,top=0&#39;); return false&#34; href=&#34;http://lifewithalacrity.blogs.com/.shared/image.html?/photos/uncategorized/stars_1.gif&#34;&gt;&lt;img width=&#34;200&#34; height=&#34;128&#34; border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/images/stars_1.gif&#34; title=&#34;Stars_1&#34; alt=&#34;Stars_1&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Of the various ratings scales, 5-point scales are probably the most common on the Internet. You can find them not just in my own RPGnet, but also on Amazon, Netflix, and iTunes, as well as many other sites and services. Unfortunately 5-point rating scales also face many challenges in their use, and different studies suggest different flaws with this particular methodology.&lt;/p&gt;
&lt;p&gt;First, &lt;a href=&#34;http://sloan.ucr.edu/category/working-papers/product-reviews/&#34;&gt;one study&lt;/a&gt; using Amazon data has shown that many &lt;em&gt;undetailed&lt;/em&gt; ratings (where the rater isn&#39;t required to add any additional information other than the rating they select) show a &lt;a href=&#34;http://en.wikipedia.org/wiki/Bimodal_distribution&#34;&gt;bimodal distribution&lt;/a&gt;.&amp;nbsp; In other words the distribution of ratings tends to cluster around two different numbers (e.g., 1 and 5) rather than offering a normal distribution where the ratings cluster around a single height (e.g., 3). Thus the median of these ratings is not an accurate reflection of product quality, but instead is a statement of conflicting opinions.&lt;/p&gt;
&lt;p&gt;Second, &lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/collective_choi.html&#34;&gt;our own study&lt;/a&gt; using RPGnet data has shown that many &lt;em&gt;detailed&lt;/em&gt; ratings (where the rater does add additional information, in this case a full review) offer normal distributions, however it is biased toward the high end of the scale. On RPGnet, for example, we discovered that 90% of this 5-point rating system was 3 or higher with an average around 4.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://360.yahoo.com/profile-9lciejI3aafX1stHPoIRNmkmv4EowQ--&#34;&gt;Randy Farmer&lt;/a&gt; of Yahoo suggests that this scale limitation is particularly troublesome for fan-based ratings, such as those found on episodic TV sites:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;em&gt;Only the fans of a show evaluate the episodes, and being fans, will never rate an episode one or two stars, ever. I&#39;ve seen this attempted over and over on the net with the same results every time: Each episode of a show is 4-stars +/- .5 stars. This goes all the way back to the Babylon-5 website, probably the first source for this kind of data.&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;(And indeed, the TV episode &lt;a href=&#34;http://www.tv.com/babylon-5/tko/episode/25148/summary.html?tag=ep_list;title;14&#34;&gt;TKO&lt;/a&gt;, from &lt;em&gt;Babylon 5&lt;/em&gt;&#39;s first season, is considered an entirely atrocious episode by even the fans. Yet it has a 6.1 of 10 &amp;quot;Fair&amp;quot; rating on &lt;a href=&#34;http://www.tv.com&#34;&gt;tv.com&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Thus even when a bimodal distribution is not a problem, on a 5-point scale the upward bias often results in only 2 or 3 meaningful data points. This is problematic because it minimizes differentiation. In many cases, a 5-star rating system where most of the ratings are either 3 or 4 is actually no better then just a thumbs-up/thumbs-down rating system. &lt;/p&gt;
&lt;p&gt;However, given that 5-point scales are probably here to stay, we are forced to make the best use of them we can.&lt;/p&gt;
&lt;p&gt;First, we need to provide raters with &lt;em&gt;incentives&lt;/em&gt;, so that they provide meaningful ratings. We&#39;ve already seen that this can be done by requesting detailed ratings: when a person takes the time to write text, and knows that his name will be attached to it, he generally does a better job in his rating. There are other possible incentives techniques as well, such as RPGnet&#39;s new &lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_191.phtml&#34;&gt;XP System&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Second, we need to provide means for a 5-point scale to become more meaningful by encouraging raters to use not just the top half of the scale, but the bottom half as well. One method to accomplish this is to make ratings &lt;em&gt;distinct&lt;/em&gt; -- as I briefly mentioned in my previous article on this topic -- and encourage standards so that an &amp;quot;average&amp;quot; rating is 2 or 3, not 4.&lt;/p&gt;
&lt;p&gt;As an example of how to accomplish both of these goals with already existing 5-point rating scales, I&#39;ve detailed my own experiences with using ratings on two popular services -- iTunes and Amazon. By providing myself with incentives and making my use of ratings very distinctive, I have created more meaningful and useful output for myself.&lt;/p&gt;
&lt;h3&gt;Music Ratings - iTunes&lt;/h3&gt;
&lt;p&gt;Apple&#39;s iTunes software offers you the ability to rate individual songs with a 0-5 Star rating. If you use iTunes with an iPod, you can change the rating of a song on your iPod and the change will be reflected in your iTunes database the next time you sync your iPod. The &amp;quot;Shuffle Songs&amp;quot; feature available on more modern iPods has an option to have songs with higher ratings be played more often. A very powerful feature, Smart Playlists, can dynamically create sophisticated playlists based on ratings. All of this makes rating music on iTunes very useful.&lt;/p&gt;
&lt;p&gt;After Shannon and I wrote our Rating Systems article, I examined the ratings in my iTunes catalog. Using the &lt;a href=&#34;http://girtby.net/archives/2005/11/03/itunes-library-preening/&#34;&gt;Alastair&#39;s&lt;/a&gt; fabulous XLST &lt;a href=&#34;http://girtby.net/offerings/itunes-ratings-stats/&#34;&gt;iTunes rating statistics tool&lt;/a&gt;, I discovered that the ratings I created in iTunes clearly were biased overly high, matching the pattern we&#39;d described. I had far too many songs rated with 4 Stars, and almost nothing rated 1 or 2. This made my ratings less useful.&lt;/p&gt;
&lt;table align=&#34;center&#34;&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;td colspan=&#34;6&#34;&gt;Here are some statistics from your iTunes Library: 4172 tracks, 412 (10%) rated&lt;/td&gt;&lt;/tr&gt;
&lt;br /&gt; &lt;tr&gt;&lt;th colspan=&#34;3&#34;&gt;&amp;nbsp;&lt;/th&gt;
&lt;th colspan=&#34;3&#34;&gt;Cumulative % of Rated&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;th&gt;Number&lt;/th&gt;
&lt;th&gt;% of rated&lt;/th&gt;
&lt;th&gt;Actual&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Shortfall&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 5 stars:&lt;/td&gt;
&lt;td&gt;112&lt;/td&gt;
&lt;td&gt;27&lt;/td&gt;
&lt;td&gt;27&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;-22&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 4 stars:&lt;/td&gt;
&lt;td&gt;183&lt;/td&gt;
&lt;td&gt;44&lt;/td&gt;
&lt;td&gt;72&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;-57&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 3 stars:&lt;/td&gt;
&lt;td&gt;92&lt;/td&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;94&lt;/td&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;-44&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 2 stars:&lt;/td&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;99&lt;/td&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;td&gt;-9&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 1 stars:&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;So over the last few months I&#39;ve completely revamped my iTunes ratings. Since I can&#39;t change the user interface, I&#39;ve changed my behavior. I&#39;m also taking advantage of two other fields: &amp;quot;checked&amp;quot; which I use to give more distinctiveness to my ratings, and &amp;quot;play count&amp;quot; which shows whether or not I&#39;ve listened to something through to the end.&lt;/p&gt;
&lt;p&gt;Here are the criteria I used:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 5 - Exemplars &lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/myrating_5_checked.png&#34; title=&#34;Myrating_5_checked&#34; alt=&#34;Myrating_5_checked&#34; /&gt;:&lt;/strong&gt; Only my most favorite songs are rated 5. They have to meet the following criteria: they make me feel good or excite me no matter how often I listen to them, I can typically listen to them often without getting tired of them, and they are the best of their particular genre.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 4 - Great &lt;img border=&#34;0&#34; alt=&#34;Myrating_4_checked&#34; title=&#34;Myrating_4_checked&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/myrating_4_checked.png&#34; /&gt;:&lt;/strong&gt; There is only a small difference between a song that is rated 4 and 5 in my ratings -- typically it doesn&#39;t excite me or make me smile quite as much, or it isn&#39;t necessarily an exemplar of its genre. However, I still can typically listen to them often without getting tired of them. Items that are rated 4 and 5 are ones that I carry on my iPod Shuffle.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 4 - Great (Unchecked) &lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/myrating_4_unchecked.png&#34; title=&#34;Myrating_4_unchecked&#34; alt=&#34;Myrating_4_unchecked&#34; /&gt;:&lt;/strong&gt; There are a few songs that I do consider to be great, but that I only want to play when I&#39;m in the mood for them, or I want to only play in a specific order, or they &amp;quot;don&#39;t play well&amp;quot; with other music. For instance I love the song &amp;quot;The Highwayman&amp;quot; by Loreena McKennitt, however, it is over 10 minutes long and I just don&#39;t want to hear that type of song unless I&#39;m in the mood for it. Other examples are the 12 songs that make up Mussorgsky&#39;s &amp;quot;Pictures at an Exhibition&amp;quot;&amp;nbsp; -- I want them played in order when I do play them, and I really don&#39;t want them played in the middle of my other songs. Unfortunately, iTunes does not let you select only unchecked items, so I don&#39;t have a Smart Playlist for these; instead I keep them in a regular playlist.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 3 - Good &lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/myrating_3_checked.png&#34; title=&#34;Myrating_3_checked&#34; alt=&#34;Myrating_3_checked&#34; /&gt;:&lt;/strong&gt; These are songs I like. Typically I can play them regularly but not too often. Songs rated 3-5 go on my iPod Nano.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 3 - Good (Unchecked) &lt;img border=&#34;0&#34; alt=&#34;Myrating_3_unchecked&#34; title=&#34;Myrating_3_unchecked&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/myrating_3_unchecked.png&#34; /&gt;:&lt;/strong&gt; There is a lot of music that I think is Good, but I don&#39;t want to play all the time. I have a large catalog of sound tracks from movies. All but a few of those tracks are in this category. Again, iTunes does not let you select only unchecked items in a Smart Playlist, so I have several regular playlists for these items.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 2 - Ok &lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/myrating_2_checked.png&#34; title=&#34;Myrating_2_checked&#34; alt=&#34;Myrating_2_checked&#34; /&gt;:&lt;/strong&gt; I have very diverse musical tastes, starting with jazz, various ethnic and world music, and also including quite a bit of pop, rap, R&amp;amp;B, punk, and metal that I enjoy. I don&#39;t enjoy them all the time -- but I do like them to pop up every once in a while for variety. So I rate these 2 and leave them checked. I have an old 40GB iPod that I take on long trips, and it stores everything I have that is checked and rated 2-5.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 2 - Ok (Unchecked) &lt;img border=&#34;0&#34; alt=&#34;Myrating_2_unchecked&#34; title=&#34;Myrating_2_unchecked&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/myrating_2_unchecked.png&#34; /&gt;:&lt;/strong&gt; Some songs are OK, but I really have to be in the mood specifically for that song. Listening to Jimmy Buffet&#39;s &amp;quot;Margaritaville&amp;quot; can be a guilty pleasure on a lazy summer day at the beach, but it isn&#39;t something I want to regularly listen to. I have a number of special playlists for songs rated like this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 1 - Don&#39;t Like &lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/myrating_1_checked.png&#34; title=&#34;Myrating_1_checked&#34; alt=&#34;Myrating_1_checked&#34; /&gt;:&lt;/strong&gt; These are the songs that I don&#39;t like. They&#39;re just not my style. Many are still quality music, they just doesn&#39;t work for me. I do keep most of these for completeness -- it might just be one or two songs on the album, and I want to keep the album complete. Or I keep it in case my tastes change. But in general, once something is rate 1 Star, I&#39;ll probably never listen to it again.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rated 1 - Trash (Unchecked) &lt;img border=&#34;0&#34; alt=&#34;Myrating_1_unchecked&#34; title=&#34;Myrating_1_unchecked&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/myrating_1_unchecked.png&#34; /&gt;:&lt;/strong&gt; These are songs that not only do I not like, they just are not good music. I don&#39;t like most rap music, but I can tell that most are still quality. Some are junk -- these I rate 1 and uncheck, and are candidates for deletion the next time I purge my collection.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unrated &amp;amp; Listened &lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/myrating_0_checked.png&#34; title=&#34;Myrating_0_checked&#34; alt=&#34;Myrating_0_checked&#34; /&gt;, playcount &amp;gt; 0:&lt;/strong&gt; If I&#39;ve listened to something through to the end, but haven&#39;t rated it yet, it shows up in this Smart Playlist. Periodically I check this Smart Playlist, sort by playcount, and try to rate everything that I&#39;ve listened to more then once.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unrated &amp;amp; Unlistened &lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/myrating_0_checked.png&#34; title=&#34;Myrating_0_checked&#34; alt=&#34;Myrating_0_checked&#34; /&gt;, play count=0:&lt;/strong&gt; This is the default when a new song is added to my library. So any song that is unrated, checked, and has a play count of 0 shows up in my &amp;quot;Unrated &amp;amp; Unlistened&amp;quot; Smart Playlist. When I&#39;m in the mood for variety, I go through this playlist and rate songs.&lt;/p&gt;
&lt;p&gt;Modifying my rating system in this way has caused my average rating for music to change from around 4 to somewhere between 2 and 3. It will probably, over time, become closer to 2 as I rate more of my collection. This gives me a lot of distinctiveness so that I can create Smart Playlists that work well for me.&lt;/p&gt;
&lt;table align=&#34;center&#34;&gt;
&lt;tbody&gt;&lt;tr&gt;&lt;td colspan=&#34;6&#34;&gt;Here are some statistics from your iTunes Library: 6519 tracks, 726 (11%) rated&lt;/td&gt;&lt;/tr&gt;
&lt;br /&gt; &lt;tr&gt;&lt;th colspan=&#34;3&#34;&gt;&amp;nbsp;&lt;/th&gt;
&lt;th colspan=&#34;3&#34;&gt;Cumulative % of Rated&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;th&gt;Number&lt;/th&gt;
&lt;th&gt;% of rated&lt;/th&gt;
&lt;th&gt;Actual&lt;/th&gt;
&lt;th&gt;Target&lt;/th&gt;
&lt;th&gt;Shortfall&lt;/th&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 5 stars:&lt;/td&gt;
&lt;td&gt;74&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;-5&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 4 stars:&lt;/td&gt;
&lt;td&gt;144&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;-15&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 3 stars:&lt;/td&gt;
&lt;td&gt;211&lt;/td&gt;
&lt;td&gt;29&lt;/td&gt;
&lt;td&gt;59&lt;/td&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;-9&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 2 stars:&lt;/td&gt;
&lt;td&gt;270&lt;/td&gt;
&lt;td&gt;37&lt;/td&gt;
&lt;td&gt;96&lt;/td&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;td&gt;-6&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tracks rated 1 stars:&lt;/td&gt;
&lt;td&gt;27&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;Obviously rating a large music collection can become a chore -- you don&#39;t want to spend your limited music listening time always fine tuning your ratings. So I have some approaches that make it easier for me to rate my music with less effort:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;p&gt;First, I sorted my catalog by my old ratings, and modified everything down by 1, Starting with everything rated 2 becoming 1, 3 becoming 2, etc. This gave me a good base to start with&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://lifewithalacrity.blogs.com/.shared/image.html?/photos/uncategorized/exemplar_smart_playlist.png&#34; onclick=&#34;window.open(this.href, &#39;_blank&#39;, &#39;width=512,height=216,scrollbars=no,resizable=no,toolbar=no,directories=no,location=no,menubar=no,status=no,left=0,top=0&#39;); return false&#34;&gt;&lt;img width=&#34;200&#34; height=&#34;84&#34; border=&#34;0&#34; alt=&#34;Exemplar_smart_playlist&#34; title=&#34;Exemplar_smart_playlist&#34; src=&#34;http://www.lifewithalacrity.com/previous/images/exemplar_smart_playlist.png&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
Next I created Smart Playlists for each rating, i.e. &amp;quot;Rating 5 - Exemplar&amp;quot; with &amp;quot;Match only checked songs&amp;quot; and &amp;quot;Live updating&amp;quot; checked. I then added &amp;quot;Play Count&amp;quot; as a column to my view, and sorted by it. This gave me the songs that I played the most and least, and I adjusted some songs up and down accordingly.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://lifewithalacrity.blogs.com/.shared/image.html?/photos/uncategorized/plays_well_with_others_smart_playlist.png&#34; onclick=&#34;window.open(this.href, &#39;_blank&#39;, &#39;width=512,height=241,scrollbars=no,resizable=no,toolbar=no,directories=no,location=no,menubar=no,status=no,left=0,top=0&#39;); return false&#34;&gt;&lt;img width=&#34;200&#34; height=&#34;94&#34; border=&#34;0&#34; alt=&#34;Plays_well_with_others_smart_playlist&#34; title=&#34;Plays_well_with_others_smart_playlist&#34; src=&#34;http://www.lifewithalacrity.com/previous/images/plays_well_with_others_smart_playlist.png&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
Then I created a new Smart Playlist that simply plays songs rated 3 to 5, limiting the list to the first 100 GB selected by random (i.e. everything random), and saved this Smart Playlist as &amp;quot;Plays Well With Others&amp;quot;. I play this on occasion in the background, and when I hear something that jars me I know something isn&#39;t rated right. Thus without a lot of effort I can change ratings for songs that no longer fit their rating, or uncheck items where the rating was appropriate but it &amp;quot;didn&#39;t play well with others&amp;quot;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I try to be aware when I&#39;m using my iPod of what a songs rating is, and change it if it seems wrong. The next time I sync the iPod my ratings will be adjusted in my iTunes catalog.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/play_count.png&#34; title=&#34;Play_count&#34; alt=&#34;Play_count&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;I also try to be aware of Play Count -- this number only goes up if you play a song to the end. So even if I&#39;m not able to take a look at the rating (for instance when I&#39;m in a car), I can at least forward to the next song. Periodically I review the play counts for songs that I&#39;ve rated and consider moving them up and down accordingly. Of course, this means that I have to be careful and not let the iPod keep running when I&#39;m not listening.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;A tip for those of you that do put a lot of effort into your iTunes ratings: I&#39;ve learned the hard way that unlike most song information, the rating is &lt;strong&gt;NOT&lt;/strong&gt; stored in the song itself, so if your iTunes database gets corrupted, or you move your music to another server, you&#39;ll lose all your ratings. One way to avoid this is to periodically backup your ratings into a field that is stored in the song itself. I personally use the &amp;quot;Grouping&amp;quot; field as it is rarely used, select all songs with the same rating and click on &amp;quot;Get Info&amp;quot;, and change the Grouping field to &amp;quot;My Rating: 5 Stars&amp;quot;.&lt;/p&gt;
&lt;p&gt;I only have 11% of my collection rated so far, but using this system I&#39;m finding it a lot easier to manage my ratings. I&#39;m already getting many benefits from it -- I&#39;m playing my music more often, my iPods typically have the music I want on them, and various music discovery services can use my ratings to help me identify new music I might enjoy. This provides the &lt;em&gt;incentive&lt;/em&gt; to keep me entering meaningful ratings.&lt;/p&gt;
&lt;h3&gt;Book Ratings - Amazon&lt;/h3&gt;
&lt;p&gt;Amazon also uses a 5-Star rating system, and your ratings can be used by Amazon to help you find books that you might like. Though I like to support my local bookstores, it is this feature that brings me back to Amazon time and again. Whenever I browse through Amazon and see a book I&#39;ve already read I try to take the time to update my rating.&lt;/p&gt;
&lt;p&gt;Amazon has a number of different tools to assist you in your ratings. If you are an Amazon customer, you can go to &lt;a href=&#34;http://www.amazon.com/gp/yourstore/iyr/?ie=UTF8&amp;amp;collection=owned&#34;&gt;Improve Your Recommendations: Edit Items You Own&lt;/a&gt; and see all the books that you&#39;ve purchased and quickly rate them with a nice AJAX interface. You can also review items that you&#39;ve already rated, whether or not you own them, at &lt;a href=&#34;http://www.amazon.com/gp/yourstore/iyr/?ie=UTF8&amp;amp;collection=rated&#34;&gt;Improve Your Recommendations: Edit Items You&#39;ve Rated&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a onclick=&#34;window.open(this.href, &#39;_blank&#39;, &#39;width=640,height=509,scrollbars=no,resizable=no,toolbar=no,directories=no,location=no,menubar=no,status=no,left=0,top=0&#39;); return false&#34; href=&#34;http://lifewithalacrity.blogs.com/.shared/image.html?/photos/uncategorized/amazon_your_media_library.png&#34;&gt;&lt;img width=&#34;200&#34; height=&#34;159&#34; border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/images/amazon_your_media_library.png&#34; title=&#34;Amazon_your_media_library&#34; alt=&#34;Amazon_your_media_library&#34; style=&#34;margin: 0px 0px 5px 5px; float: right;&#34; /&gt;&lt;/a&gt;
Amazon has also recently added a very nice web service called &lt;a href=&#34;http://www.amazon.com/gp/library/&#34;&gt;Your Media Library&lt;/a&gt; that can be used to help manage your media library of books, music, and dvds. I personally only have used it to manage my books and dvds, as I find rating albums useless -- it is songs that I prefer to rate.&lt;/p&gt;
&lt;p&gt;After browsing through my ratings to date, I discovered the same flaws I found iTunes -- my ratings typically were too high; most were a 4. This is particularly encouraged by the popup when your cursor is over the Stars &amp;quot;1 - I hate it, 2 - I don&#39;t like it, 3 - It&#39;s Ok, 4 - I like it, and 5 - I love it&amp;quot;. I suspect if I use the same trick that I use for iTunes of making a rating of 2 Stars mean &amp;quot;Ok&amp;quot; I could potentially cause the recommendation engine to be less effective (though it could possibly make it better, I don&#39;t know). So I am being much more brutal with my ratings and pushing many more down to 3, so that my ratings of 4 and 5 have more meaning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5 Stars &lt;img border=&#34;0&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/amazon_5_star.png&#34; title=&#34;Amazon_5_star&#34; alt=&#34;Amazon_5_star&#34; /&gt;:&lt;/strong&gt; These have to be the exemplars -- the best books I&#39;ve ever read, would be glad to read again, would be proud to show off on my best bookshelf, and will buy extra copies to give to friends.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4 Stars &lt;img border=&#34;0&#34; alt=&#34;Amazon_4_star&#34; title=&#34;Amazon_4_star&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/amazon_4_star.png&#34; /&gt;:&lt;/strong&gt; These have to be really good books -- most of them I&#39;m willing to read again and I promote them by offering to loan them to my more discriminating friends. Although I may keep them on my bookshelf I&#39;d rather give them to a friend then sell them at a used book store.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3 Stars &lt;img border=&#34;0&#34; alt=&#34;Amazon_3_star&#34; title=&#34;Amazon_3_star&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/amazon_3_star.png&#34; /&gt;:&lt;/strong&gt; These are books are decent books, and I do share them with my voracious reader friends. But I don&#39;t push them and I&#39;m much more likely to sell them at a used bookstore then keep them on my shelf. This is the rating that I significantly underused previously, and I&#39;m finding that the key discriminator for me so far is how much I feel like recommending this to friends who are more discriminating readers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2 Stars &lt;img border=&#34;0&#34; alt=&#34;Amazon_2_star&#34; title=&#34;Amazon_2_star&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/amazon_2_star.png&#34; /&gt;:&lt;/strong&gt; This rating is where the Amazon rating system fails the most -- these are suppost to be books that &amp;quot;I don&#39;t like&amp;quot;, however, most of the time I don&#39;t buy books that I probably wouldn&#39;t like, much less read them, so I have very few in this category. However, I&#39;ve decided this category is for books that are just not quite good enough, or are slightly disappointing. Not bad, or disliked, but just somewhat disappointing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1 Stars &lt;img border=&#34;0&#34; alt=&#34;Amazon_1_star&#34; title=&#34;Amazon_1_star&#34; src=&#34;http://www.lifewithalacrity.com/previous/photos/uncategorized/amazon_1_star.png&#34; /&gt;:&lt;/strong&gt; This is where I put the books that I don&#39;t like, or worse, I hate. Not many here, but I&#39;m willing to risk more then many people are so I have some. Also books go here that just don&#39;t fit my interest, like romance novels that get recommended to me because I like some crossover fantasy-romance authors.&lt;/p&gt;
&lt;p&gt;Since I started more accurately rating my books at Amazon, I&#39;ve found that their suggestions for other books to read to be more accurate. Thus I am getting value from rating these books, and I have &lt;em&gt;incentive&lt;/em&gt; to continue to make the effort.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Offering an incentive for people to rate is important for ratings of all sorts, with both individual gain and status recognition being powerful motivators.&lt;/p&gt;
&lt;p&gt;However the easiest technique for making a 5-point rating scale more useful is to make it &amp;quot;distinct&amp;quot;. If a user has a more specific meaning for each rating, ratings will slowly settle toward a truer average, and thus more of each rating scale will be used. We&#39;ve also tried this technique recently on RPGnet, with our new &lt;a href=&#34;http://index.rpg.net/&#34;&gt;Gaming Index&lt;/a&gt;; and thus far our new 10-point scale -- which has distinct meanings for each number -- is averaging &lt;a href=&#34;http://index.rpg.net/browse.phtml?count=50&amp;amp;type=Best&amp;amp;productCategory=Entries&#34;&gt;7.27&lt;/a&gt;. That&#39;s still a fair amount above the real average of 5.5, but at least it&#39;s below the 8+ rating that our old double 5-point scale resulted in.&lt;/p&gt;
&lt;p&gt;Often you, as a consumer of rating systems, will be making use of rating scales designed by others, rather than those you&#39;re designing yourself. For those cases it often makes sense to design your own rules for what each number means, and to do so in such a way that your median is the average of the scale, rather than toward one of the extremes. When you do, even if you&#39;re using a tight 5-point scale you&#39;ll end up with enough differentiation for it to actually be more meaningful than a thumbs up or a thumbs down.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Related articles from this blog:&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/systems_for_col.html&#34;&gt;2005-12: Systems for Collective Choice&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2005/12/collective_choi.html&#34;&gt;2005-12: Collective Choice: Rating Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2006/01/ranking_systems.html&#34;&gt;2006-01: Collective Choice: Competitive Ranking Systems&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/2007/01/collective_choi.html&#34;&gt;2007-01: Experimenting with Ratings&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;p&gt;Related articles from Shannon Appelcline&#39;s &lt;a href=&#34;http://www.skotos.net/articles/show-column.phtml?colname=TTnT_&#34;&gt;Trials, Triumphs &amp;amp; Trivialities:&lt;/a&gt;
&lt;/p&gt;&lt;blockquote&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_192.phtml&#34;&gt;#192: Managing User Creativity, Part One&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_193.phtml&#34;&gt;#193: Managing User Creativity, Part Two&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_196.phtml&#34;&gt;#196: Collective Choice: Ratings, Who Do You Trust?&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href=&#34;http://www.skotos.net/articles/TTnT_/TTnT_198.phtml&#34;&gt;#198: Collective Choice: More Thoughts About Ratings&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/blockquote&gt;
&lt;footer&gt;&lt;h3&gt;Comments&lt;/h3&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;See
http://web.archive.org/web/20010207224515/http://people.delphi.com/mike100000/p5summary.html#summary (or click my name below) for a summary of all the votes for the B5 Series.
Averaging all episodes you get a mean score of 8.19 with a standard deviation of 0.84 - a *very* narrow range.
Context and Motivation matter more than scale in getting reusable scores out of rating systems. As I like to tell the folks here at Yahoo! – the person creating the rating has to get something out of the transaction other than just altruism.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://web.archive.org/web/20010207224515/http://people.delphi.com/mike100000/p5summary.html#summary&#34;&gt;F. Randall Farmer&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-08-11T09:49:28-07:00&#34;&gt;2006-08-11T09:49:28-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;I thought the issue, like Randy Farmer suggests, was that only positive discrete scales always tend to their maximum.  The problem then in rating systems, like you mentioned I think earlier, is that readers have to interpret what the real norm and limits are.  Even if every rating is in the upper quartile.  You may want to check out what I did in Playerep (http://www.playerep.com), where the rating system is based on election but not a fixed scale (always norming to zero without influence).   FWIW. Thanks.
Adam
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://tidehorion.blogspot.com&#34;&gt;Adam MacDonald&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-08-11T10:21:24-07:00&#34;&gt;2006-08-11T10:21:24-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;URL:
I like your classification scheme for iTunes. I&#39;m going to have to work on adapting my scale (which is definitely showing signs of skewing towards the high end).
One thing, though. You mention that you can&#39;t create smart playlists to show unchecked songs. It&#39;s true that you can&#39;t do it with only one playlist, but you can do so with 2:
#1) Any playlist (for example, let&#39;s say your smart playlist for 4 star, checked songs, so you have it set (I assume) for My Rating = 4, Match Only Checked Songs = checked)
#2) Use these criteria:  My Rating = 4, Playlist Is Not &lt;playlist #1&gt;. Leave match only checked songs unchecked, and enable live updating and you have a smart playlist that shows you your unchecked 4-star songs.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;#&#34;&gt;Andy Tinkham&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-08-16T13:55:43-07:00&#34;&gt;2006-08-16T13:55:43-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;URL:
&gt;&gt;Unfortunately, iTunes does not let you select only unchecked items, so I don&#39;t have a Smart Playlist for these; instead I keep them in a regular playlist.
You can:
Make a smart playlist called &#39;Checked&#39; with a rule that is always true (e.g. artist does not contain (leave the field emtpy), or bitrate &gt; 0). Create a smart playlist called &#39;Unchecked&#39; with the rule: &#39;Playlist is not Checked&#39;. There you have all your unchecked files.
From there you can make smart ones that use this Unchecked playlist.
I like your rating system. I have 40% on a 3star now, and around 25% for 2 and 4 stars. I might scale it down a little bit more.
hmm, after typing this I read the comments and see Adam McDonald already mentioned something similar about the unchecked items. I&#39;ll leave it here anyway...
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;#&#34;&gt;Tino&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-10-16T11:52:32-07:00&#34;&gt;2006-10-16T11:52:32-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;Christopher, I think some, if not all, of the issues with the 5-point rating system could be addressed with a better UI. For example, displaying a normal distribution curve skewed by accumulated rating to nudge raters away from the extremes. Of course, this doesn&#39;t work well if sample size is too small which usually happens on the far end of the long tail.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;http://www.docuverse.com/blog/donpark/&#34;&gt;Don Park&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2006-12-17T18:25:50-07:00&#34;&gt;2006-12-17T18:25:50-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;div class=&#34;u-comment h-cite&#34;&gt;
&lt;p class=&#34;p-content p-name&#34;&gt;URL:
Thanks for writing this article. I found another view on 5 point music rating other than my own informational.
&lt;/p&gt;
&lt;a class=&#34;u-author h-card&#34; href=&#34;#&#34;&gt;Jonathan&lt;/a&gt;
&lt;time class=&#34;dt-published&#34; datetime=&#34;2008-02-06T00:33:36-07:00&#34;&gt;2008-02-06T00:33:36-07:00&lt;/time&gt;
&lt;/div&gt;
&lt;/footer&gt;
&lt;p class=&#34;previous&#34;&gt;&lt;a href=&#34;http://www.lifewithalacrity.com/previous/2006/08/using_5star_rat.html&#34; rel=&#34;syndication&#34; class=&#34;u-syndication&#34; &gt;orginal layout&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>